=== train_aug.py ===
import argparse
import json
import os
import subprocess
import matplotlib.pyplot as plt
import torch
import yaml
from crnn import CRNN, ArtifactRemovalCRNN
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset, SliceDatasetAug
from deepinv.transform import Transform
from einops import rearrange
from radial import RadialDCLayer, to_torch_complex, MCNUFFT_CRNN
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex
from eval import eval_grasp, eval_sample
import csv
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
eval_dir = os.path.join(output_dir, "eval_results")
os.makedirs(eval_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                   'lambda_S': config['model']['lambda_S'], 
                   'lambda_spatial_L': config['model']['lambda_spatial_L'],
                   'lambda_spatial_S': config['model']['lambda_spatial_S'],
                   'gamma': config['model']['gamma'],
                   'lambda_step': config['model']['lambda_step']}
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
start_epoch = 1
model_type = config["model"]["name"]
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils, N_time_eval = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"],
    config["data"]["eval_timeframes"]
)
N_spokes = int(config["data"]["total_spokes"] / N_time)
os.makedirs(os.path.join(output_dir, 'enhancement_curves'), exist_ok=True)
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
else:
    train_patient_ids = splits["train"]
val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]
if config['data']['spf_aug']:
    train_dataset = SliceDatasetAug(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        N_coils=N_coils
    )
else:
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        N_time=N_time,
        N_coils=N_coils
    )
val_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=val_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
    N_coils=N_coils
)
val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_loader = DataLoader(
    val_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)
eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], lambdas=initial_lambdas, channels=config['model']['channels'])
model = ArtifactRemovalLSFPNet(lsfp_backbone, output_dir).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
else:
    start_epoch = 1
if config['model']['losses']['mc_loss']['metric'] == "MSE":
    mc_loss_fn = MCLoss(model_type=model_type)
elif config['model']['losses']['mc_loss']['metric'] == "MAE":
    mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
else:
    raise(ValueError, "Unsupported MC Loss Metric.")
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(subsample, model_type=model_type)
        else:
            ei_loss_fn = EILoss(subsample | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "monophasic":
        ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "reverse":
        ei_loss_fn = EILoss(time_reverse | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "all":
        ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise | time_reverse) | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
            ei_loss_fn = EILoss(rotate, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
            ei_loss_fn = EILoss(diffeo, model_type=model_type)
        else:
            ei_loss_fn = EILoss(rotate | diffeo, model_type=model_type)
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    train_adj_losses = train_curves["train_adj_losses"]
    val_adj_losses = val_curves["val_adj_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
    weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
    eval_ssims = eval_curves["ssim"]
    eval_psnrs = eval_curves["psnr"]
    eval_mses = eval_curves["mse"]
    eval_dcs = eval_curves["dc"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    train_adj_losses = []
    val_adj_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
    weighted_train_adj_losses = []
    eval_ssims = []
    eval_psnrs = []
    eval_mses = []
    eval_dcs = []
grasp_ssims = []
grasp_psnrs = []
grasp_mses = []
grasp_dcs = []
lambda_Ls = []
lambda_Ss = []
lambda_spatial_Ls = []
lambda_spatial_Ss = []
gammas = []
lambda_steps = []
iteration_count = 0
if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    initial_train_adj_loss = 0.0
    initial_val_adj_loss = 0.0
    with torch.no_grad():
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            if model_type == "LSFPNet":
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                csmap = csmap.to(device).to(measured_kspace.dtype)
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, epoch="train0", norm=config['model']['norm']
                )
                initial_train_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
        train_adj_losses.append(step0_train_adj_loss)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        for measured_kspace, csmap, ground_truth, grasp_img, *_ in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
            if model_type == "LSFPNet":
                measured_kspace = measured_kspace.to(device)
                csmap = csmap.to(device)
                measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
                csmap = csmap.squeeze(0).to(device)   # Remove batch dim
                x_recon, adj_loss, *_ = model(
                    measured_kspace.to(device), eval_physics, csmap, epoch="val0", norm=config['model']['norm']
                )
                initial_val_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), eval_physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, eval_physics, model, csmap
                )
                initial_val_ei_loss += ei_loss.item()
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device)
            grasp_ssims.append(ssim_grasp)
            grasp_psnrs.append(psnr_grasp)
            grasp_mses.append(mse_grasp)
            grasp_dcs.append(dc_grasp)
        step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
        val_ei_losses.append(step0_val_ei_loss)
        step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
        val_adj_losses.append(step0_val_adj_loss)
    print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
    print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        running_adj_loss = 0.0
        epoch_eval_ssims = []
        epoch_eval_psnrs = []
        epoch_eval_mses = []
        epoch_eval_dcs = []
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            iteration_count += 1
            optimizer.zero_grad()
            if model_type == "LSFPNet":
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                csmap = csmap.to(device).to(measured_kspace.dtype)
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, epoch=f"train{epoch}", norm=config['model']['norm']
                )
                running_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            running_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                ei_loss_weight = get_cosine_ei_weight(
                    current_epoch=epoch,
                    warmup_epochs=warmup,
                    schedule_duration=duration,
                    target_weight=target_weight
                )
                running_ei_loss += ei_loss.item()
                total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(
                    mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                )
            else:
                total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
            if torch.isnan(total_loss):
                print(
                    "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                )
                raise RuntimeError("total_loss is NaN")
            total_loss.backward()
            if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                log_gradient_stats(
                    model=model,
                    epoch=epoch,
                    iteration=iteration_count,
                    output_dir=output_dir,
                    log_filename="gradient_stats.csv"
                )
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_train_adj_loss = running_adj_loss / len(train_loader)
            train_adj_losses.append(epoch_train_adj_loss)
            weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
        else:
            train_adj_losses.append(0.0)
            weighted_train_adj_losses.append(0.0)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_running_adj_loss = 0.0
        val_loader_tqdm = tqdm(
            val_dro_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask in tqdm(val_dro_loader):
                if model_type == "LSFPNet":
                    val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                    val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                    val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                    val_grasp_img = torch.flip(val_grasp_img, dims=[-3])
                    val_grasp_img = torch.rot90(val_grasp_img, k=3, dims=[-3,-1])
                    val_grasp_img_tensor = val_grasp_img.to(device)
                    val_x_recon, val_adj_loss, *_ = model(
                        val_kspace_batch.to(device), eval_physics, val_csmap, epoch=f"val{epoch}", norm=config['model']['norm']
                    )
                    val_running_adj_loss += val_adj_loss.item()
                else:
                    val_x_recon = model(val_kspace_batch.to(device), eval_physics, val_csmap)
                val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                val_running_mc_loss += val_mc_loss.item()
                if use_ei_loss:
                    val_ei_loss, val_t_img = ei_loss_fn(
                        val_x_recon, eval_physics, model, val_csmap
                    )
                    val_running_ei_loss += val_ei_loss.item()
                    val_loader_tqdm.set_postfix(
                        val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                    )
                else:
                    val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                ssim, psnr, mse, dc = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, eval_dir, f'epoch{epoch}', device)
                epoch_eval_ssims.append(ssim)
                epoch_eval_psnrs.append(psnr)
                epoch_eval_mses.append(mse)
                epoch_eval_dcs.append(dc)
        epoch_eval_ssim = np.mean(epoch_eval_ssims)
        epoch_eval_psnr = np.mean(epoch_eval_psnrs)
        epoch_eval_mse = np.mean(epoch_eval_mses)
        epoch_eval_dc = np.mean(epoch_eval_dcs)
        eval_ssims.append(epoch_eval_ssim)
        eval_psnrs.append(epoch_eval_psnr)
        eval_mses.append(epoch_eval_mse)
        eval_dcs.append(epoch_eval_dc)    
        if epoch % save_interval == 0:
            val_x_recon_rot = torch.rot90(val_x_recon, k=2, dims=[-3,-2])
            plot_reconstruction_sample(
                val_x_recon_rot,
                f"Validation Sample - Epoch {epoch}",
                f"val_sample_epoch_{epoch}",
                output_dir,
                val_grasp_img
            )
            val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                val_x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
            plot_enhancement_curve(
                val_grasp_img,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch}",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
        epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            val_adj_losses.append(epoch_val_adj_loss)
        else:
            val_adj_losses.append(0.0)
        if epoch % save_interval == 0:
            train_curves = dict(
                train_mc_losses=train_mc_losses,
                train_ei_losses=train_ei_losses,
                weighted_train_mc_losses=weighted_train_mc_losses,
                weighted_train_ei_losses=weighted_train_ei_losses,
            )
            val_curves = dict(
                val_mc_losses=val_mc_losses,
                val_ei_losses=val_ei_losses,
            )
            eval_curves = dict(
                eval_ssims=eval_ssims,
                eval_psnrs=eval_psnrs,
                eval_mses=eval_mses,
                eval_dcs=eval_dcs
            )
            model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
            save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, model_save_path)
            print(f'Model saved to {model_save_path}')
            plt.figure()
            plt.plot(train_mc_losses, label="Training MC Loss")
            plt.plot(val_mc_losses, label="Validation MC Loss")
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(train_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Training Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "train_mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(val_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Validation Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "val_mc_losses.png"))
            plt.close()
            if use_ei_loss:
                plt.figure()
                plt.plot(train_ei_losses, label="Training EI Loss")
                plt.plot(val_ei_losses, label="Validation EI Loss")
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(train_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Training Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "train_ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(val_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Validation Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "val_ei_losses.png"))
                plt.close()
            if model_type == "LSFPNet":
                plt.figure()
                plt.plot(train_adj_losses, label="Training Adjoint Loss")
                plt.plot(val_adj_losses, label="Validation Adjoint Loss")
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(train_adj_losses)
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("Training CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "train_adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(val_adj_losses)
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("Validation CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "val_adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_Ls)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_L Value")
                plt.title("Lambda_L During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_L.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_Ss)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_S Value")
                plt.title("Lambda_S During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_S.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_spatial_Ls)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_spatial_L Value")
                plt.title("Lambda_spatial_L During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_spatial_L.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_spatial_Ss)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_spatial_S Value")
                plt.title("Lambda_spatial_S During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_spatial_S.png"))
                plt.close()
                plt.figure()
                plt.plot(gammas)
                plt.xlabel("Epoch")
                plt.ylabel("Gamma Value")
                plt.title("Step Size During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "gamma.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_steps)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_step Value")
                plt.title("Lambda_step During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_step.png"))
                plt.close()
            plt.figure()
            plt.plot(weighted_train_mc_losses, label="MC Loss")
            plt.plot(weighted_train_ei_losses, label="EI Loss")
            plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Weighted Training Losses")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_ssims)
            plt.xlabel("Epoch")
            plt.ylabel("SSIM")
            plt.title("Evaluation SSIM")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_ssims.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_psnrs)
            plt.xlabel("Epoch")
            plt.ylabel("PSNR")
            plt.title("Evaluation PSNR")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_psnrs.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_mses)
            plt.xlabel("Epoch")
            plt.ylabel("MSE")
            plt.title("Evaluation MSE")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_mses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dcs)
            plt.xlabel("Epoch")
            plt.ylabel("DC")
            plt.title("Evaluation DC")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dcs.png"))
            plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
        if model_type == "LSFPNet":
            print(
                f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
            )
        print(f"--- Evaluation Metrics: Epoch {epoch} ---")
        print(f"Recon SSIM: {epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}")
        print(f"Recon PSNR: {epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}")
        print(f"Recon MSE: {epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}")
        print(f"Recon DC: {epoch_eval_dc:.4f} ± {np.std(epoch_eval_dcs):.4f}")
        print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}")
        print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}")
        print(f"GRASP MSE: {np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}")
        print(f"GRASP DC: {np.mean(grasp_dcs):.6f} ± {np.std(grasp_dcs):.4f}")
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    train_adj_losses=train_adj_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
    weighted_train_adj_losses=weighted_train_adj_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
    val_adj_losses=val_adj_losses,
)
eval_curves = dict(
    eval_ssims=eval_ssims,
    eval_psnrs=eval_psnrs,
    eval_mses=eval_mses,
    eval_dcs=eval_dcs,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, model_save_path)
print(f'Model saved to {model_save_path}')
metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
with open(metrics_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'DC'])
    writer.writerow(['DL', f'{epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}', f'{epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}', f'{epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}', f'{epoch_eval_dc:.4f} ± {np.std(epoch_eval_dcs):.4f}'])
    writer.writerow(['GRASP', f'{np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}', f'{np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}', f'{np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}', f'{np.mean(grasp_dcs):.4f} ± {np.std(grasp_dcs):.4f}'])
MAIN_EVALUATION_PLAN = [
    {
        "spokes_per_frame": 16,
        "num_frames": 20, # 16 * 20 = 320 total spokes
        "slice": slice(1, 21), # Wide window
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 20,
        "num_frames": 16, # 20 * 16 = 320 total spokes
        "slice": slice(3, 19), # Medium-wide window
        "description": "Good temporal resolution"
    },
    {
        "spokes_per_frame": 32,
        "num_frames": 10, # 32 * 10 = 320 total spokes
        "slice": slice(5, 15), # Narrow window centered on enhancement
        "description": "Standard temporal resolution"
    },
    {
        "spokes_per_frame": 40,
        "num_frames": 8,  # 40 * 8 = 320 total spokes
        "slice": slice(5, 13), # Very narrow window on peak
        "description": "Low temporal resolution"
    }
]
STRESS_TEST_PLAN = [
    {
        "spokes_per_frame": 4,
        "num_frames": 22, # 4 * 22 = 88 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 4 spokes"
    },
    {
        "spokes_per_frame": 8,
        "num_frames": 22, # 8 * 22 = 176 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 8 spokes"
    },
]
eval_spf_dataset = SimulatedSPFDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    )
eval_spf_loader = DataLoader(
    eval_spf_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
with torch.no_grad():
    spf_recon_ssim = {}
    spf_recon_psnr = {}
    spf_recon_mse = {}
    spf_recon_dc = {}
    spf_grasp_ssim = {}
    spf_grasp_psnr = {}
    spf_grasp_mse = {}
    spf_grasp_dc = {}
    print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
    for eval_config in STRESS_TEST_PLAN:
        stress_test_ssims = []
        stress_test_psnrs = []
        stress_test_mses = []
        stress_test_dcs = []
        stress_test_grasp_ssims = []
        stress_test_grasp_psnrs = []
        stress_test_grasp_mses = []
        stress_test_grasp_dcs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_img = grasp_img.to(device)
            grasp_img_plot = torch.flip(grasp_img, dims=[-3])
            grasp_img_plot = torch.rot90(grasp_img_plot, k=3, dims=[-3,-1])
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(640, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            x_recon, *_ = model(
                kspace.to(device), physics, csmap, epoch=None, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, dc = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img_plot, eval_dir, f"{spokes}spf", device)
            stress_test_ssims.append(ssim)
            stress_test_psnrs.append(psnr)
            stress_test_mses.append(mse)
            stress_test_dcs.append(dc)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device)
            stress_test_grasp_ssims.append(ssim_grasp)
            stress_test_grasp_psnrs.append(psnr_grasp)
            stress_test_grasp_mses.append(mse_grasp)
            stress_test_grasp_dcs.append(dc_grasp)
            spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
            spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
            spf_recon_mse[spokes] = np.mean(stress_test_mses)
            spf_recon_dc[spokes] = np.mean(stress_test_dcs)
            spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
            spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
            spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
            spf_grasp_dc[spokes] = np.mean(stress_test_grasp_dcs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'DC'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(stress_test_ssims):.4f} ± {np.std(stress_test_ssims):.4f}', 
            f'{np.mean(stress_test_psnrs):.4f} ± {np.std(stress_test_psnrs):.4f}', 
            f'{np.mean(stress_test_mses):.4f} ± {np.std(stress_test_mses):.4f}', 
            f'{np.mean(stress_test_dcs):.4f} ± {np.std(stress_test_dcs):.4f}'])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(stress_test_grasp_ssims):.4f} ± {np.std(stress_test_grasp_ssims):.4f}', 
            f'{np.mean(stress_test_grasp_psnrs):.4f} ± {np.std(stress_test_grasp_psnrs):.4f}', 
            f'{np.mean(stress_test_grasp_mses):.4f} ± {np.std(stress_test_grasp_mses):.4f}', 
            f'{np.mean(stress_test_grasp_dcs):.4f} ± {np.std(stress_test_grasp_dcs):.4f}'])
    print("--- Running Main Evaluation (Budget: 320 spokes) ---")
    for eval_config in MAIN_EVALUATION_PLAN:
        spf_eval_ssims = []
        spf_eval_psnrs = []
        spf_eval_mses = []
        spf_eval_dcs = []
        spf_grasp_ssims = []
        spf_grasp_psnrs = []
        spf_grasp_mses = []
        spf_grasp_dcs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_img = grasp_img.to(device)
            grasp_img_plot = torch.flip(grasp_img, dims=[-3])
            grasp_img_plot = torch.rot90(grasp_img_plot, k=3, dims=[-3,-1])
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(640, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            x_recon, *_ = model(
                kspace.to(device), physics, csmap, epoch=None, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, dc = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img_plot, eval_dir, f'{spokes}spf', device)
            spf_eval_ssims.append(ssim)
            spf_eval_psnrs.append(psnr)
            spf_eval_mses.append(mse)
            spf_eval_dcs.append(dc)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device)
            spf_grasp_ssims.append(ssim_grasp)
            spf_grasp_psnrs.append(psnr_grasp)
            spf_grasp_mses.append(mse_grasp)
            spf_grasp_dcs.append(dc_grasp)
        spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
        spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
        spf_recon_mse[spokes] = np.mean(spf_eval_mses)
        spf_recon_dc[spokes] = np.mean(spf_eval_dcs)
        spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
        spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
        spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
        spf_grasp_dc[spokes] = np.mean(spf_grasp_dcs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'DC'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(spf_eval_ssims):.4f} ± {np.std(spf_eval_ssims):.4f}', 
            f'{np.mean(spf_eval_psnrs):.4f} ± {np.std(spf_eval_psnrs):.4f}', 
            f'{np.mean(spf_eval_mses):.4f} ± {np.std(spf_eval_mses):.4f}', 
            f'{np.mean(spf_eval_dcs):.4f} ± {np.std(spf_eval_dcs):.4f}'])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(spf_grasp_ssims):.4f} ± {np.std(spf_grasp_ssims):.4f}', 
            f'{np.mean(spf_grasp_psnrs):.4f} ± {np.std(spf_grasp_psnrs):.4f}', 
            f'{np.mean(spf_grasp_mses):.4f} ± {np.std(spf_grasp_mses):.4f}', 
            f'{np.mean(spf_grasp_dcs):.4f} ± {np.std(spf_grasp_dcs):.4f}'])
plt.figure()
plt.plot(list(spf_recon_ssim.keys()), list(spf_recon_ssim.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_ssim.keys()), list(spf_grasp_ssim.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("SSIM")
plt.title("Evaluation SSIM vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_ssim.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_psnr.keys()), list(spf_recon_psnr.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_psnr.keys()), list(spf_grasp_psnr.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("PSNR")
plt.title("Evaluation PSNR vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_psnr.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_mse.keys()), list(spf_recon_mse.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_mse.keys()), list(spf_grasp_mse.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("MSE")
plt.title("Evaluation Image MSE vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_mse.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_dc.keys()), list(spf_recon_dc.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_dc.keys()), list(spf_grasp_dc.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("k-space MSE")
plt.title("Data Consistency Evaluation (MSE) vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_dc.png"))
plt.close()

=== ei.py ===
from typing import Union
import torch
import torch.nn as nn
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from einops import rearrange
from radial import to_torch_complex
class EILoss(Loss):
    r"""
    Equivariant imaging self-supervised loss.
    Assumes that the set of signals is invariant to a group of transformations (rotations, translations, etc.)
    in order to learn from incomplete measurement data alone https://https://arxiv.org/pdf/2103.14756.pdf.
    The EI loss is defined as
    .. math::
        \| T_g \hat{x} - \inverse{\forw{T_g \hat{x}}}\|^2
    where :math:`\hat{x}=\inverse{y}` is a reconstructed signal and
    :math:`T_g` is a transformation sampled at random from a group :math:`g\sim\group`.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param deepinv.transform.Transform transform: Transform to generate the virtually augmented measurement.
        It can be any torch-differentiable function (e.g., a :class:`torch.nn.Module`)
        including `torchvision transforms <https://pytorch.org/vision/stable/transforms.html>`_.
    :param Metric, torch.nn.Module metric: Metric used to compute the error between the reconstructed augmented measurement and the reference
        image.
    :param bool apply_noise: if ``True``, the augmented measurement is computed with the full sensing model
        :math:`\sensor{\noise{\forw{\hat{x}}}}` (i.e., noise and sensor model),
        otherwise is generated as :math:`\forw{\hat{x}}`.
    :param float weight: Weight of the loss.
    :param bool no_grad: if ``True``, the gradient does not propagate through :math:`T_g`. Default: ``False``.
        This option is useful for super-resolution problems, see https://arxiv.org/abs/2312.11232.
    """
    def __init__(
        self,
        transform: Transform,
        model_type: str,
        metric: Union[Metric, nn.Module] = torch.nn.MSELoss(),
        apply_noise=True,
        weight=1.0,
        no_grad=False,
        *args,
        **kwargs,
    ):
        super(EILoss, self).__init__(*args, **kwargs)
        self.name = "ei"
        self.metric = metric
        self.weight = weight
        self.T = transform
        self.noise = apply_noise
        self.no_grad = no_grad
        self.model_type = model_type
    def forward(self, x_net, physics, model, csmap, **kwargs):
        r"""
        Computes the EI loss
        :param torch.Tensor x_net: Reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: Forward operator associated with the measurements.
        :param torch.nn.Module model: Reconstruction function.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.no_grad:
            with torch.no_grad():
                x2 = self.T(x_net)
                x2 = x2.detach()
        else:
            x2 = self.T(x_net)
        if self.model_type == "CRNN":
            if self.noise:
                y = physics(x2, csmap)
            else:
                y = physics.A(x2, csmap)
            x3 = model(y, physics, csmap)
        elif self.model_type == "LSFPNet":
            x2_complex = to_torch_complex(x2)
            y = physics(inv=False, data=x2_complex, smaps=csmap).to(csmap.device)
            x3, *_ = model(y, physics, csmap, epoch=None)
        loss_ei = self.weight * self.metric(x3, x2)
        return loss_ei, x2

=== transform.py ===
import deepinv as dinv
import torch
from deepinv.transform import Transform
from einops import rearrange
import torch.nn.functional as F
from torchvision.transforms.functional import rotate
from typing import Union
class VideoRotate(Transform):
    r"""
    CORRECTED 2D Rotation for Videos (Handles deepinv composition).
    This class correctly applies a single, consistent random rotation to all frames of a video.
    It samples angles uniformly from a continuous range and is robust to being called
    from a deepinv composition operator that pre-flattens the video tensor.
    :param tuple[float, float] or float degrees: Range of degrees to select from.
        If degrees is a number instead of sequence like (min, max), the range of degrees
        will be (-degrees, +degrees).
    :param str interpolation_mode: "bilinear" or "nearest".
    :param bool constant_shape: if True, output has the same shape as the input.
    """
    def __init__(
        self,
        *args,
        degrees: Union[float, tuple[float, float]] = 180.0,
        interpolation_mode: str = "bilinear",
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        if isinstance(degrees, (int, float)):
            if degrees < 0:
                raise ValueError("If degrees is a single number, it must be non-negative.")
            self.degrees = (-degrees, degrees)
        else:
            if len(degrees) != 2:
                raise ValueError("If degrees is a sequence, it must be of length 2.")
            self.degrees = degrees
        self.interpolation_mode = interpolation_mode
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Uniformly samples `n_trans` random angles from the specified continuous range.
        """
        angles = [
            torch.empty(1).uniform_(self.degrees[0], self.degrees[1]).item()
            for _ in range(self.n_trans)
        ]
        return {"theta": angles}
    def _transform(
        self,
        x: torch.Tensor,
        theta: Union[torch.Tensor, list] = [],
        **kwargs,
    ) -> torch.Tensor:
        """
        Applies the rotation transformations. This method now explicitly handles 5D video tensors.
        """
        if not self._check_x_5D(x):
             raise ValueError("VideoRotate is designed for 5D video tensors (B, C, T, H, W).")
        B, C, T, H, W = x.shape
        if not theta:
            params = self._get_params(x)
            theta = params["theta"]
        if not theta:
            raise ValueError("Rotation angle 'theta' not provided.")
        angle_for_video = theta[0]
        angle_rad = -torch.tensor(angle_for_video) * (torch.pi / 180.0)
        cos_a, sin_a = torch.cos(angle_rad), torch.sin(angle_rad)
        matrix = torch.tensor(
            [[cos_a, -sin_a, 0], [sin_a, cos_a, 0]], 
            dtype=torch.float32, device=x.device
        ).unsqueeze(0)
        matrix = matrix.repeat(B, 1, 1)
        grid_single = F.affine_grid(matrix, (B, C, H, W), align_corners=False)
        grid_expanded = grid_single.repeat_interleave(T, dim=0)
        x_flat = dinv.physics.TimeMixin.flatten(x)
        transformed_flat = F.grid_sample(x_flat, grid_expanded, mode=self.interpolation_mode, padding_mode='zeros', align_corners=False)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class VideoDiffeo(dinv.transform.CPABDiffeomorphism):
    """A Diffeomorphism transform that correctly handles 5D video tensors."""
    def _transform(self, x: torch.Tensor, **params) -> torch.Tensor:
        if not self._check_x_5D(x):
            return super()._transform(x, **params)
        B = x.shape[0]
        x_flat = dinv.physics.TimeMixin.flatten(x)
        flat_params = self.get_params(x_flat)
        transformed_flat = super()._transform(x_flat, **flat_params)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class SubsampleTime(Transform):
    r"""
    Augments a video by taking a random contiguous temporal sub-sequence of a
    RANDOM length, and then interpolating it back to the original length.
    :param int n_trans: Number of transformed versions to generate per input image.
    :param tuple[float, float] subsample_ratio_range: The min and max ratio of the
                                                     total time frames to keep (e.g., (0.7, 0.95)).
    :param torch.Generator rng: Random number generator.
    """
    def __init__(self, *args, subsample_ratio_range: tuple[float, float] = (0.7, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False  # Operate on 5D tensor
        min_r, max_r = subsample_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "subsample_ratio_range must be a valid range (min, max) between 0 and 1."
        self.subsample_ratio_range = subsample_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a random ratio and a random start index for each transform.
        """
        total_time_frames = x.shape[2]  # Shape is (B, C, T, H, W)
        min_r, max_r = self.subsample_ratio_range
        ratios = min_r + (max_r - min_r) * torch.rand(self.n_trans, generator=self.rng)
        start_indices = []
        for ratio in ratios:
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                start_indices.append(0)
                continue
            max_start_index = total_time_frames - subsample_length
            start_idx = torch.randint(
                low=0, high=max_start_index + 1, size=(1,), generator=self.rng
            ).item()
            start_indices.append(start_idx)
        return {"ratios": ratios, "start_indices": torch.tensor(start_indices, dtype=torch.long)}
    def _transform(
        self, x: torch.Tensor, ratios: torch.Tensor, start_indices: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """Performs the temporal subsampling and resizing for each requested transform."""
        B, C, total_time_frames, H, W = x.shape
        assert B == 1, "This transform implementation assumes a batch size of 1 for simplicity."
        output_list = []
        for i in range(self.n_trans):
            ratio = ratios[i]
            start_idx = start_indices[i]
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                output_list.append(x.clone()) # Use clone to avoid issues
                continue
            sub_sequence = x[:, :, start_idx : start_idx + subsample_length, :, :]
            flat_for_interp = rearrange(sub_sequence, "b c t h w -> b (c h w) t")
            resized_flat = torch.nn.functional.interpolate(
                flat_for_interp,
                size=total_time_frames,
                mode="linear",
                align_corners=False,
            )
            resized_sequence = rearrange(
                resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W
            )
            output_list.append(resized_sequence)
        return torch.cat(output_list, dim=0)
class PeakAwareBiPhasicWarp(Transform):
    r"""
    An enhancement-peak-aware temporal augmentation that warps BOTH the wash-in
    and wash-out phases independently with different random ratios.
    It finds the time of peak enhancement, splits the video, and then
    time-warps (compresses/stretches) both phases before reassembling them.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
                                                 compressing a phase. e.g., (0.6, 0.95).
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.6, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "warp_ratio_range must be valid."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generate two independent warp ratios for each requested transform:
        one for wash-in and one for wash-out.
        """
        params_list = []
        min_r, max_r = self.warp_ratio_range
        for _ in range(self.n_trans):
            washin_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            washout_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            params_list.append({
                "washin_ratio": washin_ratio,
                "washout_ratio": washout_ratio
            })
        return {"params_list": params_list}
    def _transform(self, x: torch.Tensor, params_list: list[dict], **kwargs) -> torch.Tensor:
        """Applies the independent, bi-phasic warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        mean_signal_curve = torch.mean(x, dim=(0, 1, 3, 4)) # Avg over B, C, H, W
        peak_idx = torch.argmax(mean_signal_curve)
        if peak_idx <= 0 or peak_idx >= x.shape[2] - 1:
            return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for params in params_list:
            wash_in_phase = x[:, :, :peak_idx, :, :]
            peak_frame = x[:, :, peak_idx:peak_idx+1, :, :]
            wash_out_phase = x[:, :, peak_idx+1:, :, :]
            warped_wash_in = self._warp_phase(wash_in_phase, params["washin_ratio"])
            warped_wash_out = self._warp_phase(wash_out_phase, params["washout_ratio"])
            new_x = torch.cat([warped_wash_in, peak_frame, warped_wash_out], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to subsample and interpolate a video phase."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase <= 1: # Cannot warp a single frame or empty tensor
            return phase_tensor
        subsample_len = max(1, int(T_phase * ratio)) # Ensure at least 1 frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        subsampled_flat = F.interpolate(flat_for_interp, size=subsample_len, mode='linear', align_corners=False)
        resized_flat = F.interpolate(subsampled_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class MonophasicTimeWarp(Transform):
    r"""
    A temporal augmentation specifically designed for monophasic enhancement curves
    (e.g., persistent or plateau types) where there is no wash-out phase.
    This transform keeps the first (pre-contrast) frame fixed and applies a
    single, smooth time-warp to the entire subsequent enhancement phase.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
        compressing/stretching the enhancement phase. e.g., (0.7, 1.3).
        Values < 1 compress time, values > 1 stretch time.
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.7, 1.3), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r, "warp_ratio_range must be a valid positive range."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single random warp ratio for the entire enhancement phase.
        """
        min_r, max_r = self.warp_ratio_range
        ratios = [min_r + (max_r - min_r) * torch.rand(1, generator=self.rng) for _ in range(self.n_trans)]
        return {"ratios": ratios}
    def _transform(self, x: torch.Tensor, ratios: list[float], **kwargs) -> torch.Tensor:
        """Applies the monophasic time warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        if x.shape[2] <= 1: # Cannot warp if there's only one frame
             return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for ratio in ratios:
            pre_contrast_frame = x[:, :, :1, :, :]
            enhancement_phase = x[:, :, 1:, :, :]
            warped_enhancement_phase = self._warp_phase(enhancement_phase, ratio)
            new_x = torch.cat([pre_contrast_frame, warped_enhancement_phase], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to interpolate a video phase to a new length."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase == 0:
            return phase_tensor
        ratio = ratio.item()
        new_length = int(round(T_phase * ratio))
        if new_length == 0: new_length = 1 # Ensure at least one frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        resized_flat = F.interpolate(flat_for_interp, size=new_length, mode='linear', align_corners=False)
        if new_length != T_phase:
            resized_flat = F.interpolate(resized_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class TemporalNoise(Transform):
    """ 
    Adds low-frequency random noise to the temporal signal of a video.
    This simulates smooth, slowly varying noise sources over time.
    """
    def __init__(self, *args, noise_strength: float = 0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        self.noise_strength = noise_strength
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single low-frequency noise vector for the transformation.
        """
        B, C, T, H, W = x.shape
        low_res_T = max(1, T // 4) 
        noise_low_res = torch.randn(B, 1, low_res_T, device=x.device)
        noise_high_res = F.interpolate(noise_low_res, size=T, mode='linear', align_corners=False)
        noise_norm = (noise_high_res - noise_high_res.mean(dim=-1, keepdim=True)) / (noise_high_res.std(dim=-1, keepdim=True) + 1e-8)
        final_noise = noise_norm * self.noise_strength
        return {'noise': final_noise}
    def _transform(self, x: torch.Tensor, noise: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the pre-generated noise to the image tensor.
        """
        B, C, T, H, W = x.shape
        x_noisy = x + noise.view(B, 1, T, 1, 1)
        return x_noisy
class TimeReverse(Transform):
    r"""
    Reverses the temporal order of frames in a video tensor.
    This transform flips the video along the time axis, effectively playing it
    backwards. This is a deterministic transformation.
    :param int n_trans: Number of transformed versions to generate per input image.
                        Since this is deterministic, it will just repeat the same
                        output if n_trans > 1.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        No random parameters are needed for time reversal as it's a
        deterministic operation.
        """
        return {}
    def _transform(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the time reversal transformation using torch.flip().
        """
        if len(x.shape) != 5:
            raise ValueError(f"TimeReverse expects a 5D tensor (B, C, T, H, W), but got shape {x.shape}.")
        return torch.flip(x, dims=[2])

=== utils.py ===
import os
import subprocess
import matplotlib.pyplot as plt
import torch
import numpy as np
from einops import rearrange
import torchkbnufft as tkbn
import csv
def log_gradient_stats(model, epoch, iteration, output_dir, log_filename="gradient_stats.csv"):
    """
    Computes, prints, and logs the L2 norm of gradients for each parameter and the total gradient norm.
    Args:
        model (torch.nn.Module): The model being trained.
        epoch (int): The current epoch.
        iteration (int): The current global iteration/step count.
        output_dir (str): The main experiment output directory.
        log_filename (str): The CSV filename for storing detailed logs.
    """
    total_norm = 0.0
    param_norms = []
    for name, p in model.named_parameters():
        if p.grad is not None and p.requires_grad:
            param_norm = p.grad.data.norm(2)
            if not torch.isfinite(param_norm):
                param_norm_item = float('inf')
            else:
                param_norm_item = param_norm.item()
            param_norms.append((name, param_norm_item))
            total_norm += param_norm_item ** 2
    total_norm = total_norm ** 0.5
    print(f"--- Gradient Stats (Epoch {epoch}, Iter {iteration}) ---")
    print(f"Total Gradient Norm: {total_norm:.4e}")
    param_norms.sort(key=lambda x: x[1], reverse=True)
    print("Top 5 layers with largest gradients:")
    for name, norm in param_norms[:5]:
        print(f"  - {name}: {norm:.4e}")
    print("Top 5 layers with smallest gradients:")
    non_zero_norms = [p for p in param_norms if p[1] > 0]
    for name, norm in non_zero_norms[-5:]:
        print(f"  - {name}: {norm:.4e}")
    print("-------------------------------------------------")
    log_path = os.path.join(output_dir, log_filename)
    file_exists = os.path.isfile(log_path)
    with open(log_path, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow(['epoch', 'iteration', 'total_norm', 'param_name', 'param_norm'])
        writer.writerow([epoch, iteration, total_norm, '---TOTAL---', total_norm])
        for name, norm in param_norms:
            writer.writerow([epoch, iteration, total_norm, name, norm])
def trajGR(Nkx, Nspokes):
    '''
    function for generating golden-angle radial sampling trajectory
    :param Nkx: spoke length
    :param Nspokes: number of spokes
    :return: ktraj: golden-angle radial sampling trajectory
    '''
    ga = np.pi * ((1 - np.sqrt(5)) / 2)
    kx = np.zeros(shape=(Nkx, Nspokes))
    ky = np.zeros(shape=(Nkx, Nspokes))
    ky[:, 0] = np.linspace(-np.pi, np.pi, Nkx)
    for i in range(1, Nspokes):
        kx[:, i] = np.cos(ga) * kx[:, i - 1] - np.sin(ga) * ky[:, i - 1]
        ky[:, i] = np.sin(ga) * kx[:, i - 1] + np.cos(ga) * ky[:, i - 1]
    ky = np.transpose(ky)
    kx = np.transpose(kx)
    ktraj = np.stack((ky.flatten(), kx.flatten()), axis=0)
    return ktraj
def prep_nufft(Nsample, Nspokes, Ng):
    overSmaple = 2
    im_size = (int(Nsample/overSmaple), int(Nsample/overSmaple))
    grid_size = (Nsample, Nsample)
    ktraj = trajGR(Nsample, Nspokes * Ng)
    ktraj = torch.tensor(ktraj, dtype=torch.float)
    dcomp = tkbn.calc_density_compensation_function(ktraj=ktraj, im_size=im_size)
    dcomp = dcomp.squeeze()
    ktraju = np.zeros([2, Nspokes * Nsample, Ng], dtype=float)
    dcompu = np.zeros([Nspokes * Nsample, Ng], dtype=complex)
    for ii in range(0, Ng):
        ktraju[:, :, ii] = ktraj[:, (ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
        dcompu[:, ii] = dcomp[(ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
    ktraju = torch.tensor(ktraju, dtype=torch.float)
    dcompu = torch.tensor(dcompu, dtype=torch.complex64)
    nufft_ob = tkbn.KbNufft(im_size=im_size, grid_size=grid_size)  # forward nufft
    adjnufft_ob = tkbn.KbNufftAdjoint(im_size=im_size, grid_size=grid_size)  # backward nufft
    return ktraju, dcompu, nufft_ob, adjnufft_ob
def _calculate_top_percentile_curve(dynamic_slice: torch.Tensor, percentile: float) -> list[float]:
    """Helper function to calculate the enhancement curve for a single dynamic slice."""
    if dynamic_slice.dim() != 5 or dynamic_slice.shape[0] != 1 or dynamic_slice.shape[1] != 2:
        raise ValueError(f"Expected input shape (1, 2, T, H, W), but got {dynamic_slice.shape}")
    magnitude_video = torch.sqrt(dynamic_slice[:, 0, ...] ** 2 + dynamic_slice[:, 1, ...] ** 2).squeeze(0)
    num_time_frames = magnitude_video.shape[0]
    top_percentile_means = []
    q = percentile / 100.0
    for t in range(num_time_frames):
        frame_t = magnitude_video[t, :, :]
        if frame_t.max() == 0:
            top_percentile_means.append(0)
            continue
        threshold = torch.quantile(frame_t.flatten(), q)
        bright_pixels = frame_t[frame_t > threshold]
        mean_val = torch.mean(bright_pixels) if bright_pixels.numel() > 0 else threshold
        top_percentile_means.append(mean_val.item())
    return top_percentile_means
def plot_enhancement_curve(
    model_output: torch.Tensor,
    percentile: float = 99.0,
    title: str = "Enhancement Curve Comparison",
    output_filename: str = None
):
    """
    Calculates and plots the enhancement curves for a model output and a benchmark
    image on the same graph for direct comparison.
    Args:
        model_output (torch.Tensor): The model's reconstructed dynamic slice.
                                     Shape (1, 2, T, H, W).
        benchmark_image (torch.Tensor): The ground truth or benchmark dynamic slice.
                                        Shape (1, 2, T, H, W).
        percentile (float, optional): The percentile for defining the brightest pixels.
                                      Defaults to 99.0.
        title (str, optional): The title for the plot. Defaults to "Enhancement Curve Comparison".
        output_filename (str, optional): If provided, saves the plot to this file path.
                                         Defaults to None (displays plot).
    """
    if not 0 < percentile < 100:
        raise ValueError("Percentile must be between 0 and 100.")
    model_curve = _calculate_top_percentile_curve(model_output.detach(), percentile)
    num_time_frames = model_output.shape[2]
    time_axis = np.arange(num_time_frames)
    plt.figure(figsize=(12, 7))
    plt.plot(time_axis, model_curve, label='Model Output', marker='o', linestyle='-', color='tab:blue')
    plt.title(title, fontsize=16)
    plt.xlabel("Time Frame", fontsize=12)
    plt.ylabel(f"Mean Signal of Top {100-percentile:.1f}% Pixels", fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    if output_filename:
        output_dir = os.path.dirname(output_filename)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        plt.savefig(output_filename)
    else:
        plt.show()
    plt.close()
def get_cosine_ei_weight(
    current_epoch,
    warmup_epochs,
    schedule_duration,
    target_weight
):
    """
    Calculates the EI loss weight for the current epoch using a cosine schedule.
    This implements a curriculum learning strategy:
    1. For `warmup_epochs`, the weight is 0 (MC loss only).
    2. Over the next `schedule_duration` epochs, the weight smoothly ramps
       up from 0 to `target_weight` following a cosine curve.
    3. After the schedule is complete, the weight stays at `target_weight`.
    Args:
        current_epoch (int): The current training epoch (starting from 1).
        warmup_epochs (int): Number of epochs to train with only MC loss.
        schedule_duration (int): Number of epochs for the ramp-up.
        target_weight (float): The final EI loss weight to reach.
    Returns:
        float: The EI loss weight for the current epoch.
    """
    if current_epoch <= warmup_epochs:
        return 0.0
    schedule_progress_epoch = current_epoch - warmup_epochs
    if schedule_progress_epoch >= schedule_duration:
        return target_weight
    cosine_multiplier = 0.5 * (1 - np.cos(np.pi * schedule_progress_epoch / schedule_duration))
    return target_weight * cosine_multiplier
def plot_reconstruction_sample(x_recon, title, filename, output_dir, grasp_img=None, batch_idx=0, transform=False):
    """
    Plot reconstruction sample showing magnitude images across timeframes.
    Args:
        x_recon: Reconstructed image tensor of shape (B, C, T, H, W)
        title: Title for the plot
        filename: Filename for saving (without extension)
        output_dir: Directory to save the plot
        batch_idx: Which batch element to plot (default: 0)
    """
    os.makedirs(output_dir, exist_ok=True)
    if x_recon.shape[1] == 2:
        x_recon_mag = torch.sqrt(x_recon[:, 0, ...] ** 2 + x_recon[:, 1, ...] ** 2)
    else:
        x_recon_mag = x_recon
    grasp_img_mag = torch.sqrt(grasp_img[:, 0, ...] ** 2 + grasp_img[:, 1, ...] ** 2)
    if grasp_img_mag.shape[-1] == 320 and grasp_img_mag.shape[-2] == 320:
        n_timeframes = grasp_img_mag.shape[1]
    elif grasp_img_mag.shape[-1] == 320 and grasp_img_mag.shape[1] == 320:
        n_timeframes = grasp_img_mag.shape[-2]
    else:
        n_timeframes = grasp_img_mag.shape[-1]
    fig, axes = plt.subplots(
        nrows=2,
        ncols=n_timeframes,
        figsize=(n_timeframes * 3, 8),
        squeeze=False,
    )
    if transform:
        axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        os.makedirs(os.path.join(output_dir, "transforms"), exist_ok=True)
    else:
        axes[0, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("GRASP Benchmark", fontsize=14, labelpad=10)
    for t in range(n_timeframes):
        if x_recon_mag.shape[1] == n_timeframes:
            img = x_recon_mag[batch_idx, t, :, :].cpu().detach().numpy()
        else:
            img = x_recon_mag[batch_idx, ..., t].cpu().detach().numpy()
        if grasp_img_mag.shape[1] == n_timeframes:
            grasp_img = grasp_img_mag[batch_idx, t, :, :].cpu().detach().numpy()
        elif grasp_img_mag.shape[-1] == n_timeframes:
            grasp_img = grasp_img_mag[batch_idx, :, :, t].cpu().detach().numpy()
        else:
            grasp_img = grasp_img_mag[batch_idx, :, t, :].cpu().detach().numpy()
        ax1 = axes[0, t]
        ax1.imshow(np.rot90(img, 2), cmap="gray")
        ax1.set_title(f"t = {t}")
        ax1.set_xticks([])
        ax1.set_yticks([])
        ax2 = axes[1, t]
        ax2.imshow(grasp_img, cmap="gray")
        ax2.set_title(f"t = {t}")
        ax2.set_xticks([])
        ax2.set_yticks([])
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(output_dir, f"{filename}.png"))
    plt.close(fig)
def get_git_commit():
    try:
        commit_hash = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        return commit_hash
    except Exception as e:
        print(f"Error retrieving Git commit: {e}")
        return "unknown"
def remove_module_prefix(state_dict):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('module.', '')  # Remove 'module.' prefix
        new_state_dict[new_key] = v
    return new_state_dict
def save_checkpoint(model, optimizer, epoch,
                    train_curves, val_curves, eval_curves, filename):
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        **train_curves,   # unpack the dicts
        **val_curves,
        **eval_curves,
    }
    torch.save(checkpoint, filename)
    print(f"Checkpoint saved at epoch {epoch} to {filename}")
def load_checkpoint(model, optimizer, filename):
    ckpt = torch.load(filename, map_location="cpu")
    model.load_state_dict(remove_module_prefix(ckpt["model_state_dict"]))
    optimizer.load_state_dict(remove_module_prefix(ckpt["optimizer_state_dict"]))
    train_curves = {
        "train_mc_losses": ckpt.get("train_mc_losses", []),
        "train_ei_losses": ckpt.get("train_ei_losses", []),
        "train_adj_losses": ckpt.get("train_adj_losses", []),
        "weighted_train_mc_losses": ckpt.get("weighted_train_mc_losses", []),
        "weighted_train_ei_losses": ckpt.get("weighted_train_ei_losses", []),
        "weighted_train_adj_losses": ckpt.get("weighted_train_adj_losses", []),
    }
    val_curves = {
        "val_mc_losses": ckpt.get("val_mc_losses", []),
        "val_ei_losses": ckpt.get("val_ei_losses", []),
        "val_adj_losses": ckpt.get("val_adj_losses", []),
    }
    eval_curves = {
        "eval_ssims": ckpt.get("eval_ssims", []),
        "eval_psnrs": ckpt.get("eval_psnrs", []),
        "eval_mses": ckpt.get("eval_mses", []),
        "eval_dcs": ckpt.get("eval_dcs", []),
    }
    return model, optimizer, ckpt.get("epoch", 1), train_curves, val_curves, eval_curves
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())

