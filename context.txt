=== train.py ===
import argparse
import json
import os
import matplotlib.pyplot as plt
import torch
import yaml
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet_encoding import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex, GRASPRecon, sliding_window_inference, set_seed
from eval import eval_grasp, eval_sample
import csv
import math
import random
import time 
import seaborn as sns
from loss_metrics import LPIPSVideoMetric, SSIMVideoMetric
set_seed(12)
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
eval_dir = os.path.join(output_dir, "eval_results")
os.makedirs(eval_dir, exist_ok=True)
block_dir = os.path.join(output_dir, "block_outputs")
os.makedirs(block_dir, exist_ok=True)
ec_dir = os.path.join(output_dir, 'enhancement_curves')
os.makedirs(ec_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                   'lambda_S': config['model']['lambda_S'], 
                   'lambda_spatial_L': config['model']['lambda_spatial_L'],
                   'lambda_spatial_S': config['model']['lambda_spatial_S'],
                   'gamma': config['model']['gamma'],
                   'lambda_step': config['model']['lambda_step']}
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
model_type = config["model"]["name"]
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"]
)
Ng = config["data"]["fpg"] 
N_spokes = int(config["data"]["total_spokes"] / N_time)
N_full = config['data']['height'] * math.pi / 2
eval_chunk_size = config["evaluation"]["chunk_size"]
eval_chunk_overlap = config["evaluation"]["chunk_overlap"]
if config["data"]["train_spokes_per_frame"] != "None":
    train_spokes_per_frame = config["data"]["train_spokes_per_frame"]
else:
    train_spokes_per_frame = None
curriculum_enabled = config['training']['curriculum_learning']['enabled']
curriculum_phases = config['training']['curriculum_learning']['phases']
initial_train_spokes_range = [8, 16, 24, 36]
if curriculum_enabled:
    if not curriculum_phases:
        raise ValueError("Curriculum learning enabled but no phases defined in config.yaml")
    initial_train_spokes_range = curriculum_phases[0]['train_spokes_range']
    print(f"Curriculum Learning Enabled. Initial training with spokes range: {initial_train_spokes_range}")
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
else:
    train_patient_ids = splits["train"]
val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]
for val_id in val_patient_ids:
    if val_id in train_patient_ids:
        raise ValueError(f"Data Leakage encountered! Duplicate sample in train and val patient IDs: {val_id}")
if config['dataloader']['slice_range_start'] == "None" or config['dataloader']['slice_range_end'] == "None":
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        num_random_slices=config["dataloader"].get("num_random_slices", None),
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
        spokes_per_frame=train_spokes_per_frame,
        weight_accelerations=config['data']['weight_accelerations'],
        initial_spokes_range=initial_train_spokes_range
    )
else:
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=range(config['dataloader']['slice_range_start'], config['dataloader']['slice_range_end']),
        num_random_slices=config["dataloader"].get("num_random_slices", None),
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
        spokes_per_frame=train_spokes_per_frame,
        weight_accelerations=config['data']['weight_accelerations'],
        initial_spokes_range=initial_train_spokes_range
    )
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)
g = torch.Generator()
g.manual_seed(12)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
    pin_memory=True,
    worker_init_fn=seed_worker,
    generator=g,
)
if curriculum_enabled:
    N_spokes_eval = curriculum_phases[0]['eval_spokes_per_frame']
    N_time_eval = curriculum_phases[0]['eval_num_frames']
else:
    N_time_eval, N_spokes_eval = config["data"]["eval_timeframes"], config["data"]["eval_spokes"]
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)
eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    spokes_per_frame=N_spokes_eval,
    num_frames=N_time_eval)
val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=False,
    num_workers=config["dataloader"]["num_workers"],
    pin_memory=True,
)
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], 
                        lambdas=initial_lambdas, 
                        channels=config['model']['channels'],
                        style_dim=config['model']['style_dim'],
                        svd_mode=config['model']['svd_mode'],
                        use_lowk_dc=config['model']['use_lowk_dc'],
                        lowk_frac=config['model']['lowk_frac'],
                        lowk_alpha=config['model']['lowk_alpha'],
                        film_bounded=config['model']['film_bounded'],
                        film_gain=config['model']['film_gain'],
                        film_identity_init=config['model']['film_identity_init'],
                        svd_noise_std=config['model']['svd_noise_std'],
                        film_L=config['model']['film_L'],
                        )
if config['model']['encode_acceleration'] and config['model']['encode_time_index']:
    model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir, channels=2).to(device)
else:
    model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir, channels=1).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, target_w_ei, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
else:
    start_epoch = 1
    target_w_ei = 0.0
if config['model']['losses']['mc_loss']['metric'] == "MSE":
    mc_loss_fn = MCLoss(model_type=model_type)
elif config['model']['losses']['mc_loss']['metric'] == "MAE":
    mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
else:
    raise(ValueError, "Unsupported MC Loss Metric.")
if config['model']['losses']['ei_loss']['metric'] == "LPIPS":
    ei_loss_metric = LPIPSVideoMetric(net_type='alex') 
elif config['model']['losses']['ei_loss']['metric'] == "SSIM":
    ei_loss_metric = SSIMVideoMetric()
else:
    ei_loss_metric = torch.nn.MSELoss()
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(subsample, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(subsample | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(monophasic_warp, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise, metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp_subsample":
        ei_loss_fn = EILoss((subsample | monophasic_warp) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
            ei_loss_fn = EILoss(rotate, metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
            ei_loss_fn = EILoss(diffeo, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(rotate | diffeo, metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['spatial_transform'] == "all":
        if config['model']['losses']['ei_loss']['temporal_transform'] == "all":
            ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    train_adj_losses = train_curves["train_adj_losses"]
    val_adj_losses = val_curves["val_adj_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
    weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
    eval_ssims = eval_curves["eval_ssims"]
    eval_psnrs = eval_curves["eval_psnrs"]
    eval_mses = eval_curves["eval_mses"]
    eval_lpipses = eval_curves["eval_lpipses"]
    eval_dc_mses = eval_curves["eval_dc_mses"]
    eval_dc_maes = eval_curves["eval_dc_maes"]
    eval_curve_corrs = eval_curves["eval_curve_corrs"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    train_adj_losses = []
    val_adj_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
    weighted_train_adj_losses = []
    eval_ssims = []
    eval_lpipses = []
    eval_psnrs = []
    eval_mses = []
    eval_dc_mses = []
    eval_dc_maes = []
    eval_curve_corrs = []
grasp_ssims = []
grasp_psnrs = []
grasp_mses = []
grasp_lpipses = []
grasp_dc_mses = []
grasp_dc_maes = []
grasp_curve_corrs = []
lambda_Ls = []
lambda_Ss = []
lambda_spatial_Ls = []
lambda_spatial_Ss = []
gammas = []
lambda_steps = []
iteration_count = 0
if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    initial_train_adj_loss = 0.0
    initial_val_adj_loss = 0.0
    initial_eval_ssims = []
    initial_eval_psnrs = []
    initial_eval_mses = []
    initial_eval_lpipses = []
    initial_eval_dc_mses = []
    initial_eval_dc_maes = []
    initial_eval_curve_corrs = []
    with torch.no_grad():
        for measured_kspace, csmap, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            if N_time > Ng:
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
                ktraj_chunk = ktraj[..., random_index:random_index + Ng]
                dcomp_chunk = dcomp[..., random_index:random_index + Ng]
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
                start_timepoint_index = torch.tensor([random_index], dtype=torch.float, device=device)
            else:
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if config['model']['encode_time_index'] == False:
                start_timepoint_index = None
            x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                measured_kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch="train0", norm=config['model']['norm']
            )
            initial_train_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap, acceleration_encoding, start_timepoint_index
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
        train_adj_losses.append(step0_train_adj_loss)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        for measured_kspace, csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            if type(measured_kspace) is list:
                ground_truth_for_physics = rearrange(to_torch_complex(ground_truth), 'b t h w -> b h w t')
                kspace_path = measured_kspace[0]
                measured_kspace = eval_physics(False, ground_truth_for_physics, csmap)
                np.save(kspace_path, measured_kspace.cpu().numpy())
            measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                grasp_img = GRASPRecon(csmap, measured_kspace, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            N_spokes = eval_ktraj.shape[1] / config['data']['samples']
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if config['model']['encode_time_index'] == False:
                start_timepoint_index = None
            else:
                start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
            if N_time_eval > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, measured_kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch="val0", device=device)  
            else:
                x_recon, adj_loss, *_ = model(
                measured_kspace.to(device), eval_physics, csmap, acceleration_encoding, start_timepoint_index, epoch="val0", norm=config['model']['norm']
                )
            initial_val_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, eval_physics, model, csmap, acceleration_encoding, start_timepoint_index
                )
                initial_val_ei_loss += ei_loss.item()
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device, eval_dir)
            grasp_ssims.append(ssim_grasp)
            grasp_psnrs.append(psnr_grasp)
            grasp_mses.append(mse_grasp)
            grasp_lpipses.append(lpips_grasp)
            grasp_dc_mses.append(dc_mse_grasp)
            grasp_dc_maes.append(dc_mae_grasp)
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(measured_kspace, csmap, ground_truth, x_recon, eval_physics, mask, grasp_recon, acceleration, int(N_spokes), eval_dir, label='val0', device=device)
            initial_eval_ssims.append(ssim)
            initial_eval_psnrs.append(psnr)
            initial_eval_mses.append(mse)
            initial_eval_lpipses.append(lpips)
            initial_eval_dc_mses.append(dc_mse)
            initial_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                initial_eval_curve_corrs.append(recon_corr)
                grasp_curve_corrs.append(grasp_corr)
        step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
        val_ei_losses.append(step0_val_ei_loss)
        step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
        val_adj_losses.append(step0_val_adj_loss)
        initial_eval_ssim = np.mean(initial_eval_ssims)
        initial_eval_psnr = np.mean(initial_eval_psnrs)
        initial_eval_mse = np.mean(initial_eval_mses)
        initial_eval_lpips = np.mean(initial_eval_lpipses)
        initial_eval_dc_mse = np.mean(initial_eval_dc_mses)
        initial_eval_dc_mae = np.mean(initial_eval_dc_maes)
        initial_eval_curve_corr = np.mean(initial_eval_curve_corrs)
        eval_ssims.append(initial_eval_ssim)
        eval_psnrs.append(initial_eval_psnr)
        eval_mses.append(initial_eval_mse)
        eval_lpipses.append(initial_eval_lpips)
        eval_dc_mses.append(initial_eval_dc_mse) 
        eval_dc_maes.append(initial_eval_dc_mae) 
        eval_curve_corrs.append(initial_eval_curve_corr)
    print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
    print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
svd_fail_count = 0
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    current_curriculum_phase_idx = -1
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        running_adj_loss = 0.0
        epoch_eval_ssims = []
        epoch_eval_psnrs = []
        epoch_eval_mses = []
        epoch_eval_lpipses = []
        epoch_eval_dc_mses = []
        epoch_eval_dc_maes = []
        epoch_eval_curve_corrs = []
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        if hasattr(train_dataset, 'resample_slices'):
            print(f"Epoch {epoch}: Resampling training slices...")
            train_dataset.resample_slices()
        if curriculum_enabled:
            for i, phase in enumerate(curriculum_phases):
                if epoch >= phase['start_epoch']:
                    if i > current_curriculum_phase_idx: # Transition to a new phase
                        print(f"\n--- Entering Curriculum Phase: {phase['name']} at Epoch {epoch} ---")
                        train_dataset.spokes_range = phase['train_spokes_range']
                        train_dataset.update_spokes_weights()
                        current_curriculum_phase_idx = i
                        N_spokes_eval = phase['eval_spokes_per_frame']
                        N_time_eval = phase['eval_num_frames']
                        eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
                        eval_ktraj = eval_ktraj.to(device)
                        eval_dcomp = eval_dcomp.to(device)
                        eval_nufft_ob = eval_nufft_ob.to(device)
                        eval_adjnufft_ob = eval_adjnufft_ob.to(device)
                        eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
                        val_dro_dataset.spokes_per_frame = N_spokes_eval
                        val_dro_dataset.num_frames = N_time_eval
                        val_dro_dataset._update_sample_paths()
                        val_dro_loader = DataLoader(
                            val_dro_dataset,
                            batch_size=config["dataloader"]["batch_size"],
                            shuffle=False,
                            num_workers=config["dataloader"]["num_workers"],
                            pin_memory=True,
                        )
        if use_ei_loss:
            if epoch < warmup + 1:
                target_w_ei = 0.0
            elif epoch == warmup + 1:
                mc_loss_at_transition = epoch_train_mc_loss
                print(f"Transitioning at Epoch {epoch}. MC Loss: {mc_loss_at_transition:.4e}")
                if step0_train_ei_loss > 0:
                    target_w_ei = mc_loss_at_transition / step0_train_ei_loss
                else:
                    target_w_ei = 0.0 # Prevent division by zero
                print(f"Dynamically calculated target EI weight: {target_w_ei:.4f}")
        for measured_kspace, csmap, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            print("spokes per frame: ", N_spokes)
            start = time.time()
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            if N_time > Ng:
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
                ktraj_chunk = ktraj[..., random_index:random_index + Ng]
                dcomp_chunk = dcomp[..., random_index:random_index + Ng]
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
                start_timepoint_index = torch.tensor([random_index], dtype=torch.float, device=device)
            else:
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
            iteration_count += 1
            optimizer.zero_grad()
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if config['model']['encode_time_index'] == False:
                start_timepoint_index = None
            try:
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=f"train{epoch}", norm=config['model']['norm']
                )
                running_adj_loss += adj_loss.item()
                mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
                running_mc_loss += mc_loss.item()
                if use_ei_loss:
                    ei_loss, t_img = ei_loss_fn(
                        x_recon, physics, model, csmap, acceleration_encoding, start_timepoint_index
                    )
                    ei_loss_weight = get_cosine_ei_weight(
                        current_epoch=epoch,
                        warmup_epochs=warmup,
                        schedule_duration=duration,
                        target_weight=target_w_ei
                    )
                    running_ei_loss += ei_loss.item()
                    total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                    train_loader_tqdm.set_postfix(
                        mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                    )
                else:
                    total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                    train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
                if torch.isnan(total_loss):
                    print(
                        "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                    )
                    raise RuntimeError("total_loss is NaN")
                total_loss.backward()
                if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                    log_gradient_stats(
                        model=model,
                        epoch=epoch,
                        iteration=iteration_count,
                        output_dir=output_dir,
                        log_filename="gradient_stats.csv"
                    )
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                end = time.time()
                print("time for one iteration: ", end-start)
            except RuntimeError as e:
                if "svd" in str(e).lower():
                    svd_fail_count += 1
                    optimizer.zero_grad()
                    print(f"[Warning] Skipping batch {iteration_count} in epoch {epoch} due to SVD failure. "
                        f"Total failures so far: {svd_fail_count}")
                    continue  # skip this batch, go to next one
                else:
                    raise  # re-raise other errors
        if epoch % save_interval == 0:
            plot_reconstruction_sample(
                x_recon,
                f"Training Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                f"train_sample_epoch_{epoch}",
                output_dir,
            )
            x_recon_reshaped = rearrange(x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'train_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    t_img,
                    f"Transformed Train Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                    f"transforms/transform_train_sample_epoch_{epoch}",
                    output_dir,
                    x_recon,
                    transform=True
                )
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        epoch_train_adj_loss = running_adj_loss / len(train_loader)
        train_adj_losses.append(epoch_train_adj_loss)
        weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_running_adj_loss = 0.0
        val_loader_tqdm = tqdm(
            val_dro_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask, grasp_path in tqdm(val_dro_loader):
                val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                if type(val_kspace_batch) is list:
                    ground_truth_for_physics = rearrange(to_torch_complex(val_ground_truth), 'b t h w -> b h w t')
                    kspace_path = val_kspace_batch[0]
                    val_kspace_batch = eval_physics(False, ground_truth_for_physics, val_csmap)
                val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                if type(val_grasp_img) is int or len(val_grasp_img.shape) == 1:
                    print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                    val_grasp_img = GRASPRecon(val_csmap, val_kspace_batch, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                    val_grasp_img = torch.from_numpy(val_grasp_img).permute(2, 0, 1) # T, H, W
                    val_grasp_img = torch.stack([val_grasp_img.real, val_grasp_img.imag], dim=0)
                    val_grasp_img = torch.flip(val_grasp_img, dims=[-3])
                    val_grasp_img = torch.rot90(val_grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                val_grasp_img_tensor = val_grasp_img.to(device)
                N_spokes = eval_ktraj.shape[1] / config['data']['samples']
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                if config['model']['encode_time_index'] == False:
                    start_timepoint_index = None
                else:
                    start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                try:
                    if N_time_eval > eval_chunk_size:
                        print("Performing sliding window eval...")
                        val_x_recon, val_adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, val_kspace_batch, val_csmap, acceleration_encoding, start_timepoint_index, model, epoch=f"val{epoch}", device=device)  
                    else:
                        val_x_recon, val_adj_loss, *_ = model(
                        val_kspace_batch.to(device), eval_physics, val_csmap, acceleration_encoding, start_timepoint_index, epoch=f"val{epoch}", norm=config['model']['norm']
                        )
                    val_running_adj_loss += val_adj_loss.item()
                    val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                    val_running_mc_loss += val_mc_loss.item()
                    if use_ei_loss:
                        val_ei_loss, val_t_img = ei_loss_fn(
                            val_x_recon, eval_physics, model, val_csmap, acceleration_encoding, start_timepoint_index
                        )
                        val_running_ei_loss += val_ei_loss.item()
                        val_loader_tqdm.set_postfix(
                            val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                        )
                    else:
                        val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                    ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, _ = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, acceleration, int(N_spokes), eval_dir, f'epoch{epoch}', device)
                    epoch_eval_ssims.append(ssim)
                    epoch_eval_psnrs.append(psnr)
                    epoch_eval_mses.append(mse)
                    epoch_eval_lpipses.append(lpips)
                    epoch_eval_dc_mses.append(dc_mse)
                    epoch_eval_dc_maes.append(dc_mae)
                    if recon_corr is not None:
                        epoch_eval_curve_corrs.append(recon_corr)
                except RuntimeError as e:
                    if "svd" in str(e).lower():
                        svd_fail_count += 1
                        optimizer.zero_grad()
                        print(f"[Warning] Skipping batch validation sample in epoch {epoch} due to SVD failure. "
                            f"Total failures so far: {svd_fail_count}")
                        continue  # skip this batch, go to next one
                    else:
                        raise  # re-raise other errors
        epoch_eval_ssim = np.mean(epoch_eval_ssims)
        epoch_eval_psnr = np.mean(epoch_eval_psnrs)
        epoch_eval_mse = np.mean(epoch_eval_mses)
        epoch_eval_lpips = np.mean(epoch_eval_lpipses)
        epoch_eval_dc_mse = np.mean(epoch_eval_dc_mses)
        epoch_eval_dc_mae = np.mean(epoch_eval_dc_maes)
        epoch_eval_curve_corr = np.mean(epoch_eval_curve_corrs)
        eval_ssims.append(epoch_eval_ssim)
        eval_psnrs.append(epoch_eval_psnr)
        eval_mses.append(epoch_eval_mse)
        eval_lpipses.append(epoch_eval_lpips)
        eval_dc_mses.append(epoch_eval_dc_mse) 
        eval_dc_maes.append(epoch_eval_dc_mae)    
        eval_curve_corrs.append(epoch_eval_curve_corr)  
        if epoch % save_interval == 0:
            plot_reconstruction_sample(
                val_x_recon,
                f"Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                f"val_sample_epoch_{epoch}",
                output_dir,
                val_grasp_img
            )
            val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                val_x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
            plot_enhancement_curve(
                val_grasp_img,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
        epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            val_adj_losses.append(epoch_val_adj_loss)
        else:
            val_adj_losses.append(0.0)
        if epoch % save_interval == 0:
            train_curves = dict(
                train_mc_losses=train_mc_losses,
                train_ei_losses=train_ei_losses,
                weighted_train_mc_losses=weighted_train_mc_losses,
                weighted_train_ei_losses=weighted_train_ei_losses,
            )
            val_curves = dict(
                val_mc_losses=val_mc_losses,
                val_ei_losses=val_ei_losses,
            )
            eval_curves = dict(
                eval_ssims=eval_ssims,
                eval_psnrs=eval_psnrs,
                eval_mses=eval_mses,
                eval_lpipses=eval_lpipses,
                eval_dc_mses=eval_dc_mses,
                eval_dc_maes=eval_dc_maes,
                eval_curve_corrs=eval_curve_corrs
            )
            model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
            save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, target_w_ei, model_save_path)
            print(f'Model saved to {model_save_path}')
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            sns.lineplot(x=range(len(train_adj_losses)), y=train_adj_losses, ax=axes[0, 0])
            axes[0, 0].set_title("Training Adjoint Loss")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("Adjoint Loss")
            sns.lineplot(x=range(len(train_mc_losses)), y=train_mc_losses, ax=axes[0, 1])
            axes[0, 1].set_title("Training MC Loss")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("MC Loss")
            sns.lineplot(x=range(len(train_ei_losses)), y=train_ei_losses, ax=axes[0, 2])
            axes[0, 2].set_title("Training EI Loss")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("EI Loss")
            sns.lineplot(x=range(len(val_adj_losses)), y=val_adj_losses, ax=axes[1, 0], color='orange')
            axes[1, 0].set_title(f"Validation Adjoint Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("Adjoint Loss")
            sns.lineplot(x=range(len(val_mc_losses)), y=val_mc_losses, ax=axes[1, 1], color='orange')
            axes[1, 1].set_title(f"Validation MC Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("MC Loss")
            sns.lineplot(x=range(len(val_ei_losses)), y=val_ei_losses, ax=axes[1, 2], color='orange')
            axes[1, 2].set_title(f"Validation EI Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("EI Loss")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "losses.png"))
            plt.close()
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            sns.lineplot(x=range(len(lambda_Ls)), y=lambda_Ls, ax=axes[0, 0])
            axes[0, 0].set_title("Lambda_L Parameter Value")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("Lambda_L")
            sns.lineplot(x=range(len(lambda_Ss)), y=lambda_Ss, ax=axes[0, 1])
            axes[0, 1].set_title("Lambda_S Parameter Value")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("Lambda_S")
            sns.lineplot(x=range(len(lambda_spatial_Ls)), y=lambda_spatial_Ls, ax=axes[0, 2])
            axes[0, 2].set_title("Spatial Lambda_L Parameter Value")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("Spatial Lambda_L")
            sns.lineplot(x=range(len(lambda_spatial_Ss)), y=lambda_spatial_Ss, ax=axes[1, 0])
            axes[1, 0].set_title("Spatial Lambda_S Parameter Value")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("Spatial Lambda_S")
            sns.lineplot(x=range(len(gammas)), y=gammas, ax=axes[1, 1])
            axes[1, 1].set_title("Gamma Parameter Value")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("Gamma")
            sns.lineplot(x=range(len(lambda_steps)), y=lambda_steps, ax=axes[1, 2])
            axes[1, 2].set_title("Lambda Step Parameter Value")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("Lambda Step")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "parameters.png"))
            plt.close()
            plt.figure()
            plt.plot(weighted_train_mc_losses, label="MC Loss")
            plt.plot(weighted_train_ei_losses, label="EI Loss")
            plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Weighted Training Losses")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
            plt.close()
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle(f'Evaluation Metrics Over Epochs ({N_spokes_eval} spokes/frame)', fontsize=20)
            sns.lineplot(x=range(len(eval_ssims)), y=eval_ssims, ax=axes[0, 0])
            axes[0, 0].set_title("Evaluation SSIM")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("SSIM")
            sns.lineplot(x=range(len(eval_psnrs)), y=eval_psnrs, ax=axes[0, 1])
            axes[0, 1].set_title("Evaluation PSNR")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("PSNR")
            sns.lineplot(x=range(len(eval_mses)), y=eval_mses, ax=axes[0, 2])
            axes[0, 2].set_title("Evaluation Image MSE")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("MSE")
            sns.lineplot(x=range(len(eval_lpipses)), y=eval_lpipses, ax=axes[1, 0])
            axes[1, 0].set_title("Evaluation LPIPS")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("LPIPS")
            sns.lineplot(x=range(len(eval_dc_maes)), y=eval_dc_maes, ax=axes[1, 1])
            axes[1, 1].set_title("Evaluation k-space MAE")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("MAE")
            sns.lineplot(x=range(len(eval_curve_corrs)), y=eval_curve_corrs, ax=axes[1, 2])
            axes[1, 2].set_title("Tumor Enhancement Curve Correlation")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.savefig(os.path.join(output_dir, "eval_metrics.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dc_mses)
            plt.xlabel("Epoch")
            plt.ylabel("k-space MSE")
            plt.title("Evaluation Data Consistency (MSE)")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dc_mses.png"))
            plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
        if model_type == "LSFPNet":
            print(
                f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
            )
        print(f"--- Evaluation Metrics: Epoch {epoch} ---")
        print(f"Recon SSIM: {epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}")
        print(f"Recon PSNR: {epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}")
        print(f"Recon MSE: {epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}")
        print(f"Recon LPIPS: {epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}")
        print(f"Recon DC MSE: {epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}")
        print(f"Recon DC MAE: {epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}")
        print(f"Recon Enhancement Curve Correlation: {epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}")
        print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}")
        print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}")
        print(f"GRASP MSE: {np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}")
        print(f"GRASP LPIPS: {np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}")
        print(f"GRASP DC MSE: {np.mean(grasp_dc_mses):.6f} ± {np.std(grasp_dc_mses):.4f}")
        print(f"GRASP DC MAE: {np.mean(grasp_dc_maes):.6f} ± {np.std(grasp_dc_maes):.4f}")
        print(f"GRASP Enhancement Curve Correlation: {np.mean(grasp_curve_corrs):.6f} ± {np.std(grasp_curve_corrs):.4f}")
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    train_adj_losses=train_adj_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
    weighted_train_adj_losses=weighted_train_adj_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
    val_adj_losses=val_adj_losses,
)
eval_curves = dict(
    eval_ssims=eval_ssims,
    eval_psnrs=eval_psnrs,
    eval_mses=eval_mses,
    eval_lpipses=eval_lpipses,
    eval_dc_mses=eval_dc_mses,
    eval_dc_maes=eval_dc_maes,
    eval_curve_corrs=eval_curve_corrs,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, target_w_ei, model_save_path)
print(f'Model saved to {model_save_path}')
metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
with open(metrics_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
    writer.writerow(['DL', 
                     f'{epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}', 
                     f'{epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}', 
                     f'{epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}',
                     f'{epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}',  
                     f'{epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}', 
                     f'{epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}', 
                     f'{epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}'])
    writer.writerow(['GRASP', 
                     f'{np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}', 
                     f'{np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}', 
                     f'{np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}', 
                     f'{np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}', 
                     f'{np.mean(grasp_dc_mses):.4f} ± {np.std(grasp_dc_mses):.4f}', 
                     f'{np.mean(grasp_dc_maes):.4f} ± {np.std(grasp_dc_maes):.4f}', 
                     f'{np.mean(grasp_curve_corrs):.4f} ± {np.std(grasp_curve_corrs):.4f}'])
MAIN_EVALUATION_PLAN = [
    {
        "spokes_per_frame": 8,
        "num_frames": 36, # 8 * 36 = 288 total spokes
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 16,
        "num_frames": 18, # 16 * 18 = 288 total spokes
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 24,
        "num_frames": 12, # 24 * 12 = 288 total spokes
        "description": "Good temporal resolution"
    },
    {
        "spokes_per_frame": 32,
        "num_frames": 8, # 36 * 8 = 288 total spokes
        "description": "Standard temporal resolution"
    },
]
STRESS_TEST_PLAN = [
    {
        "spokes_per_frame": 2,
        "num_frames": 144, # 2 * 144 = 288 total spokes
        "description": "Stress test: max temporal points, 2 spokes"
    },
    {
        "spokes_per_frame": 4,
        "num_frames": 72, # 4 * 72 = 288 total spokes
        "description": "Stress test: max temporal points, 4 spokes"
    },
]
eval_spf_dataset = SimulatedSPFDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    )
eval_spf_loader = DataLoader(
    eval_spf_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=False,
    num_workers=config["dataloader"]["num_workers"],
)
with torch.no_grad():
    spf_recon_ssim = {}
    spf_recon_psnr = {}
    spf_recon_mse = {}
    spf_recon_lpips = {}
    spf_recon_dc_mse = {}
    spf_recon_dc_mae = {}
    spf_recon_corr = {}
    spf_grasp_ssim = {}
    spf_grasp_psnr = {}
    spf_grasp_mse = {}
    spf_grasp_lpips = {}
    spf_grasp_dc_mse = {}
    spf_grasp_dc_mae = {}
    spf_grasp_corr = {}
    print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
    for eval_config in STRESS_TEST_PLAN:
        stress_test_ssims = []
        stress_test_psnrs = []
        stress_test_mses = []
        stress_test_lpipses = []
        stress_test_dc_mses = []
        stress_test_dc_maes = []
        stress_test_corrs = []
        stress_test_grasp_ssims = []
        stress_test_grasp_psnrs = []
        stress_test_grasp_mses = []
        stress_test_grasp_lpipses = []
        stress_test_grasp_dc_mses = []
        stress_test_grasp_dc_maes = []
        stress_test_grasp_corrs = []
        spokes = eval_config["spokes_per_frame"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.num_frames = num_frames
        eval_spf_dataset._update_sample_paths()
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if config['model']['encode_time_index'] == False:
                start_timepoint_index = None
            else:
                start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            if num_frames > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, _ = sliding_window_inference(H, W, num_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch=None, device=device)  
            else:
                x_recon, *_ = model(
                    kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=None, norm=config['model']['norm']
                )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, int(spokes), eval_dir, f"{spokes}spf", device)
            stress_test_ssims.append(ssim)
            stress_test_psnrs.append(psnr)
            stress_test_mses.append(mse)
            stress_test_lpipses.append(lpips)
            stress_test_dc_mses.append(dc_mse)
            stress_test_dc_maes.append(dc_mae)
            if recon_corr is not None:
                stress_test_corrs.append(recon_corr)
                stress_test_grasp_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            stress_test_grasp_ssims.append(ssim_grasp)
            stress_test_grasp_psnrs.append(psnr_grasp)
            stress_test_grasp_mses.append(mse_grasp)
            stress_test_grasp_lpipses.append(lpips_grasp)
            stress_test_grasp_dc_mses.append(dc_mse_grasp)
            stress_test_grasp_dc_maes.append(dc_mae_grasp)
            spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
            spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
            spf_recon_mse[spokes] = np.mean(stress_test_mses)
            spf_recon_lpips[spokes] = np.mean(stress_test_lpipses)
            spf_recon_dc_mse[spokes] = np.mean(stress_test_dc_mses)
            spf_recon_dc_mae[spokes] = np.mean(stress_test_dc_maes)
            spf_recon_corr[spokes] = np.mean(stress_test_corrs)
            spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
            spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
            spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
            spf_grasp_lpips[spokes] = np.mean(stress_test_grasp_lpipses)
            spf_grasp_dc_mse[spokes] = np.mean(stress_test_grasp_dc_mses)
            spf_grasp_dc_mae[spokes] = np.mean(stress_test_grasp_dc_maes)
            spf_grasp_corr[spokes] = np.mean(stress_test_grasp_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', "LPIPS", 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(stress_test_ssims):.4f} ± {np.std(stress_test_ssims):.4f}', 
            f'{np.mean(stress_test_psnrs):.4f} ± {np.std(stress_test_psnrs):.4f}', 
            f'{np.mean(stress_test_mses):.4f} ± {np.std(stress_test_mses):.4f}', 
            f'{np.mean(stress_test_lpipses):.4f} ± {np.std(stress_test_lpipses):.4f}', 
            f'{np.mean(stress_test_dc_mses):.4f} ± {np.std(stress_test_dc_mses):.4f}',
            f'{np.mean(stress_test_dc_maes):.4f} ± {np.std(stress_test_dc_maes):.4f}',
            f'{np.mean(stress_test_corrs):.4f} ± {np.std(stress_test_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(stress_test_grasp_ssims):.4f} ± {np.std(stress_test_grasp_ssims):.4f}', 
            f'{np.mean(stress_test_grasp_psnrs):.4f} ± {np.std(stress_test_grasp_psnrs):.4f}', 
            f'{np.mean(stress_test_grasp_mses):.4f} ± {np.std(stress_test_grasp_mses):.4f}', 
            f'{np.mean(stress_test_grasp_lpipses):.4f} ± {np.std(stress_test_grasp_lpipses):.4f}', 
            f'{np.mean(stress_test_grasp_dc_mses):.4f} ± {np.std(stress_test_grasp_dc_mses):.4f}',
            f'{np.mean(stress_test_grasp_dc_maes):.4f} ± {np.std(stress_test_grasp_dc_maes):.4f}',
            f'{np.mean(stress_test_grasp_corrs):.4f} ± {np.std(stress_test_grasp_corrs):.4f}',
            ])
    print("--- Running Main Evaluation (Budget: 320 spokes) ---")
    for eval_config in MAIN_EVALUATION_PLAN:
        spf_eval_ssims = []
        spf_eval_psnrs = []
        spf_eval_mses = []
        spf_eval_lpipses = []
        spf_eval_dc_mses = []
        spf_eval_dc_maes = []
        spf_eval_curve_corrs = []
        spf_grasp_ssims = []
        spf_grasp_psnrs = []
        spf_grasp_mses = []
        spf_grasp_lpipses = []
        spf_grasp_dc_mses = []
        spf_grasp_dc_maes = []
        spf_grasp_curve_corrs = []
        spokes = eval_config["spokes_per_frame"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.num_frames = num_frames
        eval_spf_dataset._update_sample_paths()
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if config['model']['encode_time_index'] == False:
                start_timepoint_index = None
            else:
                start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            if num_frames > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, _ = sliding_window_inference(H, W, num_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch=None, device=device)  
            else:
                x_recon, *_ = model(
                kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=None, norm=config['model']['norm']
                )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, int(spokes), eval_dir, f'{spokes}spf', device)
            spf_eval_ssims.append(ssim)
            spf_eval_psnrs.append(psnr)
            spf_eval_mses.append(mse)
            spf_eval_lpipses.append(lpips)
            spf_eval_dc_mses.append(dc_mse)
            spf_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                spf_eval_curve_corrs.append(recon_corr)
                spf_grasp_curve_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            spf_grasp_ssims.append(ssim_grasp)
            spf_grasp_psnrs.append(psnr_grasp)
            spf_grasp_mses.append(mse_grasp)
            spf_grasp_lpipses.append(lpips_grasp)
            spf_grasp_dc_mses.append(dc_mse_grasp)
            spf_grasp_dc_maes.append(dc_mae_grasp)
        spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
        spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
        spf_recon_mse[spokes] = np.mean(spf_eval_mses)
        spf_recon_lpips[spokes] = np.mean(spf_eval_lpipses)
        spf_recon_dc_mse[spokes] = np.mean(spf_eval_dc_mses)
        spf_recon_dc_mae[spokes] = np.mean(spf_eval_dc_maes)
        spf_recon_corr[spokes] = np.mean(spf_eval_curve_corrs)
        spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
        spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
        spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
        spf_grasp_lpips[spokes] = np.mean(spf_grasp_lpipses)
        spf_grasp_dc_mse[spokes] = np.mean(spf_grasp_dc_mses)
        spf_grasp_dc_mae[spokes] = np.mean(spf_grasp_dc_maes)
        spf_grasp_corr[spokes] = np.mean(spf_grasp_curve_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(spf_eval_ssims):.4f} ± {np.std(spf_eval_ssims):.4f}', 
            f'{np.mean(spf_eval_psnrs):.4f} ± {np.std(spf_eval_psnrs):.4f}', 
            f'{np.mean(spf_eval_mses):.4f} ± {np.std(spf_eval_mses):.4f}', 
            f'{np.mean(spf_eval_lpipses):.4f} ± {np.std(spf_eval_lpipses):.4f}', 
            f'{np.mean(spf_eval_dc_mses):.4f} ± {np.std(spf_eval_dc_mses):.4f}',
            f'{np.mean(spf_eval_dc_maes):.4f} ± {np.std(spf_eval_dc_maes):.4f}',
            f'{np.mean(spf_eval_curve_corrs):.4f} ± {np.std(spf_eval_curve_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(spf_grasp_ssims):.4f} ± {np.std(spf_grasp_ssims):.4f}', 
            f'{np.mean(spf_grasp_psnrs):.4f} ± {np.std(spf_grasp_psnrs):.4f}', 
            f'{np.mean(spf_grasp_mses):.4f} ± {np.std(spf_grasp_mses):.4f}', 
            f'{np.mean(spf_grasp_lpipses):.4f} ± {np.std(spf_grasp_lpipses):.4f}', 
            f'{np.mean(spf_grasp_dc_mses):.4f} ± {np.std(spf_grasp_dc_mses):.4f}',
            f'{np.mean(spf_grasp_dc_maes):.4f} ± {np.std(spf_grasp_dc_maes):.4f}',
            f'{np.mean(spf_grasp_curve_corrs):.4f} ± {np.std(spf_grasp_curve_corrs):.4f}'
            ])
sns.set_style("whitegrid")
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
sns.lineplot(x=list(spf_recon_ssim.keys()), 
             y=list(spf_recon_ssim.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 0])
sns.lineplot(x=list(spf_grasp_ssim.keys()), 
             y=list(spf_grasp_ssim.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 0])
axes[0, 0].set_title("Evaluation SSIM vs Spokes/Frame")
axes[0, 0].set_xlabel("Spokes per Frame")
axes[0, 0].set_ylabel("SSIM")
sns.lineplot(x=list(spf_recon_psnr.keys()), 
             y=list(spf_recon_psnr.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 1])
sns.lineplot(x=list(spf_grasp_psnr.keys()), 
             y=list(spf_grasp_psnr.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 1])
axes[0, 1].set_title("Evaluation PSNR vs Spokes/Frame")
axes[0, 1].set_xlabel("Spokes per Frame")
axes[0, 1].set_ylabel("PSNR")
sns.lineplot(x=list(spf_recon_mse.keys()), 
             y=list(spf_recon_mse.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 2])
sns.lineplot(x=list(spf_grasp_mse.keys()), 
             y=list(spf_grasp_mse.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 2])
axes[0, 2].set_title("Evaluation Image MSE vs Spokes/Frame")
axes[0, 2].set_xlabel("Spokes per Frame")
axes[0, 2].set_ylabel("MSE")
sns.lineplot(x=list(spf_recon_lpips.keys()), 
             y=list(spf_recon_lpips.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 0])
sns.lineplot(x=list(spf_grasp_lpips.keys()), 
             y=list(spf_grasp_lpips.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 0])
axes[1, 0].set_title("Evaluation LPIPS vs Spokes/Frame")
axes[1, 0].set_xlabel("Spokes per Frame")
axes[1, 0].set_ylabel("LPIPS")
sns.lineplot(x=list(spf_recon_dc_mae.keys()), 
             y=list(spf_recon_dc_mae.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 1])
sns.lineplot(x=list(spf_grasp_dc_mae.keys()), 
             y=list(spf_grasp_dc_mae.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 1])
axes[1, 1].set_title("Evaluation k-space MAE vs Spokes/Frame")
axes[1, 1].set_xlabel("Spokes per Frame")
axes[1, 1].set_ylabel("MAE")
sns.lineplot(x=list(spf_recon_corr.keys()), 
             y=list(spf_recon_corr.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 2])
sns.lineplot(x=list(spf_grasp_corr.keys()), 
             y=list(spf_grasp_corr.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 2])
axes[1, 2].set_title("Tumor Enhancement Curve Correlation vs Spokes/Frame")
axes[1, 2].set_xlabel("Spokes per Frame")
axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "spf_eval_metrics.png"))
plt.close()

=== lsfpnet_encoding.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
import matplotlib.pyplot as plt
import numpy as np
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
def _realify(a: torch.Tensor) -> torch.Tensor:
    """2x2 real block representation of complex matrix
    returns R(a) = [[Re a, -Im a], [Im a, Re a]] with shape (2m, 2n)
    """
    ar = torch.view_as_real(a)                              # (m, n, 2)
    x = ar[..., 0]                                          # Re
    y = ar[..., 1]                                          # Im
    top = torch.cat([x, -y], dim=-1)                        # (m, 2n)
    bot = torch.cat([y,  x], dim=-1)                        # (m, 2n)
    return torch.cat([top, bot], dim=-2)                    # (2m, 2n)
def _de_realify(r: torch.Tensor) -> torch.Tensor:
    """inverse of _realify; expects r with shape (2m, 2n) laid out as [[X, -Y], [Y, X]]"""
    m2, n2 = r.shape[-2], r.shape[-1]
    m, n = m2 // 2, n2 // 2
    x = r[..., :m, :n]
    y = r[..., m:, :n]
    return x + 1j * y
class MappingNetwork(nn.Module):
    """Maps a scalar input to a style vector using a simple MLP."""
    def __init__(self, style_dim, channels, num_layers=4):
        super().__init__()
        layers = [nn.Linear(channels, style_dim), nn.ReLU(True)]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(style_dim, style_dim), nn.ReLU(True)])
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        if x.dim() == 0:
            x = x.unsqueeze(0)
        if x.dim() == 1:
            x = x.unsqueeze(1)
        return self.net(x)
class BasicBlock(nn.Module):
    """
    one unrolled LS+S block with:
      - nuclear-norm prox via SVD (two modes: 'detached_uv' or 'mag')
      - positivity via softplus on all hyper-parameters
      - optional hard low-k projection inside the DC gradient
      - FiLM conditioning with identity init and bounded modulation
    Args:
        lambdas: dict with keys {'lambda_L','lambda_S','lambda_spatial_L','lambda_spatial_S','gamma','lambda_step'}
        channels: conv width
        style_dim: FiLM MLP latent dim
        svd_mode: 'detached_uv' (recommended) or 'mag' (real-SVD on |Z|)
        use_lowk_dc: if True, replace low-k residual with measured data inside gradient
        lowk_frac: fraction of radii treated as “low-k” (e.g., 0.10–0.15)
        lowk_alpha: blend for low-k (1.0 hard replace, 0.9 soft blend)
        film_bounded: if True, use tanh-bounded modulation; else raw scale+1, bias
        film_gain: magnitude of modulation when film_bounded=True
        film_identity_init: if True, FiLM heads are zero-initialized (identity)
        svd_mag_noise_std: optional noise std added to |Z| in 'mag' mode (0 means none)
    """
    def __init__(
        self,
        lambdas,
        channels=32,
        style_dim=128,
        svd_mode: str = "detached_uv",
        use_lowk_dc: bool = True,
        lowk_frac: float = 0.125,
        lowk_alpha: float = 1.0,
        film_bounded: bool = True,
        film_gain: float = 0.10,
        film_identity_init: bool = True,
        svd_noise_std: float = 0.0,
        film_L: bool = True,
    ):
        super().__init__()
        self.channels = channels
        self.style_dim = style_dim
        self.film_L = film_L
        self.lambda_L        = nn.Parameter(torch.tensor([lambdas['lambda_L']]))
        self.lambda_S        = nn.Parameter(torch.tensor([lambdas['lambda_S']]))
        self.lambda_spatial_L= nn.Parameter(torch.tensor([lambdas['lambda_spatial_L']]))
        self.lambda_spatial_S= nn.Parameter(torch.tensor([lambdas['lambda_spatial_S']]))
        self.gamma           = nn.Parameter(torch.tensor([lambdas['gamma']]))
        self.lambda_step     = nn.Parameter(torch.tensor([lambdas['lambda_step']]))
        if self.film_L:
            self.style_injector_L = nn.Linear(self.style_dim, self.channels * 2)
        self.style_injector_S = nn.Linear(self.style_dim, self.channels * 2)
        if film_identity_init:
            if self.film_L:
                init.zeros_(self.style_injector_L.weight); init.zeros_(self.style_injector_L.bias)
            init.zeros_(self.style_injector_S.weight); init.zeros_(self.style_injector_S.bias)
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.svd_mode          = svd_mode
        self.use_lowk_dc       = use_lowk_dc
        self.lowk_frac         = lowk_frac
        self.lowk_alpha        = lowk_alpha
        self.film_bounded      = film_bounded
        self.film_gain         = film_gain
        self.svd_noise_std = svd_noise_std
    @staticmethod
    def _film(x, style_head, style_embedding, bounded: bool, gain: float):
        """apply FiLM modulation; identity at init; bounded if requested"""
        params = style_head(style_embedding)
        scale_raw, bias_raw = params.chunk(2, dim=-1)
        if bounded:
            scale = gain * torch.tanh(scale_raw)
            bias  = gain * torch.tanh(bias_raw)
            scale = scale.view(1, -1, 1, 1, 1)
            bias  = bias.view(1, -1, 1, 1, 1)
            return F.relu(x * (1.0 + scale) + bias)
        else:
            scale = scale_raw.view(1, -1, 1, 1, 1)
            bias  = bias_raw.view(1, -1, 1, 1, 1)
            return F.relu(x * (scale + 1.0) + bias)
    @staticmethod
    def _lowk_project(k_pred: torch.Tensor, y: torch.Tensor, ktraj: torch.Tensor, frac: float, alpha: float):
        """replace (or blend) low-k samples with measurements (vectorized, complex-safe)"""
        r = (ktraj[0]**2 + ktraj[1]**2).sqrt()
        thr = torch.quantile(r.reshape(-1), frac)
        M = (r <= thr)
        M = rearrange(M, 's t -> 1 s t')  # broadcast over coils
        return torch.where(M, alpha * y + (1.0 - alpha) * k_pred, k_pred)
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmaps, style_embedding=None):
        """
        runs one LS+S iteration
        inputs are complex in (nx, ny, nt) except p_*, pt_* which are packed as in your code
        returns same tuple as your original implementation
        """
        gamma          = F.softplus(self.gamma) + 1e-6
        lambda_step    = F.softplus(self.lambda_step) + 1e-6
        lambda_L_eff   = F.softplus(self.lambda_L) + 1e-8
        lambda_S_eff   = F.softplus(self.lambda_S) + 1e-8
        lam_sp_L_eff   = F.softplus(self.lambda_spatial_L) + 1e-8
        lam_sp_S_eff   = F.softplus(self.lambda_spatial_S) + 1e-8
        c = lambda_step / gamma
        nx, ny, nt = M0.size()
        x_sum  = torch.reshape(L + S, [nx, ny, nt])
        k_pred = param_E(inv=False, data=x_sum, smaps=csmaps)
        k_meas = param_d
        if self.use_lowk_dc:
            k_proj = self._lowk_project(k_pred, k_meas, param_E.ktraj, self.lowk_frac, self.lowk_alpha)
        else:
            k_proj = k_pred
        gradient = param_E(inv=True, data=k_proj - k_meas, smaps=csmaps)
        gradient = torch.reshape(gradient, [nx * ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = rearrange(pb_L.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)  # already flattened
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - gamma * gradient - gamma * pt_L - gamma * pb_L
        Z = c * y_L + pt_L  # (nx*ny, nt) complex
        if self.svd_mode == "detached_uv":
            U, Svals, Vh = torch.linalg.svd(Z, full_matrices=False)
            U_d, Vh_d = U.detach(), Vh.detach()
            S_shrunk = Project_inf(Svals, lambda_L_eff)
            pt_L = U_d @ torch.diag_embed(S_shrunk) @ Vh_d
        elif self.svd_mode == "mag":
            mag   = Z.abs() + 1e-8
            phase = Z / mag
            if self.svd_noise_std > 0.0:
                mag = mag + torch.randn_like(mag) * self.svd_noise_std
            U, Svals, Vh = torch.linalg.svd(mag, full_matrices=False)
            S_shrunk = Project_inf(Svals, lambda_L_eff, to_complex=False)
            pt_L_mag = U @ torch.diag_embed(S_shrunk) @ Vh
            pt_L = pt_L_mag * phase
        elif self.svd_mode == "real":
            R = _realify(Z)                                     # (2M, 2N) real
            print("noise std: ", self.svd_noise_std)
            if self.svd_noise_std > 0.0:
                print("adding noise...")
                R = R + self.svd_noise_std * torch.randn_like(R)
            Ur, Sr, VrT = torch.linalg.svd(R, full_matrices=False)
            Sr_shrunk = Project_inf(Sr, self.lambda_L, to_complex=False)        # same tau as complex case
            R_prox = Ur @ torch.diag_embed(Sr_shrunk) @ VrT
            pt_L = _de_realify(R_prox) 
        else:
            raise ValueError(f"unsupported svd_mode: {self.svd_mode}")
        tL_in = from_torch_complex(y_L)
        tL_in  = rearrange(tL_in, '(nx ny) two nt -> two 1 nx ny nt', nx=nx, ny=ny)
        tL     = F.conv3d(tL_in, self.conv1_forward_l, padding=1); tL = F.relu(tL)
        tL     = F.conv3d(tL,    self.conv2_forward_l, padding=1)
        if style_embedding is not None and self.film_L:
            tL = self._film(tL, self.style_injector_L, style_embedding, self.film_bounded, self.film_gain)
        else:
            tL = F.relu(tL)
        tL_out = F.conv3d(tL, self.conv3_forward_l, padding=1)
        tL_out_c = tL_out + p_L
        tL_out_c = tL_out_c[0, :, :, :, :] + 1j * tL_out_c[1, :, :, :, :]
        p_L = Project_inf(c * tL_out_c, lam_sp_L_eff)
        p_L = from_torch_complex(p_L)
        p_L = rearrange(p_L, 'ch two nx ny nt -> two ch nx ny nt')
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L_out = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = rearrange(pb_L_out.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - gamma * gradient - gamma * pt_L - gamma * pb_L
        adjloss_L = tL_out * p_L - pb_L_out * tL_in
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = rearrange(pb_S.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S  = S - gamma * gradient - gamma * Wtxs(pt_S) - gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, lambda_S_eff)
        tS_in = from_torch_complex(y_S)
        tS_in  = rearrange(tS_in, '(nx ny) two nt -> two 1 nx ny nt', nx=nx, ny=ny)
        tS     = F.conv3d(tS_in, self.conv1_forward_s, padding=1); tS = F.relu(tS)
        tS     = F.conv3d(tS,    self.conv2_forward_s, padding=1)
        if style_embedding is not None:
            tS = self._film(tS, self.style_injector_S, style_embedding, self.film_bounded, self.film_gain)
        else:
            tS = F.relu(tS)
        tS_out = F.conv3d(tS, self.conv3_forward_s, padding=1)
        tS_out_c = tS_out + p_S
        tS_out_c = tS_out_c[0, :, :, :, :] + 1j * tS_out_c[1, :, :, :, :]
        p_S = Project_inf(c * tS_out_c, lam_sp_S_eff)
        p_S = from_torch_complex(p_S)
        p_S = rearrange(p_S, 'ch two nx ny nt -> two ch nx ny nt')
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S_out = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = rearrange(pb_S_out.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - gamma * gradient - gamma * Wtxs(pt_S) - gamma * pb_S
        adjloss_S = tS_out * p_S - pb_S_out * tS_in
        return [
            L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S,
            lambda_L_eff, lambda_S_eff, lam_sp_L_eff, lam_sp_S_eff, gamma, lambda_step
        ]
class LSFPNet(nn.Module):
    def __init__(self, 
                 LayerNo: int, 
                 lambdas: dict, 
                 channels: int = 32, 
                 style_dim: int = 128,
                 svd_mode: str = "detached_uv",
                 use_lowk_dc: bool = True,
                 lowk_frac: float = 0.125,
                 lowk_alpha: float = 1.0,
                 film_bounded: bool = True,
                 film_gain: float = 0.10,
                 film_identity_init: bool = True,
                 svd_noise_std: float = 0.0,
                 film_L: bool = True,
        ):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        self.channels = channels
        self.style_dim = style_dim
        for ii in range(LayerNo):
            onelayer.append(BasicBlock(lambdas=lambdas, 
                                       channels=self.channels, 
                                       style_dim=style_dim,
                                       svd_mode=svd_mode,
                                       use_lowk_dc=use_lowk_dc,
                                       lowk_frac=lowk_frac,
                                       lowk_alpha=lowk_alpha,
                                       film_bounded=film_bounded,
                                       film_gain=film_gain,
                                       film_identity_init=film_identity_init,
                                       svd_noise_std=svd_noise_std,
                                       film_L=film_L,
                                       ))
        self.fcs = nn.ModuleList(onelayer)
    def plot_block_output(self, M0, L, S, iter, epoch, output_dir):
        time_frame_index = 3
        nx, ny, nt = M0.size()
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        output_image = L + S
        fig, axes = plt.subplots(1, 4, figsize=(24, 6))
        fig.suptitle(f"Basic Block Output at Time Frame {time_frame_index} and Iteration {iter}", fontsize=20)
        axes[0].imshow(np.abs(M0[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[0].set_title("Input Image")
        axes[0].axis("off")
        axes[1].imshow(np.abs(L[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[1].set_title("Background Component (L)")
        axes[1].axis("off")
        axes[2].imshow(np.abs(S[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[2].set_title("Dynamic Component (S)")
        axes[2].axis("off")
        axes[3].imshow(np.abs(output_image[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[3].set_title("Combined Image (L + S)")
        axes[3].axis("off")
        filename = os.path.join(output_dir, f'basic_block_output_{epoch}_iter{iter}.png')
        plt.savefig(filename)
        plt.close()
    def forward(self, M0, param_E, param_d, csmap, epoch, output_dir, style_embedding=None):
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmap, style_embedding)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
            self.plot_block_output(M0, L, S, iter=ii, epoch=epoch, output_dir=output_dir)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step]
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, output_dir, channels, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
        self.output_dir = output_dir
        self.style_dim = 128  # You can tune this hyperparameter
        self.mapping_network = MappingNetwork(style_dim=self.style_dim, channels=channels)
    @staticmethod
    def _normalise_both(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max() + 1e-8                     # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_baseline(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf[..., 0].abs().mean() + 1e-8                     # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_indep(x: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = torch.quantile(x.abs(), 0.99) + 1e-6
        if scale < 1e-6: # Handle case where input is all zeros
             scale = 1.0
        return x / scale, scale
    def forward(self, y, E, csmap, acceleration=None, start_timepoint_index=None, epoch=None, norm="both", **kwargs):
        x_init = E(inv=True, data=y, smaps=csmap)
        if norm =="both":
            x_init_norm, y_norm, scale = self._normalise_both(x_init, y)
        elif norm == "independent":
            x_init_norm, scale = self._normalise_indep(x_init)
            y_norm, scale_y = self._normalise_indep(y)
        elif norm == "baseline":
            x_init_norm, y_norm, scale = self._normalise_baseline(x_init, y)
        elif norm == "none":
            x_init_norm = x_init
            y_norm = y
            scale = 1.0
        if acceleration or start_timepoint_index:                                          # already small
            if start_timepoint_index is not None:
                T = x_init_norm.shape[-1]
                start_frac = (start_timepoint_index / max(T - 1, 1)).view(-1, 1)
                if acceleration is not None:
                    H = x_init_norm.shape[-2]
                    N_full = H * np.pi / 2.0
                    inv_af = (1.0 / acceleration.clamp_min(1e-6)).view(-1, 1)          # smaller numbers are safer
                    spf_est = (N_full / acceleration.clamp_min(1e-6)).view(-1, 1)      # useful too
                    inv_af_feat = inv_af   
                    combined_input = torch.cat([inv_af_feat, start_frac], dim=1).to(x_init_norm.device)
                else:
                    combined_input = start_frac
            else:
                H = x_init_norm.shape[-2]
                N_full = H * np.pi / 2.0
                inv_af = (1.0 / acceleration.clamp_min(1e-6)).view(-1, 1)          # smaller numbers are safer
                spf_est = (N_full / acceleration.clamp_min(1e-6)).view(-1, 1)      # useful too
                inv_af_feat = inv_af   
                combined_input = inv_af_feat
            style_embedding = self.mapping_network(combined_input)
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir, style_embedding)
        else:
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir)
        loss_constraint_L = torch.square(torch.mean(loss_layers_adj_L[0])) / self.backbone_net.LayerNo
        loss_constraint_S = torch.square(torch.mean(loss_layers_adj_S[0])) / self.backbone_net.LayerNo
        for k in range(self.backbone_net.LayerNo - 1):
            loss_constraint_S += torch.square(torch.mean(loss_layers_adj_S[k + 1])) / self.backbone_net.LayerNo
            loss_constraint_L += torch.square(torch.mean(loss_layers_adj_L[k + 1])) / self.backbone_net.LayerNo
        recon = (L + S) * scale                 # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat, loss_constraint_L + loss_constraint_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step

=== radial_lsfp.py ===
import torch
import torch.nn as nn
import numpy as np
from time import time
dtype = torch.complex64
class MCNUFFT(nn.Module):
    def __init__(self, nufft_ob, adjnufft_ob, ktraj, dcomp):
        super(MCNUFFT, self).__init__()
        self.nufft_ob = nufft_ob
        self.adjnufft_ob = adjnufft_ob
        self.ktraj = torch.squeeze(ktraj)
        self.dcomp = torch.squeeze(dcomp)
    def forward(self, inv, data, smaps):
        data = torch.squeeze(data)
        Nx, Ny = smaps.shape[2], smaps.shape[3]
        if len(data.shape) > 2:  # multi-frame
            is_complex_data = torch.is_complex(data)
            if inv: # Adjoint NUFFT (k-space -> image)
                kd = data.permute(2, 0, 1) # -> [time, coils, samples]
                d = self.dcomp.permute(1, 0) # -> [time, samples]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                d = d.unsqueeze(1) 
                x_temp = self.adjnufft_ob(kd * d, k, smaps=smaps.to(dtype))
                x = x_temp.squeeze(1).permute(1, 2, 0) / np.sqrt(Nx * Ny)
            else: # Forward NUFFT (image -> k-space)
                image = data.permute(2, 0, 1).unsqueeze(1) # -> [time, 1, Nx, Ny]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                x_temp = self.nufft_ob(image, k, smaps=smaps)
                x = x_temp.permute(1, 2, 0) / np.sqrt(Nx * Ny)
        else:  # single frame (original logic is fine)
            if inv:
                kd = data.unsqueeze(0)
                d = self.dcomp.unsqueeze(0).unsqueeze(0)
                x = self.adjnufft_ob(kd * d, self.ktraj, smaps=smaps.to(dtype))
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
            else:
                image = data.unsqueeze(0).unsqueeze(0)
                x = self.nufft_ob(image, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        return x

=== utils.py ===
import os
import subprocess
import matplotlib.pyplot as plt
import torch
import numpy as np
from einops import rearrange
import torchkbnufft as tkbn
import csv
import sigpy as sp
from sigpy.mri import app
from radial_lsfp import MCNUFFT
import random
def log_gradient_stats(model, epoch, iteration, output_dir, log_filename="gradient_stats.csv"):
    """
    Computes, prints, and logs the L2 norm of gradients for each parameter and the total gradient norm.
    Args:
        model (torch.nn.Module): The model being trained.
        epoch (int): The current epoch.
        iteration (int): The current global iteration/step count.
        output_dir (str): The main experiment output directory.
        log_filename (str): The CSV filename for storing detailed logs.
    """
    total_norm = 0.0
    param_norms = []
    for name, p in model.named_parameters():
        if p.grad is not None and p.requires_grad:
            param_norm = p.grad.data.norm(2)
            if not torch.isfinite(param_norm):
                param_norm_item = float('inf')
            else:
                param_norm_item = param_norm.item()
            param_norms.append((name, param_norm_item))
            total_norm += param_norm_item ** 2
    total_norm = total_norm ** 0.5
    print(f"--- Gradient Stats (Epoch {epoch}, Iter {iteration}) ---")
    print(f"Total Gradient Norm: {total_norm:.4e}")
    param_norms.sort(key=lambda x: x[1], reverse=True)
    print("Top 5 layers with largest gradients:")
    for name, norm in param_norms[:5]:
        print(f"  - {name}: {norm:.4e}")
    print("Top 5 layers with smallest gradients:")
    non_zero_norms = [p for p in param_norms if p[1] > 0]
    for name, norm in non_zero_norms[-5:]:
        print(f"  - {name}: {norm:.4e}")
    print("-------------------------------------------------")
    log_path = os.path.join(output_dir, log_filename)
    file_exists = os.path.isfile(log_path)
    with open(log_path, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow(['epoch', 'iteration', 'total_norm', 'param_name', 'param_norm'])
        writer.writerow([epoch, iteration, total_norm, '---TOTAL---', total_norm])
        for name, norm in param_norms:
            writer.writerow([epoch, iteration, total_norm, name, norm])
def trajGR(Nkx, Nspokes):
    '''
    function for generating golden-angle radial sampling trajectory
    :param Nkx: spoke length
    :param Nspokes: number of spokes
    :return: ktraj: golden-angle radial sampling trajectory
    '''
    ga = np.pi * ((1 - np.sqrt(5)) / 2)
    kx = np.zeros(shape=(Nkx, Nspokes))
    ky = np.zeros(shape=(Nkx, Nspokes))
    ky[:, 0] = np.linspace(-np.pi, np.pi, Nkx)
    for i in range(1, Nspokes):
        kx[:, i] = np.cos(ga) * kx[:, i - 1] - np.sin(ga) * ky[:, i - 1]
        ky[:, i] = np.sin(ga) * kx[:, i - 1] + np.cos(ga) * ky[:, i - 1]
    ky = np.transpose(ky)
    kx = np.transpose(kx)
    ktraj = np.stack((ky.flatten(), kx.flatten()), axis=0)
    return ktraj
def prep_nufft(Nsample, Nspokes, Ng):
    overSmaple = 2
    im_size = (int(Nsample/overSmaple), int(Nsample/overSmaple))
    grid_size = (Nsample, Nsample)
    ktraj = trajGR(Nsample, Nspokes * Ng)
    ktraj = torch.tensor(ktraj, dtype=torch.float)
    dcomp = tkbn.calc_density_compensation_function(ktraj=ktraj, im_size=im_size)
    dcomp = dcomp.squeeze()
    ktraju = np.zeros([2, Nspokes * Nsample, Ng], dtype=float)
    dcompu = np.zeros([Nspokes * Nsample, Ng], dtype=complex)
    for ii in range(0, Ng):
        ktraju[:, :, ii] = ktraj[:, (ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
        dcompu[:, ii] = dcomp[(ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
    ktraju = torch.tensor(ktraju, dtype=torch.float)
    dcompu = torch.tensor(dcompu, dtype=torch.complex64)
    nufft_ob = tkbn.KbNufft(im_size=im_size, grid_size=grid_size)  # forward nufft
    adjnufft_ob = tkbn.KbNufftAdjoint(im_size=im_size, grid_size=grid_size)  # backward nufft
    return ktraju, dcompu, nufft_ob, adjnufft_ob
def _calculate_top_percentile_curve(dynamic_slice: torch.Tensor, percentile: float) -> list[float]:
    """Helper function to calculate the enhancement curve for a single dynamic slice."""
    if dynamic_slice.dim() != 5 or dynamic_slice.shape[0] != 1 or dynamic_slice.shape[1] != 2:
        raise ValueError(f"Expected input shape (1, 2, T, H, W), but got {dynamic_slice.shape}")
    magnitude_video = torch.sqrt(dynamic_slice[:, 0, ...] ** 2 + dynamic_slice[:, 1, ...] ** 2).squeeze(0)
    num_time_frames = magnitude_video.shape[0]
    top_percentile_means = []
    q = percentile / 100.0
    for t in range(num_time_frames):
        frame_t = magnitude_video[t, :, :]
        if frame_t.max() == 0:
            top_percentile_means.append(0)
            continue
        threshold = torch.quantile(frame_t.flatten(), q)
        bright_pixels = frame_t[frame_t > threshold]
        mean_val = torch.mean(bright_pixels) if bright_pixels.numel() > 0 else threshold
        top_percentile_means.append(mean_val.item())
    return top_percentile_means
def plot_enhancement_curve(
    model_output: torch.Tensor,
    percentile: float = 99.0,
    title: str = "Enhancement Curve Comparison",
    output_filename: str = None
):
    """
    Calculates and plots the enhancement curves for a model output and a benchmark
    image on the same graph for direct comparison.
    Args:
        model_output (torch.Tensor): The model's reconstructed dynamic slice.
                                     Shape (1, 2, T, H, W).
        benchmark_image (torch.Tensor): The ground truth or benchmark dynamic slice.
                                        Shape (1, 2, T, H, W).
        percentile (float, optional): The percentile for defining the brightest pixels.
                                      Defaults to 99.0.
        title (str, optional): The title for the plot. Defaults to "Enhancement Curve Comparison".
        output_filename (str, optional): If provided, saves the plot to this file path.
                                         Defaults to None (displays plot).
    """
    if not 0 < percentile < 100:
        raise ValueError("Percentile must be between 0 and 100.")
    model_curve = _calculate_top_percentile_curve(model_output.detach(), percentile)
    num_time_frames = model_output.shape[2]
    time_axis = np.arange(num_time_frames)
    plt.figure(figsize=(12, 7))
    plt.plot(time_axis, model_curve, label='Model Output', marker='o', linestyle='-', color='tab:blue')
    plt.title(title, fontsize=16)
    plt.xlabel("Time Frame", fontsize=12)
    plt.ylabel(f"Mean Signal of Top {100-percentile:.1f}% Pixels", fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    if output_filename:
        output_dir = os.path.dirname(output_filename)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        plt.savefig(output_filename)
    else:
        plt.show()
    plt.close()
def get_cosine_ei_weight(
    current_epoch,
    warmup_epochs,
    schedule_duration,
    target_weight
):
    """
    Calculates the EI loss weight for the current epoch using a cosine schedule.
    This implements a curriculum learning strategy:
    1. For `warmup_epochs`, the weight is 0 (MC loss only).
    2. Over the next `schedule_duration` epochs, the weight smoothly ramps
       up from 0 to `target_weight` following a cosine curve.
    3. After the schedule is complete, the weight stays at `target_weight`.
    Args:
        current_epoch (int): The current training epoch (starting from 1).
        warmup_epochs (int): Number of epochs to train with only MC loss.
        schedule_duration (int): Number of epochs for the ramp-up.
        target_weight (float): The final EI loss weight to reach.
    Returns:
        float: The EI loss weight for the current epoch.
    """
    if current_epoch <= warmup_epochs:
        return 0.0
    schedule_progress_epoch = current_epoch - warmup_epochs
    if schedule_progress_epoch >= schedule_duration:
        return target_weight
    cosine_multiplier = 0.5 * (1 - np.cos(np.pi * schedule_progress_epoch / schedule_duration))
    return target_weight * cosine_multiplier
def plot_reconstruction_sample(x_recon, title, filename, output_dir, grasp_img=None, batch_idx=0, transform=False):
    """
    Plot reconstruction sample showing magnitude images across timeframes.
    Args:
        x_recon: Reconstructed image tensor of shape (B, C, T, H, W)
        title: Title for the plot
        filename: Filename for saving (without extension)
        output_dir: Directory to save the plot
        batch_idx: Which batch element to plot (default: 0)
    """
    os.makedirs(output_dir, exist_ok=True)
    if x_recon.shape[1] == 2:
        x_recon_mag = torch.sqrt(x_recon[:, 0, ...] ** 2 + x_recon[:, 1, ...] ** 2)
    else:
        x_recon_mag = x_recon
    print(x_recon_mag.shape)
    n_timeframes = x_recon_mag.shape[-1]
    if grasp_img is not None:
        grasp_img_mag = torch.sqrt(grasp_img[:, 0, ...] ** 2 + grasp_img[:, 1, ...] ** 2)
        fig, axes = plt.subplots(
            nrows=2,
            ncols=n_timeframes,
            figsize=(n_timeframes * 3, 8),
            squeeze=False,
        )
        if transform:
            axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
            axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
            os.makedirs(os.path.join(output_dir, "transforms"), exist_ok=True)
        else:
            axes[0, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
            axes[1, 0].set_ylabel("GRASP Benchmark", fontsize=14, labelpad=10)
    else:
        fig, axes = plt.subplots(
            nrows=1,
            ncols=n_timeframes,
            figsize=(n_timeframes * 3, 4),
            squeeze=True,
        )
    for t in range(n_timeframes):
        if x_recon_mag.shape[1] == n_timeframes:
            img = x_recon_mag[batch_idx, t, :, :].cpu().detach().numpy()
        else:
            img = x_recon_mag[batch_idx, ..., t].cpu().detach().numpy()
        if grasp_img is not None:
            if grasp_img_mag.shape[1] == n_timeframes:
                grasp_img = grasp_img_mag[batch_idx, t, :, :].cpu().detach().numpy()
            elif grasp_img_mag.shape[-1] == n_timeframes:
                grasp_img = grasp_img_mag[batch_idx, :, :, t].cpu().detach().numpy()
            else:
                grasp_img = grasp_img_mag[batch_idx, :, t, :].cpu().detach().numpy()
            ax1 = axes[0, t]
        else:
            ax1 = axes[t]
        ax1.imshow(img, cmap="gray")
        ax1.set_title(f"t = {t}")
        ax1.set_xticks([])
        ax1.set_yticks([])
        if grasp_img is not None:
            ax2 = axes[1, t]
            ax2.imshow(grasp_img, cmap="gray")
            ax2.set_title(f"t = {t}")
            ax2.set_xticks([])
            ax2.set_yticks([])
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(output_dir, f"{filename}.png"))
    plt.close(fig)
def get_git_commit():
    try:
        commit_hash = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        return commit_hash
    except Exception as e:
        print(f"Error retrieving Git commit: {e}")
        return "unknown"
def remove_module_prefix(state_dict):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('module.', '')  # Remove 'module.' prefix
        new_state_dict[new_key] = v
    return new_state_dict
def save_checkpoint(model, optimizer, epoch,
                    train_curves, val_curves, eval_curves, ei_weight, filename):
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "ei_weight": ei_weight,
        **train_curves,   # unpack the dicts
        **val_curves,
        **eval_curves,
    }
    torch.save(checkpoint, filename)
    print(f"Checkpoint saved at epoch {epoch} to {filename}")
def load_checkpoint(model, optimizer, filename):
    ckpt = torch.load(filename, map_location="cpu")
    model.load_state_dict(remove_module_prefix(ckpt["model_state_dict"]))
    optimizer.load_state_dict(remove_module_prefix(ckpt["optimizer_state_dict"]))
    train_curves = {
        "train_mc_losses": ckpt.get("train_mc_losses", []),
        "train_ei_losses": ckpt.get("train_ei_losses", []),
        "train_adj_losses": ckpt.get("train_adj_losses", []),
        "weighted_train_mc_losses": ckpt.get("weighted_train_mc_losses", []),
        "weighted_train_ei_losses": ckpt.get("weighted_train_ei_losses", []),
        "weighted_train_adj_losses": ckpt.get("weighted_train_adj_losses", []),
    }
    val_curves = {
        "val_mc_losses": ckpt.get("val_mc_losses", []),
        "val_ei_losses": ckpt.get("val_ei_losses", []),
        "val_adj_losses": ckpt.get("val_adj_losses", []),
    }
    eval_curves = {
        "eval_ssims": ckpt.get("eval_ssims", []),
        "eval_psnrs": ckpt.get("eval_psnrs", []),
        "eval_mses": ckpt.get("eval_mses", []),
        "eval_lpipses": ckpt.get("eval_lpipses", []),
        "eval_dc_mses": ckpt.get("eval_dc_mses", []),
        "eval_dc_maes": ckpt.get("eval_dc_maes", []),
        "eval_curve_corrs": ckpt.get("eval_curve_corrs", []),
    }
    return model, optimizer, ckpt.get("epoch", 1), ckpt.get("ei_weight"), train_curves, val_curves, eval_curves
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def get_traj(N_spokes=13, N_time=1, base_res=320, gind=1):
    N_tot_spokes = N_spokes * N_time
    N_samples = base_res * 2
    base_lin = np.arange(N_samples).reshape(1, -1) - base_res
    tau = 0.5 * (1 + 5**0.5)
    base_rad = np.pi / (gind + tau - 1)
    base_rot = np.arange(N_tot_spokes).reshape(-1, 1) * base_rad
    traj = np.zeros((N_tot_spokes, N_samples, 2))
    traj[..., 0] = np.cos(base_rot) @ base_lin
    traj[..., 1] = np.sin(base_rot) @ base_lin
    traj = traj / 2
    traj = traj.reshape(N_time, N_spokes, N_samples, 2)
    return np.squeeze(traj)
def GRASPRecon(csmaps, kspace, spokes_per_frame, num_frames, grasp_path):
    traj = get_traj(N_spokes=spokes_per_frame, N_time=num_frames)
    device = sp.Device(0 if torch.cuda.is_available() else -1)
    kspace = rearrange(kspace, 'c (sp sam) t -> t c sp sam', sam=640).unsqueeze(1).unsqueeze(3).cpu().numpy()
    csmaps = rearrange(csmaps, 'b c h w -> c b h w').cpu().numpy()
    R1 = app.HighDimensionalRecon(kspace, csmaps,
                            combine_echo=False,
                            lamda=0.001,
                            coord=traj,
                            regu='TV', regu_axes=[0],
                            max_iter=10,
                            solver='ADMM', rho=0.1,
                            device=device,
                            show_pbar=False,
                            verbose=False).run()
    R1 = np.squeeze(R1.get())
    np.save(grasp_path, R1)
    print(f"GRASP Recon with {spokes_per_frame} spokes/frame and {num_frames} timeframes saved to {grasp_path}")
    return R1
def generate_sliding_window_indices(N_frames, chunk_size, overlap_size):
    """
    Generates start and end indices for a sliding window reconstruction.
    Args:
        N_frames (int): Total number of time frames in the dynamic MRI.
        chunk_size (int): The number of frames in each chunk.
        overlap_size (int): The number of frames that consecutive chunks will overlap.
    Returns:
        list of tuple: A list where each tuple contains (start_index, end_index)
                       for a chunk.
    """
    if chunk_size <= 0 or N_frames <= 0:
        raise ValueError("chunk_size and N_frames must be positive.")
    if overlap_size >= chunk_size:
        raise ValueError("overlap_size must be less than chunk_size.")
    if overlap_size < 0:
        raise ValueError("overlap_size cannot be negative.")
    chunks = []
    step_size = chunk_size - overlap_size
    current_start = 0
    while True:
        current_end = current_start + chunk_size
        if current_end > N_frames:
            if N_frames - chunk_size >= 0:
                current_start = N_frames - chunk_size
                current_end = N_frames
            else:
                current_start = 0
                current_end = N_frames
            chunks.append((current_start, current_end))
            break # We've covered the end of the sequence
        chunks.append((current_start, current_end))
        if current_end == N_frames:
            break
        current_start += step_size
    unique_chunks = []
    seen = set()
    for chunk in chunks:
        if chunk not in seen:
            unique_chunks.append(chunk)
            seen.add(chunk)
    return unique_chunks
def sliding_window_inference(H, W, N_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, chunk_size, chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch, device):
    chunk_indices = generate_sliding_window_indices(N_frames, chunk_size, chunk_overlap)
    stitched_recon = torch.zeros(1, 2, H, W, N_frames).to(device)
    frame_contribution_count = torch.zeros(H, W, N_frames).to(device)
    csmap = csmap.to(device)
    for i, (start_idx, end_idx) in enumerate(chunk_indices):
        print(f"Processing chunk {i+1}: frames {start_idx}-{end_idx}")
        kspace_chunk = kspace[..., start_idx:end_idx].to(device)
        ktraj_chunk = ktraj[..., start_idx:end_idx].to(device)
        dcomp_chunk = dcomp[..., start_idx:end_idx].to(device)
        physics_chunk = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
        if start_timepoint_index is not None:
            start_timepoint_index = torch.tensor([start_idx], dtype=torch.float, device=device)
        x_recon_chunk, adj_loss, *_ = model(
            kspace_chunk.to(device), physics_chunk, csmap, acceleration_encoding, start_timepoint_index, epoch=epoch, norm="both"
        )
        stitched_recon[..., start_idx:end_idx] += x_recon_chunk
        frame_contribution_count[..., start_idx:end_idx] += 1
    stitched_recon /= frame_contribution_count # This performs element-wise division
    return stitched_recon, adj_loss
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

=== eval.py ===
import os
import matplotlib.pyplot as plt
import torch
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import torchmetrics
import time
from dataloader import SimulatedDataset
from lsfpnet import to_torch_complex, from_torch_complex
import numpy as np
from scipy.optimize import curve_fit
from scipy.interpolate import PchipInterpolator
from tqdm import tqdm # A library for a nice progress bar
from scipy.stats import mannwhitneyu
from skimage.metrics import structural_similarity as ssim_map_func
import matplotlib.gridspec as gridspec
from skimage.measure import find_contours
from typing import List, Dict
from scipy.stats import pearsonr
def normalize_for_lpips(image, data_range):
    """Normalizes an image tensor to the [-1, 1] range for LPIPS."""
    min_val, max_val = data_range
    image_0_1 = (image - min_val) / (max_val - min_val)
    image_minus1_1 = 2 * image_0_1 - 1
    return image_minus1_1
def calc_image_metrics(input, reference, data_range, device, filename):
    """
    Calculates image metrics for a given input and reference image.
    """
    ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range).to(device)
    psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range).to(device)
    mse = torchmetrics.MeanSquaredError().to(device)
    lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=False).to(device)
    ssim = ssim(input, reference)
    psnr = psnr(input, reference)
    mse = mse(input, reference)
    if input.dim() == 5:
        num_slices = input.shape[2]
        lpips_scores = []
        for i in range(num_slices):
            input_slice = input[:, :, i, :, :]
            reference_slice = reference[:, :, i, :, :]
            input_lpips = normalize_for_lpips(input_slice.clone(), data_range)
            reference_lpips = normalize_for_lpips(reference_slice.clone(), data_range)
            if input_lpips.shape[1] == 1:
                input_lpips = input_lpips.repeat(1, 3, 1, 1)
                reference_lpips = reference_lpips.repeat(1, 3, 1, 1)
            input_lpips = input_lpips.to(reference_lpips.dtype)
            lpips_scores.append(lpips_metric(input_lpips, reference_lpips).item())
        final_lpips = sum(lpips_scores) / len(lpips_scores)
    return ssim.item(), psnr.item(), mse.item(), final_lpips
def calc_dc(input, reference, device):
    """
    Calculates data consistency MSE for a given input and reference k-space tensor.
    """
    mse = torchmetrics.MeanSquaredError().to(device)
    mae = torchmetrics.MeanAbsoluteError().to(device)
    input = from_torch_complex(input).to(device)
    reference = from_torch_complex(reference).to(device)
    mse = mse(input, reference)
    mae = mae(input, reference)
    return mse.item(), mae.item()
def evaluate_reconstruction_fidelity(
    ground_truth_params: np.ndarray,
    estimated_params: np.ndarray,
    masks: dict,
    param_names: list = None,
    regions_to_evaluate: list = None,
    display_plots: bool = True,
    filename: str = 'pk_param_maps.png'
) -> dict:
    """
    Evaluates the fidelity of reconstructed pharmacokinetic (PK) parameters against ground truth.
    This function performs a quantitative and visual comparison, mimicking the evaluation
    methods described in the research paper (e.g., Figures 6 and 8).
    Args:
        ground_truth_params (np.ndarray): The ground truth PK parameter map, typically a
                                          (H, W, 4) array from the `gen_dro` output.
        estimated_params (np.ndarray): The PK parameter map estimated from your reconstructed
                                       images, with the same shape as ground_truth_params.
        masks (dict): A dictionary of boolean masks for different tissue regions, typically
                      from the `gen_dro` output (e.g., dro_results['mask']).
        param_names (list, optional): A list of names for the 4 parameters.
                                      Defaults to ['ve', 'vp', 'Fp', 'PS'].
        regions_to_evaluate (list, optional): A list of region names (keys in the `masks`
                                            dict) to analyze. Defaults to all available masks.
        display_plots (bool): If True, generates and shows summary plots.
    Returns:
        dict: A nested dictionary containing the evaluation results (median error and p-value)
              for each region and each parameter.
    """
    if param_names is None:
        param_names = ['ve', 'vp', 'Fp (F_p)', 'PS'] # As ordered in gen_dro
    if regions_to_evaluate is None:
        regions_to_evaluate = [name for name, mask in masks.items() if mask.any()]
    if ground_truth_params.shape != estimated_params.shape:
        raise ValueError("Ground truth and estimated parameter maps must have the same shape.")
    evaluation_results = {}
    print("--- Reconstruction Fidelity Evaluation ---")
    print("-" * 40)
    for region in regions_to_evaluate:
        if region not in masks or not masks[region].any():
            continue
        print(f"Region: {region.capitalize()}")
        evaluation_results[region] = {}
        mask = masks[region]
        for i, p_name in enumerate(param_names):
            print("ground_truth_params: ", type(ground_truth_params))
            gt_values = ground_truth_params[:, :, i][mask]
            est_values = estimated_params[:, :, i][mask]
            print("gt_values: ", type(gt_values))
            gt_values_safe = gt_values.copy()
            gt_values_safe[gt_values_safe == 0] = 1e-9 # Add a small epsilon
            relative_error = (est_values - gt_values) / gt_values_safe
            median_err = np.median(relative_error)
            try:
                stat, p_value = mannwhitneyu(gt_values, est_values, alternative='two-sided')
            except ValueError: # Happens if all values are identical
                stat, p_value = 0, 1.0
            evaluation_results[region][p_name] = {
                'median_relative_error': median_err,
                'p_value': p_value
            }
            print(f"  - {p_name:<10}: Median Error = {median_err:+.2%}, p-value = {p_value:.4f}")
    if not display_plots:
        return evaluation_results
    num_params = ground_truth_params.shape[2]
    fig, axes = plt.subplots(num_params, 3, figsize=(15, 4 * num_params), sharex=True, sharey=True)
    fig.suptitle("Visual Comparison of PK Parameter Maps", fontsize=16)
    for i in range(num_params):
        p_name = param_names[i]
        gt_map = ground_truth_params[:, :, i]
        est_map = estimated_params[:, :, i]
        error_map = est_map - gt_map
        vmax = np.percentile(gt_map[gt_map > 0], 99) if (gt_map > 0).any() else 1.0
        vmin = 0
        im_gt = axes[i, 0].imshow(gt_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 0].set_title(f"Ground Truth: {p_name}")
        axes[i, 0].axis('off')
        fig.colorbar(im_gt, ax=axes[i, 0])
        im_est = axes[i, 1].imshow(est_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 1].set_title(f"Your Estimation: {p_name}")
        axes[i, 1].axis('off')
        fig.colorbar(im_est, ax=axes[i, 1])
        err_vmax = np.percentile(np.abs(error_map), 99)
        im_err = axes[i, 2].imshow(error_map, vmin=-err_vmax, vmax=err_vmax, cmap='coolwarm')
        axes[i, 2].set_title(f"Error Map (Est - GT)")
        axes[i, 2].axis('off')
        fig.colorbar(im_err, ax=axes[i, 2])
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return evaluation_results
def plot_spatial_quality(
    recon_img: np.ndarray,
    gt_img: np.ndarray,
    grasp_img: np.ndarray,
    time_frame_index: int,
    filename: str,
    grasp_comparison_filename: str,
    data_range: float, 
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Generates a comparison plot for a single time frame in a 2x4 grid.
    Each row includes: Ground Truth, Reconstruction, Error Map, and SSIM Map.
    Args:
        recon_img (np.ndarray): Your model's reconstructed image for this frame.
        gt_img (np.ndarray): The ground truth image for this frame.
        grasp_img (np.ndarray): The GRASP reconstruction image for this frame.
        time_frame_index (int): The index of the time frame for titling.
        filename (str): The path to save the output plot.
    """
    error_map_dl = recon_img - gt_img
    error_map_grasp = grasp_img - gt_img
    ssim_dl, ssim_map_dl = ssim_map_func(gt_img, recon_img, data_range=data_range, full=True)
    ssim_grasp, ssim_map_grasp = ssim_map_func(gt_img, grasp_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(2, 4, figsize=(24, 12))
    fig.suptitle(f"Spatial Quality Comparison at Time Frame {time_frame_index} with AF {acceleration} and SPF {spokes_per_frame}", fontsize=20)
    axes[0, 0].imshow(gt_img, cmap='gray')
    axes[0, 0].set_title("Ground Truth")
    axes[0, 1].imshow(recon_img, cmap='gray')
    axes[0, 1].set_title("DL Reconstruction")
    im_err_dl = axes[0, 2].imshow(error_map_dl, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[0, 2].set_title("DL Error Map (Recon - GT)")
    fig.colorbar(im_err_dl, ax=axes[0, 2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[0, 3].imshow(ssim_map_dl, cmap='viridis', vmin=0, vmax=1)
    axes[0, 3].set_title(f"DL SSIM Map (SSIM Recon vs GT: {round(ssim_dl, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[0, 3], fraction=0.046, pad=0.04)
    axes[1, 0].imshow(gt_img, cmap='gray')
    axes[1, 0].set_title("Ground Truth")
    axes[1, 1].imshow(grasp_img, cmap='gray')
    axes[1, 1].set_title("GRASP Reconstruction")
    im_err_grasp = axes[1, 2].imshow(error_map_grasp, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[1, 2].set_title("GRASP Error Map (Recon - GT)")
    fig.colorbar(im_err_grasp, ax=axes[1, 2], fraction=0.046, pad=0.04)
    im_ssim_grasp = axes[1, 3].imshow(ssim_map_grasp, cmap='viridis', vmin=0, vmax=1)
    axes[1, 3].set_title(f"GRASP SSIM Map (SSIM Recon vs GT: {round(ssim_grasp, 3)})")
    fig.colorbar(im_ssim_grasp, ax=axes[1, 3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    plt.savefig(filename)
    plt.close()
    error_map = recon_img - grasp_img
    ssim, ssim_map = ssim_map_func(grasp_img, recon_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(1, 4, figsize=(24, 6))
    fig.suptitle(f"DL vs GRASP Comparison at Time Frame {time_frame_index} with AF {acceleration} and SPF {spokes_per_frame}", fontsize=20)
    axes[0].imshow(grasp_img, cmap='gray')
    axes[0].set_title("GRASP Reconstruction")
    axes[1].imshow(recon_img, cmap='gray')
    axes[1].set_title("DL Reconstruction")
    im_err_dl = axes[2].imshow(error_map, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[2].set_title("Error Map (DL Recon - GRASP)")
    fig.colorbar(im_err_dl, ax=axes[2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[3].imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)
    axes[3].set_title(f"SSIM Map (SSIM between DL and GRASP Recons: {round(ssim, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    print("SSIM between GRASP and DL Recon: ", ssim)
    plt.savefig(grasp_comparison_filename)
    plt.close()
def plot_temporal_curves(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    masks: dict,
    time_points: np.ndarray,
    filename: str, 
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Plots the mean signal intensity vs. time for different tissue regions.
    This is CRITICAL for debugging PK model fitting.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        masks (dict): Dictionary of boolean NumPy masks for different regions.
        time_points (np.ndarray): The time vector for the x-axis.
        filename (str): The path to save the output plot.
    """
    regions = [r for r in ['malignant', 'glandular', 'muscle'] if r in masks and masks[r].any()]
    if not regions:
        print("No relevant regions found in mask to plot temporal curves.")
        return
    fig, axes = plt.subplots(1, len(regions), figsize=(7 * len(regions), 5), sharey=True)
    if len(regions) == 1: axes = [axes] # Ensure axes is always a list
    fig.suptitle(f"Temporal Fidelity: Mean Signal vs. Time (AF = {acceleration}, SPF = {spokes_per_frame})", fontsize=16)
    for i, region in enumerate(regions):
        mask = masks[region]
        gt_curve = [gt_img_stack[:, :, t][mask].mean() for t in range(gt_img_stack.shape[2])]
        recon_curve = [recon_img_stack[:, :, t][mask].mean() for t in range(recon_img_stack.shape[2])]
        grasp_curve = [grasp_img_stack[:, :, t][mask].mean() for t in range(grasp_img_stack.shape[2])]
        if region == 'malignant':
            recon_correlation, _ = pearsonr(recon_curve, gt_curve)
            grasp_correlation, _ = pearsonr(grasp_curve, gt_curve)
        axes[i].plot(time_points, gt_curve, 'k-', label='Ground Truth', linewidth=2, marker='o')
        axes[i].plot(time_points, recon_curve, 'r--', label='DL Recon', marker='o')
        axes[i].plot(time_points, grasp_curve, 'b:', label='GRASP Recon', marker='o')
        axes[i].set_title(f"Region: {region.capitalize()}")
        axes[i].set_xlabel("Time (s)")
        axes[i].grid(True)
        axes[i].legend()
    axes[0].set_ylabel("Mean Signal Intensity")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return recon_correlation, grasp_correlation
def plot_single_temporal_curve(
    img_stack: np.ndarray,
    masks: Dict[str, np.ndarray],
    time_points: np.ndarray,
    num_frames: int,
    filename: str,
    acceleration: float,
    spokes_per_frame: int,
    frames_to_show: List[int] = None,
):
    """
    Generates a comprehensive analysis plot for a single sample, showing the
    Tumor Contrast Enhancement Curve (CEC) and corresponding image frames with
    the tumor Region of Interest (ROI) highlighted.
    This function is modified to produce a detailed analysis plot for the
    'malignant' tissue type, using the ground truth data.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        grasp_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        masks (dict): Dictionary of boolean NumPy masks. Expects a 'malignant' key.
        time_points (np.ndarray): The time vector for the x-axis (e.g., frame numbers).
        filename (str): The path to save the output plot.
        sample_name (str): The name of the sample for the main plot title.
        frames_to_show (List[int]): A list of 4 frame indices to display in the
                                    image grid and highlight on the curve.
                                    If None, defaults to [0, 6, 13, 20].
    """
    region_key = 'malignant'
    if region_key not in masks or not masks[region_key].any():
        print(f"'{region_key}' mask not found or is empty. Skipping plot generation.")
        return
    tumor_mask = masks[region_key]
    if frames_to_show is None:
        interval = round(num_frames / 4)
        frames_to_show = [0, interval, 2*interval, num_frames-1]
    if len(frames_to_show) != 4:
        raise ValueError(f"This function is designed to show exactly 4 frames, but {len(frames_to_show)} were provided.")
    fig = plt.figure(figsize=(20, 8.5))
    fig.suptitle(f"Tumor Enhancement Over Time (AF = {acceleration}, SPF = {spokes_per_frame})")
    gs = gridspec.GridSpec(2, 4, figure=fig, hspace=0.1, wspace=0.1)
    ax_curve = fig.add_subplot(gs[:, 0:2])
    ax_imgs = [
        fig.add_subplot(gs[0, 2]), fig.add_subplot(gs[0, 3]),
        fig.add_subplot(gs[1, 2]), fig.add_subplot(gs[1, 3])
    ]
    mean_curve = [img_stack[:, :, t][tumor_mask].mean() for t in range(img_stack.shape[2])]
    ax_curve.plot(time_points, mean_curve, 'o-', label='Mean Tumor Signal', linewidth=2, markersize=6)
    highlight_times = [time_points[i] for i in frames_to_show]
    highlight_vals = [mean_curve[i] for i in frames_to_show]
    ax_curve.plot(highlight_times, highlight_vals, 'r*', markersize=18, zorder=10) # zorder to ensure stars are on top
    ax_curve.set_title("Tumor Contrast Enhancement Curve (CEC)", fontsize=18, pad=10)
    ax_curve.set_xlabel("Time Frame", fontsize=16)
    ax_curve.set_ylabel("Mean Signal Intensity", fontsize=16)
    ax_curve.legend(fontsize=14)
    ax_curve.grid(True, linestyle='--')
    ax_curve.tick_params(axis='both', which='major', labelsize=14)
    contours = find_contours(tumor_mask, 0.5)
    for i, frame_idx in enumerate(frames_to_show):
        ax = ax_imgs[i]
        image = img_stack[:, :, frame_idx]
        ax.imshow(image, cmap='gray')#, vmin=vmin, vmax=vmax)
        for contour in contours:
            ax.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='red')
        ax.set_title(f"Frame {frame_idx}", fontsize=16)
        ax.axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect for suptitle
    plt.savefig(filename, bbox_inches='tight', dpi=150)
    plt.close(fig)
def plot_time_series(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    filename: str,
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Plots the middle 5 time points for Ground Truth, DL Recon, and GRASP.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        filename (str): The path to save the output plot.
    """
    num_frames = gt_img_stack.shape[2]
    indices = np.linspace(0, num_frames - 1, 5, dtype=int)
    fig, axes = plt.subplots(3, 5, figsize=(25, 15))
    fig.suptitle(f"Temporal Series Comparison (AF = {acceleration}, SPF = {spokes_per_frame})", fontsize=20)
    for i, frame_idx in enumerate(indices):
        img = gt_img_stack[:, :, frame_idx]
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f"GT: Frame {frame_idx}")
        axes[0, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = recon_img_stack[:, :, frame_idx]
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].set_title(f"DL: Frame {frame_idx}")
        axes[1, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = grasp_img_stack[:, :, frame_idx]
        axes[2, i].imshow(img, cmap='gray')
        axes[2, i].set_title(f"GRASP: Frame {frame_idx}")
        axes[2, i].axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(filename)
    plt.close()
def tofts_model(t, Ktrans, ve, aif_t, aif_c):
    """Calculates the tissue concentration curve using the standard Tofts model."""
    ve = max(ve, 1e-6)
    interp_func = PchipInterpolator(aif_t, aif_c, extrapolate=True)
    aif_interp = interp_func(t)
    impulse_response = Ktrans * np.exp(-t * (Ktrans / ve))
    dt = t[1] - t[0] if len(t) > 1 else 1.0
    Ct = np.convolve(aif_interp, impulse_response, mode='full')[:len(t)] * dt
    return Ct
def signal_to_concentration(signal_curve, S0_pixel, T10_pixel, TR, r1, flip_angle_rad):
    """Converts an MRI signal curve S(t) to a concentration curve C(t)."""
    if S0_pixel < 1e-6:
        return np.zeros_like(signal_curve)
    norm_signal = signal_curve / S0_pixel
    sin_fa, cos_fa = np.sin(flip_angle_rad), np.cos(flip_angle_rad)
    denominator = sin_fa - norm_signal * cos_fa
    E1 = (sin_fa - norm_signal) / np.maximum(denominator, 1e-9)
    E1 = np.maximum(E1, 1e-9)
    R1_t = -np.log(E1) / TR
    R10 = 1.0 / T10_pixel
    concentration_curve = (R1_t - R10) / r1
    return np.maximum(0, concentration_curve)
def estimate_pk_parameters(
    reconstructed_images: np.ndarray,
    aif_t: np.ndarray,
    aif_c: np.ndarray,
    S0_map: np.ndarray,
    T10_map: np.ndarray,
    TR: float = 4.87e-3,
    r1: float = 4.3,
    flip_angle_deg: float = 10.0
) -> np.ndarray:
    """
    Estimates pharmacokinetic parameters (Ktrans, ve) from reconstructed DCE-MRI images.
    Args:
        reconstructed_images (np.ndarray): A (H, W, Time) array of dynamic images,
                                           THIS IS THE OUTPUT FROM YOUR DL MODEL.
        aif_t, aif_c (np.ndarray): The time points and concentrations for the AIF.
        S0_map, T10_map (np.ndarray): Baseline maps from the ground truth DRO.
        TR, r1, flip_angle_deg: Sequence parameters.
    Returns:
        np.ndarray: A (H, W, 4) array containing the estimated [ve, Ktrans, 0, 0] maps.
    """
    height, width, num_frames = reconstructed_images.shape
    flip_angle_rad = np.deg2rad(flip_angle_deg)
    time_points = aif_t
    ktrans_map = np.zeros((height, width))
    ve_map = np.zeros((height, width))
    fitting_func = lambda t, Ktrans, ve: tofts_model(t, Ktrans, ve, aif_t, aif_c)
    DEBUG_PIXEL_R, DEBUG_PIXEL_C = 150, 150
    print("Estimating PK parameters from the reconstructed images...")
    for r in tqdm(range(height), desc="Fitting PK Model"):
        for c in range(width):
            if S0_map[r, c] < np.mean(S0_map) * 0.1:
                continue
            signal_curve = np.abs(reconstructed_images[r, c, :])
            concentration_curve = signal_to_concentration(
                signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
            )
            try:
                initial_guess = [0.1 / 60, 0.2] # Ktrans in s^-1
                bounds = ([0, 0], [2.0 / 60, 1.0])
                params, _ = curve_fit(
                    fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                )
                ktrans_map[r, c] = params[0] * 60 # Convert from s^-1 to min^-1
                ve_map[r, c] = params[1]
            except RuntimeError:
                pass # Fit failed, leave as 0
            if r == DEBUG_PIXEL_R and c == DEBUG_PIXEL_C:
                print(f"\n--- DEBUGGING PIXEL ({r}, {c}) ---")
                signal_curve = np.abs(reconstructed_images[r, c, :])
                concentration_curve = signal_to_concentration(
                    signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
                )
                plt.figure(figsize=(10, 6))
                plt.plot(time_points, concentration_curve, 'bo', label='Measured Concentration (from DL Recon)')
                try:
                    params, _ = curve_fit(
                        fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                    )
                    ktrans_fit, ve_fit = params
                    fitted_curve = tofts_model(time_points, ktrans_fit, ve_fit, aif_t, aif_c)
                    plt.plot(time_points, fitted_curve, 'r-', label=f'Tofts Fit (Ktrans={ktrans_fit*60:.3f}, ve={ve_fit:.3f})')
                except RuntimeError:
                    plt.title(f"DEBUG: Curve fit FAILED for pixel ({r}, {c})")
                plt.xlabel("Time (s)")
                plt.ylabel("Concentration")
                plt.legend()
                plt.grid(True)
                plt.savefig(f"debug_pixel_fit_{r}_{c}.png")
                plt.close()
                print("--- DEBUG PLOT SAVED ---")
    zeros_map = np.zeros_like(ktrans_map)
    estimated_pk_map = np.stack([ve_map, ktrans_map, zeros_map, zeros_map], axis=-1)
    return estimated_pk_map
def eval_grasp(kspace, csmap, ground_truth, grasp_recon, physics, device, output_dir):
    grasp_recon_complex = rearrange(to_torch_complex(grasp_recon).squeeze(), 'h t w -> h w t')
    kspace = kspace.squeeze()
    grasp_kspace = physics(False, grasp_recon_complex.to(csmap.dtype), csmap)
    dc_mse_grasp, dc_mae_grasp = calc_dc(grasp_kspace, kspace, device)
    grasp_recon_np = grasp_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    c = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_recon = torch.tensor(c * grasp_recon_np, device=device)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    grasp_mag = torch.sqrt(grasp_recon[:, 0, ...]**2 + grasp_recon[:, 1, ...]**2)
    grasp_mag = rearrange(grasp_mag, 'c h t w -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp = calc_image_metrics(grasp_mag.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    return ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp
def eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, spokes_per_frame, output_dir, label, device):
    acceleration = round(acceleration.item(), 1)
    x_recon_complex = to_torch_complex(x_recon).squeeze()
    kspace = kspace.squeeze()
    recon_kspace = physics(False, x_recon_complex, csmap)
    dc_mse, dc_mae = calc_dc(recon_kspace, kspace, device)
    x_recon_np = x_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    grasp_recon_np = grasp_img.cpu().numpy()
    c = np.dot(x_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(x_recon_np.flatten(), x_recon_np.flatten())
    recon_complex_scaled = torch.tensor(c * x_recon_np, device=device)
    c_grasp = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_img = torch.tensor(c_grasp * grasp_recon_np, device=device)
    recon_mag_scaled = torch.sqrt(recon_complex_scaled[:, 0, ...]**2 + recon_complex_scaled[:, 1, ...]**2)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    recon_mag_scaled = rearrange(recon_mag_scaled, 'c h w t -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim, psnr, mse, lpips = calc_image_metrics(recon_mag_scaled.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    grasp_recon_complex_np = rearrange(to_torch_complex(grasp_img).squeeze(), 'h t w -> h w t').cpu().numpy()
    grasp_mag_np = np.abs(grasp_recon_complex_np)
    x_recon_complex_np = to_torch_complex(recon_complex_scaled).squeeze().cpu().numpy()
    gt_squeezed = ground_truth.squeeze()  # Shape: (C, T, H, W) -> (2, 22, 320, 320)
    gt_rearranged = rearrange(gt_squeezed, 'c t h w -> t c h w') # Shape: (22, 320, 320, 2)
    gt_complex_tensor = to_torch_complex(gt_rearranged) # Shape: (22, 320, 320)
    gt_final_tensor = rearrange(gt_complex_tensor, 't h w -> h w t') # Shape: (320, 320, 22)
    gt_complex_np = gt_final_tensor.cpu().numpy()
    recon_mag_np = np.abs(x_recon_complex_np)
    gt_mag_np = np.abs(gt_complex_np)
    masks_np = {key: val.cpu().numpy().squeeze().astype(bool) for key, val in mask.items()}
    num_frames = recon_mag_np.shape[2]
    aif_time_points = np.linspace(0, 150, num_frames)
    print("\nGenerating diagnostic plots...")
    if mask['malignant'].any() and label is not None:
        peak_frame = num_frames // 3
        data_range = gt_mag_np[:, :, peak_frame].max() - gt_mag_np[:, :, peak_frame].min()
        plot_spatial_quality(
            recon_img=recon_mag_np[:, :, peak_frame],
            gt_img=gt_mag_np[:, :, peak_frame],
            grasp_img=grasp_mag_np[:, :, peak_frame],
            time_frame_index=peak_frame,
            filename=os.path.join(output_dir, f"spatial_quality_{label}.png"),
            grasp_comparison_filename=os.path.join(output_dir, f"grasp_comparison_{label}.png"),
            data_range=data_range,
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        recon_corr, grasp_corr = plot_temporal_curves(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            filename=os.path.join(output_dir, f"temporal_curves_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        plot_single_temporal_curve(
            img_stack=recon_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            num_frames=num_frames,
            filename=os.path.join(output_dir, f"recon_temporal_curve_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        plot_time_series(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            filename=os.path.join(output_dir, f"time_points_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        print("Diagnostic plots saved.")
    else:
        recon_corr, grasp_corr = None, None
    return ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr

=== mc.py ===
from typing import Union
import torch
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from radial import to_torch_complex
class MCLoss(Loss):
    r"""
    Measurement consistency loss
    This loss enforces that the reconstructions are measurement-consistent, i.e., :math:`y=\forw{\inverse{y}}`.
    The measurement consistency loss is defined as
    .. math::
        \|y-\forw{\inverse{y}}\|^2
    where :math:`\inverse{y}` is the reconstructed signal and :math:`A` is a forward operator.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param Metric, torch.nn.Module metric: metric used for computing data consistency, which is set as the mean squared error by default.
    """
    def __init__(self, model_type, metric: Union[Metric, torch.nn.Module] = torch.nn.MSELoss()):
        super(MCLoss, self).__init__()
        self.name = "mc"
        self.metric = metric
        self.device = torch.device("cuda")
        self.model_type = model_type
    def forward(self, y, x_net, physics, csmap, **kwargs):
        r"""
        Computes the measurement splitting loss
        :param torch.Tensor y: measurements.
        :param torch.Tensor x_net: reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: forward operator associated with the measurements.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.model_type == "CRNN":
            return self.metric(physics.A(x_net, csmap), y)
        elif self.model_type == "LSFPNet":
            x_net = to_torch_complex(x_net)
            y_hat = physics(inv=False, data=x_net, smaps=csmap).to(self.device)
            y_hat = torch.stack([y_hat.real, y_hat.imag], dim=-1)
            y = torch.stack([y.real, y.imag], dim=-1)
            return self.metric(y_hat, y)

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
from typing import Union, List, Optional
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Can either use a fixed set of slices or randomly sample N slices per volume
        at the start of each epoch.
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx: Optional[Union[int, range]] = 41,
        num_random_slices: Optional[int] = None,  # New parameter for random sampling
        N_time=8,
        N_coils=16,
        spf_aug=False,
        spokes_per_frame=None,
        weight_accelerations=False, 
        initial_spokes_range=[8, 16, 24, 36]
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            patient_ids (list): List of patient IDs to filter the files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset.
            file_pattern (str): Glob pattern to match your HDF5 files.
            slice_idx (int, range, optional): A fixed slice index or range of indices to use.
                                              This is ignored if num_random_slices is set.
            num_random_slices (int, optional): If provided, the dataset will randomly sample
                                               this many slices from each volume at the beginning
                                               of each epoch.
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.num_random_slices = num_random_slices
        self.N_time = N_time
        self.N_coils = N_coils
        self.spf_aug = spf_aug
        self.weight_acc = weight_accelerations
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
        if self.num_random_slices is not None:
            print(f"Initializing in random slice sampling mode with N={self.num_random_slices} slices per volume.")
            self.volume_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    num_slices = f[self.dataset_key].shape[0]
                    self.volume_map.append((fp, num_slices))
            self.resample_slices()
        else:
            print(f"Initializing in fixed slice mode with slice_idx={self.slice_idx}.")
            self.slice_index_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    ds = f[self.dataset_key]
                    num_slices = ds.shape[0]
                slices_to_add = []
                if isinstance(self.slice_idx, int):
                    if self.slice_idx < num_slices:
                        slices_to_add = [self.slice_idx]
                    else:
                        print(f"Warning: slice_idx {self.slice_idx} is out of bounds for {fp} "
                              f"(size {num_slices}). Skipping this file for this slice.")
                elif isinstance(self.slice_idx, range):
                    slices_to_add = [s for s in self.slice_idx if s < num_slices]
                    if len(slices_to_add) < len(self.slice_idx):
                        print(f"Warning: Some requested slices were out of bounds for {fp}. "
                              f"Using only the valid slice indices from the provided range.")
                else:
                    raise TypeError(f"slice_idx must be an int, range, or None, but got {type(self.slice_idx)}")
                for z in slices_to_add:
                    self.slice_index_map.append((fp, z))
        print(f"Dataset initialized with {len(self.slice_index_map)} total slice examples.")
        self.spokes_per_frame = spokes_per_frame
        self.spokes_range = initial_spokes_range
        self.update_spokes_weights()
    def update_spokes_weights(self):
        if self.weight_acc:
            self.spf_weights = [1.0 / spf for spf in self.spokes_range]
        else:
            self.spf_weights = [1.0 for spf in self.spokes_range]
    def resample_slices(self):
        """
        Resamples N unique slices from each volume. This should be called at the
        beginning of each training epoch to ensure the model sees different data.
        """
        if self.num_random_slices is None:
            return
        self.slice_index_map = []
        for file_path, num_slices in self.volume_map:
            if num_slices >= self.num_random_slices:
                selected_slices = random.sample(range(num_slices), self.num_random_slices)
            else:
                print(f"Warning: Volume {os.path.basename(file_path)} has only {num_slices} slices, "
                      f"which is less than the requested {self.num_random_slices}. Using all available slices.")
                selected_slices = list(range(num_slices))
            for z in selected_slices:
                self.slice_index_map.append((file_path, z))
    def load_dynamic_img(self, patient_id, slice):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{slice:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data)
    def load_csmaps(self, patient_id, slice):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{slice:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.slice_index_map)
    def __getitem__(self, idx):
        file_path, current_slice_idx = self.slice_index_map[idx]
        current_slice_idx = int(current_slice_idx)
        patient_id = file_path.split('/')[-1].strip('.h5')
        csmap = self.load_csmaps(patient_id, current_slice_idx)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[current_slice_idx]
        if self.spf_aug or self.spokes_per_frame:
            total_spokes = kspace_slice.shape[0] * kspace_slice.shape[2]
            N_samples = kspace_slice.shape[-1]
            kspace = rearrange(kspace_slice, 't c sp sam -> t sp c sam')
            kspace_flat = kspace.contiguous().view(total_spokes, self.N_coils, N_samples)
            if self.spf_aug:
                print("setting random spokes per frame...")
                spokes_per_frame = random.choices(self.spokes_range, self.spf_weights, k=1)[0]
            else:
                spokes_per_frame = self.spokes_per_frame
                print(f"training with fixed spokes per frame ({spokes_per_frame})")
            N_time = total_spokes // spokes_per_frame
            kspace_binned = kspace_flat.view(N_time, spokes_per_frame, self.N_coils, N_samples)
            kspace_slice = rearrange(kspace_binned, 't sp c sam -> t c sp sam')
        else:
            N_time = self.N_time
            N_samples = kspace_slice.shape[-1]
            spokes_per_frame = kspace_slice.shape[-2]
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        kspace_final = torch.flip(kspace_final, dims=[-1])
        csmap_tensor = torch.from_numpy(csmap)
        csmap_tensor = torch.rot90(csmap_tensor, k=2, dims=[-2, -1])
        csmap = csmap_tensor.numpy()
        return kspace_final, csmap, N_samples, spokes_per_frame, N_time
class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids, spokes_per_frame=36, num_frames=8):
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.model_type = model_type
        self.spokes_per_frame = spokes_per_frame
        self.num_frames = num_frames
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            print("loading grasp image from ", grasp_path)
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            print("setting grasp img to zero")
            grasp_recon_torch = 0
        kspace_path = os.path.join(sample_dir, f'simulated_kspace_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(kspace_path):
            kspace_complex = np.load(kspace_path, allow_pickle=True)
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            kspace_torch = kspace_path
        ground_truth_complex = dro['ground_truth_images']
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask
class SimulatedSPFDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.spokes_per_frame = 16
        self.num_frames = 18
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        print(f"  Testing {self.spokes_per_frame} spokes/frame with {self.num_frames} frames.")
        print("loading data from ", sample_dir)
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            grasp_recon_torch = 0
        ground_truth_complex = dro['ground_truth_images']
        smap_torch = rearrange(torch.tensor(csmaps), 'h w c -> c h w').unsqueeze(0)
        simImg_torch = torch.tensor(ground_truth_complex).to(torch.cfloat)
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        return smap_torch, simImg_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask

