=== lsp.py ===
import torch
import numpy as np
from time import time
dtype=torch.complex64
def Project_inf(x, c):
    x_max = torch.maximum((abs(x) / c), torch.tensor(1))
    x_max = x_max.to(dtype)
    s = torch.div(x, x_max)
    return s
def Wxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0:x.shape[1]-2] = x[:, 1:x.shape[1]-1]
    temp_x[:, x.shape[1]-1] = temp_x[:, x.shape[1]-1]
    res = temp_x - x
    return res
def Wtxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0] = temp_x[:, 0]
    temp_x[:, 1:x.shape[1]-1] = x[:, 0:x.shape[1]-2]
    res = temp_x - x
    res[:, 0] = -x[:, 0]
    res[:, x.shape[1]-1] = x[:, x.shape[1]-2]
    return res
def LSP(param_E, param_d, param_lambda_L, param_lambda_S, param_nite, param_tol):
    '''
    :param param_E:
    :param param_d:
    :param param_lambda_L:
    :param param_lambda_S:
    :param param_nite:
    :param param_tol:
    :return:
    '''
    M = param_E(inv=True, data=param_d)
    nx, ny, nt = M.size()
    L = torch.zeros([nx * ny, nt], dtype=dtype)
    S = torch.zeros([nx * ny, nt], dtype=dtype)
    p_L = torch.zeros([nx * ny, nt], dtype=dtype)
    p_S = torch.zeros([nx * ny, nt], dtype=dtype)
    gamma = 0.5
    lambda_step = 1/10
    c = lambda_step/gamma
    loss = torch.zeros(param_nite, dtype=float)
    for itr in range(0, param_nite):
        temp_data = torch.reshape(L+S, [nx, ny, nt])
        gradient = param_E(inv=True, data=param_E(inv=False, data=temp_data) - param_d)
        gradient = torch.reshape(gradient, [nx * ny, nt])
        y_L = L - gamma * gradient - gamma * p_L
        Par_L = c * y_L + p_L
        Ut, St, Vt = torch.svd(Par_L)
        temp_St = torch.diag(Project_inf(St, param_lambda_L))
        p_L = Ut.mm(temp_St).mm(Vt.T)
        L = L - gamma * gradient - gamma * p_L
        y_S = S - gamma * gradient - gamma * Wtxs(p_S)
        Par_S = c * Wxs(y_S) + p_S
        p_S = Project_inf(Par_S, param_lambda_S)
        S = S - gamma * gradient - gamma * Wtxs(p_S)
        loss[itr] = 0
        print(' iteration: %d/%d, Loss: %f' % (itr+1, param_nite, loss[itr]))
    Ut, St, Vt = torch.svd(L)
    L = torch.matmul((Ut[:, 0]*St[0]).unsqueeze(1), Vt[:, 0].unsqueeze(0))
    L = torch.reshape(L, [nx, ny, nt])
    S = torch.reshape(S, [nx, ny, nt])
    return L, S, loss

=== lsfpnet.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class BasicBlock(nn.Module):
    def __init__(self):
        super(BasicBlock, self).__init__()
        self.lambda_L = nn.Parameter(torch.tensor([0.0025]))
        self.lambda_S = nn.Parameter(torch.tensor([0.05]))
        self.lambda_spatial_L = nn.Parameter(torch.tensor([5e-2]))
        self.lambda_spatial_S = nn.Parameter(torch.tensor([5e-2]))
        self.gamma = nn.Parameter(torch.tensor([0.5]))
        self.lambda_step = nn.Parameter(torch.tensor([1/10]))
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, 32, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, 32, 3, 3, 3)))
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S):
        c = self.lambda_step / self.gamma
        nx, ny, nt = M0.size()
        temp_data = torch.reshape(L + S, [nx, ny, nt])
        temp_data = param_E(inv=False, data=temp_data).to(param_d.device)
        gradient = param_E(inv=True, data=temp_data - param_d)
        gradient = torch.reshape(gradient, [nx * ny, nt]).to(param_d.device)
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(torch.squeeze(pb_L), [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        Ut, St, Vt = torch.linalg.svd((c * y_L + pt_L), full_matrices=False)
        temp_St = torch.diag(Project_inf(St, self.lambda_L))
        pt_L = Ut.mm(temp_St).mm(Vt)
        temp_y_L_input = torch.cat((torch.real(y_L), torch.imag(y_L)), 0).to(torch.float32)
        temp_y_L_input = torch.reshape(temp_y_L_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_L = F.conv3d(temp_y_L_input, self.conv1_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L = F.conv3d(temp_y_L, self.conv2_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L_output = F.conv3d(temp_y_L, self.conv3_forward_l, padding=1)
        temp_y_L = temp_y_L_output + p_L
        temp_y_L = temp_y_L[0, :, :, :, :] + 1j * temp_y_L[1, :, :, :, :]
        p_L = Project_inf(c * temp_y_L, self.lambda_spatial_L)
        p_L = torch.cat((torch.real(p_L), torch.imag(p_L)), 0).to(torch.float32)
        p_L = torch.reshape(p_L, [2, 32, nx, ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L_output = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(pb_L_output, [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        adjloss_L = temp_y_L_output * p_L - pb_L_output * temp_y_L_input
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, self.lambda_S)
        temp_y_S_input = torch.cat((torch.real(y_S), torch.imag(y_S)), 0).to(torch.float32)
        temp_y_S_input = torch.reshape(temp_y_S_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_S = F.conv3d(temp_y_S_input, self.conv1_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S = F.conv3d(temp_y_S, self.conv2_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S_output = F.conv3d(temp_y_S, self.conv3_forward_s, padding=1)
        temp_y_Sp = temp_y_S_output + p_S
        temp_y_Sp = temp_y_Sp[0, :, :, :, :] + 1j * temp_y_Sp[1, :, :, :, :]
        p_S = Project_inf(c * temp_y_Sp, self.lambda_spatial_S)
        p_S = torch.cat((torch.real(p_S), torch.imag(p_S)), 0).to(torch.float32)
        p_S = torch.reshape(p_S, [2, 32, nx, ny, nt])
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S_output = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S_output, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        adjloss_S = temp_y_S_output * p_S - pb_S_output * temp_y_S_input
        return [L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S]
class LSFPNet(nn.Module):
    def __init__(self, LayerNo):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        for ii in range(LayerNo):
            onelayer.append(BasicBlock())
        self.fcs = nn.ModuleList(onelayer)
    def forward(self, M0, param_E, param_d):
        M0 = M0[..., 0] + 1j * M0[..., 1]
        param_d = param_d[..., 0] + 1j * param_d[..., 1]
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, 32, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, 32, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S]
class EWrapper:
    """
    Make `physics` look like the callable interface expected by LSFP-Net:
        E(inv=False, data=image)  -> k-space
        E(inv=True , data=data )  -> image
    Scaling is handled *outside*, so we just delegate.
    """
    def __init__(self, physics, csmap):
        self.physics, self.csmap = physics, csmap
    def __call__(self, *, inv: bool, data: torch.Tensor) -> torch.Tensor:
        if inv:   # adjoint
            data = from_torch_complex(data.unsqueeze(0))
            data = self.physics.A_adjoint(data, self.csmap)
            data = to_torch_complex(data).squeeze(0)
            return rearrange(data, 't h w -> h w t')
        else:     # forward
            data = from_torch_complex(data.unsqueeze(0))
            data = self.physics.A(data, self.csmap)
            return to_torch_complex(data).squeeze(0)
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
    @staticmethod
    def _normalise(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max()                       # scalar, grads OK
        return zf / scale, data / scale, scale
    def forward(self, y, physics, csmap, **kwargs):
        x_init = physics.A_adjoint(y, csmap)
        x_init_norm, y_norm, scale = self._normalise(x_init, y)
        E  = EWrapper(physics, csmap)
        if len(y_norm.shape) == 6:
            y_norm = rearrange(y_norm[0], 'c t co sp sam -> t co sp sam c')
        elif len(y_norm.shape) == 5:
            y_norm = rearrange(y_norm[0], 'c t sp sam -> t sp sam c')
        x_init_norm = rearrange(x_init_norm[0], 'c t h w -> t h w c')
        L, S, *_ = self.backbone_net(x_init_norm, E, y_norm)
        recon = (L + S) * scale                  # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat

=== radial.py ===
import deepinv as dinv
import numpy as np
import torch
import torch.nn as nn
from deepinv.physics.time import TimeMixin
from einops import rearrange
from torchkbnufft import KbNufft, KbNufftAdjoint
from noise import ZeroNoise
import warnings
from torch import Tensor
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class Physics(torch.nn.Module):  # parent class for forward models
    r"""
    Parent class for forward operators
    It describes the general forward measurement process
    .. math::
        y = \noise(\forw(x))
    where :math:`x` is an image of :math:`n` pixels, :math:`y` is the measurements of size :math:`m`,
    :math:`\forw:\xset\mapsto \yset` is a deterministic mapping capturing the physics of the acquisition
    and :math:`\noise:\yset\mapsto \yset` is a stochastic mapping which characterizes the noise affecting
    the measurements.
    :param Callable A: forward operator function which maps an image to the observed measurements :math:`x\mapsto y`.
    :param deepinv.physics.NoiseModel, Callable noise_model: function that adds noise to the measurements :math:`N(z)`.
        See the noise module for some predefined functions.
    :param Callable sensor_model: function that incorporates any sensor non-linearities to the sensing process,
        such as quantization or saturation, defined as a function :math:`\eta(z)`, such that
        :math:`y=\eta\left(N(A(x))\right)`. By default, the `sensor_model` is set to the identity :math:`\eta(z)=z`.
    :param int max_iter: If the operator does not have a closed form pseudoinverse, the gradient descent algorithm
        is used for computing it, and this parameter fixes the maximum number of gradient descent iterations.
    :param float tol: If the operator does not have a closed form pseudoinverse, the gradient descent algorithm
        is used for computing it, and this parameter fixes the absolute tolerance of the gradient descent algorithm.
    :param str solver: least squares solver to use. Only gradient descent is available for non-linear operators.
    """
    def __init__(
        self,
        A=lambda x, **kwargs: x,
        noise_model=ZeroNoise(),
        sensor_model=lambda x: x,
        solver="gradient_descent",
        max_iter=50,
        tol=1e-4,
    ):
        super().__init__()
        self.noise_model = noise_model
        self.sensor_model = sensor_model
        self.forw = A
        self.SVD = False  # flag indicating SVD available
        self.max_iter = max_iter
        self.tol = tol
        self.solver = solver
    def __mul__(self, other):
        r"""
        Concatenates two forward operators :math:`A = A_1\circ A_2` via the mul operation
        The resulting operator keeps the noise and sensor models of :math:`A_1`.
        :param deepinv.physics.Physics other: Physics operator :math:`A_2`
        :return: (:class:`deepinv.physics.Physics`) concatenated operator
        """
        warnings.warn(
            "You are composing two physics objects. The resulting physics will not retain the original attributes. "
            "You may instead retrieve attributes of the original physics by indexing the resulting physics."
        )
        return compose(other, self, max_iter=self.max_iter, tol=self.tol)
    def stack(self, other):
        r"""
        Stacks two forward operators :math:`A(x) = \begin{bmatrix} A_1(x) \\ A_2(x) \end{bmatrix}`
        The measurements produced by the resulting model are :class:`deepinv.utils.TensorList` objects, where
        each entry corresponds to the measurements of the corresponding operator.
        Returns a :class:`deepinv.physics.StackedPhysics` object.
        See :ref:`physics_combining` for more information.
        :param deepinv.physics.Physics other: Physics operator :math:`A_2`
        :return: (:class:`deepinv.physics.StackedPhysics`) stacked operator
        """
        return stack(self, other)
    def forward(self, x, csmaps, **kwargs):
        r"""
        Computes forward operator
        .. math::
                y = N(A(x), \sigma)
        :param torch.Tensor, list[torch.Tensor] x: signal/image
        :return: (:class:`torch.Tensor`) noisy measurements
        """
        return self.sensor(self.noise(self.A(x, csmaps, **kwargs), **kwargs))
    def A(self, x, **kwargs):
        r"""
        Computes forward operator :math:`y = A(x)` (without noise and/or sensor non-linearities)
        :param torch.Tensor,list[torch.Tensor] x: signal/image
        :return: (:class:`torch.Tensor`) clean measurements
        """
        return self.forw(x, **kwargs)
    def sensor(self, x):
        r"""
        Computes sensor non-linearities :math:`y = \eta(y)`
        :param torch.Tensor,list[torch.Tensor] x: signal/image
        :return: (:class:`torch.Tensor`) clean measurements
        """
        return self.sensor_model(x)
    def set_noise_model(self, noise_model, **kwargs):
        r"""
        Sets the noise model
        :param Callable noise_model: noise model
        """
        self.noise_model = noise_model
    def noise(self, x, **kwargs) -> Tensor:
        r"""
        Incorporates noise into the measurements :math:`\tilde{y} = N(y)`
        :param torch.Tensor x:  clean measurements
        :param None, float noise_level: optional noise level parameter
        :return: noisy measurements
        """
        return self.noise_model(x, **kwargs)
    def A_dagger(self, y, x_init=None):
        r"""
        Computes an inverse as:
        .. math::
            x^* \in \underset{x}{\arg\min} \quad \|\forw{x}-y\|^2.
        This function uses gradient descent to find the inverse. It can be overwritten by a more efficient pseudoinverse in cases where closed form formulas exist.
        :param torch.Tensor y: a measurement :math:`y` to reconstruct via the pseudoinverse.
        :param torch.Tensor x_init: initial guess for the reconstruction.
        :return: (:class:`torch.Tensor`) The reconstructed image :math:`x`.
        """
        if self.solver == "gradient_descent":
            if x_init is None:
                x_init = self.A_adjoint(y)
            x = x_init
            lr = 1e-1
            loss = torch.nn.MSELoss()
            for _ in range(self.max_iter):
                x = x - lr * self.A_vjp(x, self.A(x) - y)
                err = loss(self.A(x), y)
                if err < self.tol:
                    break
        else:
            raise NotImplementedError(
                f"Solver {self.solver} not implemented for A_dagger"
            )
        return x.clone()
    def set_ls_solver(self, solver, max_iter=None, tol=None):
        r"""
        Change default solver for computing the least squares solution:
        .. math::
            x^* \in \underset{x}{\arg\min} \quad \|\forw{x}-y\|^2.
        :param str solver: solver to use. If the physics are non-linear, the only available solver is `'gradient_descent'`.
            For linear operators, the options are `'CG'`, `'lsqr'`, `'BiCGStab'` and `'minres'` (see :func:`deepinv.optim.utils.least_squares` for more details).
        :param int max_iter: maximum number of iterations for the solver.
        :param float tol: relative tolerance for the solver, stopping when :math:`\|A(x) - y\| < \text{tol} \|y\|`.
        """
        if max_iter is not None:
            self.max_iter = max_iter
        if tol is not None:
            self.tol = tol
        self.solver = solver
    def A_vjp(self, x, v):
        r"""
        Computes the product between a vector :math:`v` and the Jacobian of the forward operator :math:`A` evaluated at :math:`x`, defined as:
        .. math::
            A_{vjp}(x, v) = \left. \frac{\partial A}{\partial x}  \right|_x^\top  v.
        By default, the Jacobian is computed using automatic differentiation.
        :param torch.Tensor x: signal/image.
        :param torch.Tensor v: vector.
        :return: (:class:`torch.Tensor`) the VJP product between :math:`v` and the Jacobian.
        """
        _, vjpfunc = torch.func.vjp(self.A, x)
        return vjpfunc(v)[0]
    def update(self, **kwargs):
        r"""
        Update the parameters of the physics: forward operator and noise model.
        :param dict kwargs: dictionary of parameters to update.
        """
        self.update_parameters(**kwargs)
        if hasattr(self.noise_model, "update_parameters"):
            self.noise_model.update_parameters(**kwargs)
    def update_parameters(self, **kwargs):
        r"""
        Update the parameters of the forward operator.
        :param dict kwargs: dictionary of parameters to update.
        """
        if kwargs:
            for key, value in kwargs.items():
                if (
                    value is not None
                    and hasattr(self, key)
                    and isinstance(value, torch.Tensor)
                ):
                    self.register_buffer(key, value)
class RadialPhysics(Physics):
    def __init__(self, im_size, N_spokes, N_samples, **kwargs):
        super().__init__(**kwargs)
        self.im_size = im_size
        self.N_spokes = N_spokes
        self.N_samples = N_samples
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        grid_size = [int(s * 2.0) for s in im_size]
        self.NUFFT = KbNufft(im_size=im_size, grid_size=grid_size).to(self.device)
        self.AdjNUFFT = KbNufftAdjoint(im_size=im_size, grid_size=grid_size).to(
            self.device
        )
        self.traj, self.sqrt_dcf = self.get_traj_and_dcf()
        self.traj = self.traj.to(self.device)
        self.sqrt_dcf = self.sqrt_dcf.to(self.device)
    def get_traj_and_dcf(self, angle_offset_rad=0.0):
        base_res = self.im_size[0]
        gind = 1
        N_samples = base_res * 2
        if N_samples != self.N_samples:
            print(
                f"Warning: Vendor logic implies N_samples should be {N_samples}, but class has {self.N_samples}. Using class value."
            )
            N_samples = self.N_samples  # Trust the class value passed during init
        base_lin = np.arange(N_samples).reshape(1, -1) - (N_samples // 2)
        tau = 0.5 * (1 + 5**0.5)
        base_rad = np.pi / (gind + tau - 1)
        base_rad = np.pi / (gind + tau - 1)
        spoke_indices = np.arange(self.N_spokes)
        base_rot = (spoke_indices * base_rad + angle_offset_rad).reshape(-1, 1)
        traj_flat = np.zeros((self.N_spokes, N_samples, 2))
        traj_flat[..., 0] = np.cos(base_rot) @ base_lin
        traj_flat[..., 1] = np.sin(base_rot) @ base_lin
        max_radius = N_samples / 2.0
        traj_flat = (traj_flat / max_radius) * np.pi
        traj = torch.from_numpy(traj_flat).float()
        traj_nufft_ready = rearrange(traj, "s i xy -> 1 xy (s i)")
        dcf_vals = torch.sqrt(
            traj_nufft_ready[0, 0, :] ** 2 + traj_nufft_ready[0, 1, :] ** 2
        )
        sqrt_dcf_vals = torch.sqrt(dcf_vals)
        sqrt_dcf = rearrange(sqrt_dcf_vals, "(s i) -> 1 1 (s i)", s=self.N_spokes)
        return traj_nufft_ready, sqrt_dcf
    def A(self, x: torch.Tensor, csmaps, **kwargs) -> torch.Tensor:
        x_complex = to_torch_complex(x).unsqueeze(1)
        k_complex_nufft = self.NUFFT(x_complex, self.traj, csmaps)
        y_complex_weighted = k_complex_nufft * self.sqrt_dcf
        y = from_torch_complex(y_complex_weighted.squeeze(1))
        return rearrange(y, "b c (s i) -> b c s i", s=self.N_spokes)
    def A_adjoint(self, y: torch.Tensor, csmaps, **kwargs) -> torch.Tensor:
        y_flat = rearrange(y, "b c s i -> b c (s i)")
        y_complex = to_torch_complex(y_flat).unsqueeze(1)
        y_dcf_complex = y_complex * self.sqrt_dcf
        if torch.isnan(y_dcf_complex).any():
            print("!!! ERROR: NaN detected in y_dcf_complex in A_adjoint !!!")
        x_complex = self.AdjNUFFT(y_dcf_complex, self.traj, csmaps).squeeze(1)
        return from_torch_complex(x_complex)
class DynamicRadialPhysics(RadialPhysics, TimeMixin):
    def __init__(self, im_size, N_spokes, N_samples, N_time, N_coils=1, **kwargs):
        TimeMixin.__init__(self)
        Physics.__init__(self, **kwargs)
        self.im_size = im_size[:2]  # Static image size
        self.N_spokes = N_spokes  # Spokes PER FRAME
        self.N_samples = N_samples
        self.N_time = N_time
        self.N_coils = N_coils
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        grid_size = [int(s * 2.0) for s in self.im_size]
        self.NUFFT = KbNufft(im_size=self.im_size, grid_size=grid_size).to(self.device)
        self.AdjNUFFT = KbNufftAdjoint(im_size=self.im_size, grid_size=grid_size).to(
            self.device
        )
        total_spokes_in_scan = self.N_spokes * self.N_time
        temp_physics = RadialPhysics(
            self.im_size, N_spokes=total_spokes_in_scan, N_samples=self.N_samples
        )
        full_traj, full_sqrt_dcf = temp_physics.get_traj_and_dcf()
        self.traj_per_frame = rearrange(
            full_traj, "b c (t s i) -> t b c (s i)", t=self.N_time, s=self.N_spokes
        )
        self.sqrt_dcf_per_frame = rearrange(
            full_sqrt_dcf, "b c (t s i) -> t b c (s i)", t=self.N_time, s=self.N_spokes
        )
        self.traj_per_frame = self.traj_per_frame.to(self.device)
        self.sqrt_dcf_per_frame = self.sqrt_dcf_per_frame.to(self.device)
        if self.N_coils == 1:
            self.mask = torch.ones(1, 2, self.N_time, self.N_spokes, self.N_samples).to(
                self.device
            )
        else:
            self.mask = torch.ones(1, 2, self.N_time, self.N_coils, self.N_spokes, self.N_samples).to(
                self.device
            )
    def A(self, x: torch.Tensor, csmap, **kwargs) -> torch.Tensor:
        B, C, T, H, W = x.shape
        output_kspace_frames = []
        csmap = csmap.to(self.device)
        x_complex = to_torch_complex(x) # -> (B, T, H, W)
        for t in range(T):
            x_complex_frame = x_complex[:, t, :, :].unsqueeze(1)  # -> (B, Co, H, W)
            if self.N_coils > 1:
                x_complex_frame = x_complex_frame * csmap.to(x_complex_frame.dtype)
            traj_t = self.traj_per_frame[t]
            sqrt_dcf_t = self.sqrt_dcf_per_frame[t]
            k_complex_nufft = self.NUFFT(x_complex_frame, traj_t)
            y_complex_weighted = k_complex_nufft * sqrt_dcf_t
            if self.N_coils == 1:
                y_frame = from_torch_complex(y_complex_weighted.squeeze(1))
                y_frame_reshaped = rearrange(
                    y_frame, "b c (s i) -> b c s i", s=self.N_spokes
                )
            else: 
                y_frame = from_torch_complex(y_complex_weighted)
                y_frame_reshaped = rearrange(
                    y_frame, "b c co (s i) -> b c co s i", s=self.N_spokes
                )
            output_kspace_frames.append(y_frame_reshaped)
        y = torch.stack(output_kspace_frames, dim=2)  # Stack along the time dimension
        return y * self.mask
    def A_adjoint(self, y: torch.Tensor, csmap: torch.Tensor, **kwargs) -> torch.Tensor:
        if len(y.shape) == 5:
            B, C, T, S, I = y.shape
        elif len(y.shape) == 6:
            B, C, T, Co, S, I = y.shape
        output_image_frames = []
        y_masked = y * self.mask
        for t in range(T):
            y_frame = y_masked[:, :, t, :, :]  # -> (B, C, S, I)
            traj_t = self.traj_per_frame[t]
            sqrt_dcf_t = self.sqrt_dcf_per_frame[t]
            if self.N_coils == 1:
                y_flat = rearrange(y_frame, "b c s i -> b c (s i)")
                y_complex = to_torch_complex(y_flat).unsqueeze(1)
                y_dcf_complex = y_complex * sqrt_dcf_t
                x_complex_frame = self.AdjNUFFT(y_dcf_complex, traj_t).squeeze(1) # -> (B, H, W)
            else:
                y_flat = rearrange(y_frame, "b c co s i -> b c co (s i)")
                y_complex = to_torch_complex(y_flat)
                y_dcf_complex = y_complex * sqrt_dcf_t
                x_complex_frame = self.AdjNUFFT(y_dcf_complex, traj_t) # -> (B, Co, H, W)
            if self.N_coils > 1:
                csmap = csmap.to(x_complex_frame.dtype).to(self.device)
                sens_weighted_imgs = x_complex_frame * csmap.conj()
                combined_numerator = torch.sum(sens_weighted_imgs, dim=1)
                sos_sens_maps = torch.sum(csmap.abs()**2, dim=1)
                epsilon = torch.finfo(sos_sens_maps.dtype).eps
                x_complex_frame = combined_numerator / (sos_sens_maps + epsilon)
            x_frame = from_torch_complex(x_complex_frame)
            output_image_frames.append(x_frame)
        x = torch.stack(output_image_frames, dim=2)  # -> (B, C, T, H, W)
        return x
class RadialDCLayer(nn.Module):
    """
    Final Data Consistency layer.
    It takes the network's current image estimate and the original measurements,
    and returns a new image estimate that is a weighted average in k-space.
    """
    def __init__(
        self,
        physics: nn.Module,  # The DC layer now requires the physics operator
        lambda_init=np.log(np.exp(1) - 1.0) / 1.0,
        learnable=True,
    ):
        super(RadialDCLayer, self).__init__()
        self.learnable = learnable
        self.lambda_ = nn.Parameter(
            torch.ones(1) * lambda_init, requires_grad=self.learnable
        )
        self.physics = physics
    def forward(self, x_img_permuted, y_kspace_meas, mask_kspace, csmap):
        x_img = rearrange(x_img_permuted, "b h w t c -> b c t h w")
        y = y_kspace_meas
        A_x = self.physics.A(x_img, csmap)
        lambda_ = torch.sigmoid(self.lambda_)
        k_dc = (1 - mask_kspace) * A_x + mask_kspace * (
            lambda_ * A_x + (1 - lambda_) * y
        )
        x_dc_img = self.physics.A_adjoint(k_dc, csmap)
        x_dc_permuted = rearrange(x_dc_img, "b c t h w -> b h w t c")
        return x_dc_permuted

=== train.py ===
import argparse
import json
import os
import subprocess
import deepinv as dinv
import matplotlib.pyplot as plt
import torch
import yaml
from crnn import CRNN, ArtifactRemovalCRNN
from dataloader import SliceDataset
from deepinv.transform import Transform
from einops import rearrange
from radial import DynamicRadialPhysics, RadialDCLayer
from torch.utils.data import DataLoader
from torchvision.transforms import InterpolationMode
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet import LSFPNet, ArtifactRemovalLSFPNet
def _calculate_top_percentile_curve(dynamic_slice: torch.Tensor, percentile: float) -> list[float]:
    """Helper function to calculate the enhancement curve for a single dynamic slice."""
    if dynamic_slice.dim() != 5 or dynamic_slice.shape[0] != 1 or dynamic_slice.shape[1] != 2:
        raise ValueError(f"Expected input shape (1, 2, T, H, W), but got {dynamic_slice.shape}")
    magnitude_video = torch.sqrt(dynamic_slice[:, 0, ...] ** 2 + dynamic_slice[:, 1, ...] ** 2).squeeze(0)
    num_time_frames = magnitude_video.shape[0]
    top_percentile_means = []
    q = percentile / 100.0
    for t in range(num_time_frames):
        frame_t = magnitude_video[t, :, :]
        if frame_t.max() == 0:
            top_percentile_means.append(0)
            continue
        threshold = torch.quantile(frame_t.flatten(), q)
        bright_pixels = frame_t[frame_t > threshold]
        mean_val = torch.mean(bright_pixels) if bright_pixels.numel() > 0 else threshold
        top_percentile_means.append(mean_val.item())
    return top_percentile_means
def plot_enhancement_curve(
    model_output: torch.Tensor,
    percentile: float = 99.0,
    title: str = "Enhancement Curve Comparison",
    output_filename: str = None
):
    """
    Calculates and plots the enhancement curves for a model output and a benchmark
    image on the same graph for direct comparison.
    Args:
        model_output (torch.Tensor): The model's reconstructed dynamic slice.
                                     Shape (1, 2, T, H, W).
        benchmark_image (torch.Tensor): The ground truth or benchmark dynamic slice.
                                        Shape (1, 2, T, H, W).
        percentile (float, optional): The percentile for defining the brightest pixels.
                                      Defaults to 99.0.
        title (str, optional): The title for the plot. Defaults to "Enhancement Curve Comparison".
        output_filename (str, optional): If provided, saves the plot to this file path.
                                         Defaults to None (displays plot).
    """
    if not 0 < percentile < 100:
        raise ValueError("Percentile must be between 0 and 100.")
    model_curve = _calculate_top_percentile_curve(model_output.detach(), percentile)
    num_time_frames = model_output.shape[2]
    time_axis = np.arange(num_time_frames)
    plt.figure(figsize=(12, 7))
    plt.plot(time_axis, model_curve, label='Model Output', marker='o', linestyle='-', color='tab:blue')
    plt.title(title, fontsize=16)
    plt.xlabel("Time Frame", fontsize=12)
    plt.ylabel(f"Mean Signal of Top {100-percentile:.1f}% Pixels", fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    if output_filename:
        output_dir = os.path.dirname(output_filename)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        plt.savefig(output_filename)
    else:
        plt.show()
    plt.close()
def get_cosine_ei_weight(
    current_epoch,
    warmup_epochs,
    schedule_duration,
    target_weight
):
    """
    Calculates the EI loss weight for the current epoch using a cosine schedule.
    This implements a curriculum learning strategy:
    1. For `warmup_epochs`, the weight is 0 (MC loss only).
    2. Over the next `schedule_duration` epochs, the weight smoothly ramps
       up from 0 to `target_weight` following a cosine curve.
    3. After the schedule is complete, the weight stays at `target_weight`.
    Args:
        current_epoch (int): The current training epoch (starting from 1).
        warmup_epochs (int): Number of epochs to train with only MC loss.
        schedule_duration (int): Number of epochs for the ramp-up.
        target_weight (float): The final EI loss weight to reach.
    Returns:
        float: The EI loss weight for the current epoch.
    """
    if current_epoch <= warmup_epochs:
        return 0.0
    schedule_progress_epoch = current_epoch - warmup_epochs
    if schedule_progress_epoch >= schedule_duration:
        return target_weight
    cosine_multiplier = 0.5 * (1 - np.cos(np.pi * schedule_progress_epoch / schedule_duration))
    return target_weight * cosine_multiplier
def plot_reconstruction_sample(x_recon, title, filename, output_dir, grasp_img=None, batch_idx=0, transform=False):
    """
    Plot reconstruction sample showing magnitude images across timeframes.
    Args:
        x_recon: Reconstructed image tensor of shape (B, C, T, H, W)
        title: Title for the plot
        filename: Filename for saving (without extension)
        output_dir: Directory to save the plot
        batch_idx: Which batch element to plot (default: 0)
    """
    os.makedirs(output_dir, exist_ok=True)
    x_recon_mag = torch.sqrt(x_recon[:, 0, ...] ** 2 + x_recon[:, 1, ...] ** 2)
    grasp_img_mag = torch.sqrt(grasp_img[:, 0, ...] ** 2 + grasp_img[:, 1, ...] ** 2)
    n_timeframes = x_recon_mag.shape[1]
    fig, axes = plt.subplots(
        nrows=2,
        ncols=n_timeframes,
        figsize=(n_timeframes * 3, 8),
        squeeze=False,
    )
    if transform:
        axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
    if transform:
        axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        os.makedirs(os.path.join(output_dir, "transforms"), exist_ok=True)
    else:
        axes[0, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("GRASP Benchmark", fontsize=14, labelpad=10)
    for t in range(n_timeframes):
        img = x_recon_mag[batch_idx, t, :, :].cpu().detach().numpy()
        grasp_img = grasp_img_mag[batch_idx, t, :, :].cpu().detach().numpy()
        ax1 = axes[0, t]
        ax1.imshow(np.rot90(img, 2), cmap="gray")
        ax1.set_title(f"t = {t}")
        ax1.set_xticks([])
        ax1.set_yticks([])
        ax2 = axes[1, t]
        ax2.imshow(grasp_img, cmap="gray")
        ax2.set_title(f"t = {t}")
        ax2.set_xticks([])
        ax2.set_yticks([])
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(output_dir, f"{filename}.png"))
    plt.close(fig)
def get_git_commit():
    try:
        commit_hash = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        return commit_hash
    except Exception as e:
        print(f"Error retrieving Git commit: {e}")
        return "unknown"
def remove_module_prefix(state_dict):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('module.', '')  # Remove 'module.' prefix
        new_state_dict[new_key] = v
    return new_state_dict
def save_checkpoint(model, optimizer, epoch,
                    train_curves, val_curves, filename):
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        **train_curves,   # unpack the dicts
        **val_curves,
    }
    torch.save(checkpoint, filename)
    print(f"Checkpoint saved at epoch {epoch} to {filename}")
def load_checkpoint(model, optimizer, filename):
    ckpt = torch.load(filename, map_location="cpu")
    model.load_state_dict(remove_module_prefix(ckpt["model_state_dict"]))
    optimizer.load_state_dict(remove_module_prefix(ckpt["optimizer_state_dict"]))
    train_curves = {
        "train_mc_losses": ckpt.get("train_mc_losses", []),
        "train_ei_losses": ckpt.get("train_ei_losses", []),
        "weighted_train_mc_losses": ckpt.get("weighted_train_mc_losses", []),
        "weighted_train_ei_losses": ckpt.get("weighted_train_ei_losses", []),
    }
    val_curves = {
        "val_mc_losses": ckpt.get("val_mc_losses", []),
        "val_ei_losses": ckpt.get("val_ei_losses", []),
    }
    return model, optimizer, ckpt.get("epoch", 1), train_curves, val_curves
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
start_epoch = 1
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils = (
    config["data"]["timeframes"],
    config["data"]["spokes_per_frame"],
    config["data"]["coils"],
)
N_spokes = int(config["data"]["total_spokes"] / N_time)
os.makedirs(os.path.join(output_dir, 'enhancement_curves'), exist_ok=True)
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = max_subjects * (1 - config["data"]["val_split_ratio"])
    max_val = max_subjects * config["data"]["val_split_ratio"]
    train_patient_ids = splits["train"][:max_train]
    val_patient_ids = splits["val"][:max_val]
else:
    train_patient_ids = splits["train"]
    val_patient_ids = splits["val"]
train_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=train_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
    N_coils=N_coils
)
val_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=val_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
    N_coils=N_coils
)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_loader = DataLoader(
    val_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
physics = DynamicRadialPhysics(
    im_size=(H, W, N_time),
    N_spokes=N_spokes,
    N_samples=N_samples,
    N_time=N_time,
    N_coils=N_coils,
)
datalayer = RadialDCLayer(physics=physics)
if config["model"]["name"] == "CRNN":
    backbone = CRNN(
        num_cascades=config["model"]["cascades"],
        chans=config["model"]["channels"],
        datalayer=datalayer,
    ).to(device)
    model = ArtifactRemovalCRNN(backbone_net=backbone).to(device)
elif config["model"]["name"] == "LSFPNet":
    lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"])
    model = ArtifactRemovalLSFPNet(lsfp_backbone).to(device)
else:
    raise(ValueError("Unsupported model."))
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, train_curves, val_curves = load_checkpoint(model, optimizer, checkpoint_file)
    print("start epoch: ", start_epoch)
else:
    start_epoch = 1
mc_loss_fn = MCLoss()
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        ei_loss_fn = EILoss(subsample | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "monophasic":
        ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "reverse":
        ei_loss_fn = EILoss(time_reverse | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "all":
        ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise | time_reverse) | (diffeo | rotate))
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
print(
    "--- Generating and saving a Zero-Filled (ZF) reconstruction sample before training ---"
)
with torch.no_grad():
    val_kspace_sample, csmap, grasp_img = next(iter(val_loader))
    val_kspace_sample = val_kspace_sample.to(device)
    x_zf = physics.A_adjoint(val_kspace_sample, csmap)
    plot_reconstruction_sample(
        x_zf,
        "Zero-Filled (ZF) Reconstruction (Before Training)",
        "zf_reconstruction_baseline",
        output_dir,
        grasp_img
    )
print("--- ZF baseline image saved to output directory. Starting training. ---")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
iteration_count = 0
if args.from_checkpoint == False:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    with torch.no_grad():
        for measured_kspace, csmap, grasp_img in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            x_recon = model(
                measured_kspace.to(device), physics, csmap
            )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        for measured_kspace, csmap, grasp_img in tqdm(val_loader, desc="Step 0 Validation Evaluation"):
            x_recon = model(
                measured_kspace.to(device), physics, csmap
            )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                initial_val_ei_loss += ei_loss.item()
        step0_val_mc_loss = initial_val_mc_loss / len(val_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_loader)
        val_ei_losses.append(step0_val_ei_loss)
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        with torch.autograd.set_detect_anomaly(False):
            train_loader_tqdm = tqdm(
                train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
            )
            for measured_kspace, csmap, grasp_img in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
                iteration_count += 1
                optimizer.zero_grad()
                x_recon = model(
                    measured_kspace.to(device), physics, csmap
                )  # model output shape: (B, C, T, H, W)
                mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
                running_mc_loss += mc_loss.item()
                if use_ei_loss:
                    ei_loss, t_img = ei_loss_fn(
                        x_recon, physics, model, csmap
                    )
                    ei_loss_weight = get_cosine_ei_weight(
                        current_epoch=epoch,
                        warmup_epochs=warmup,
                        schedule_duration=duration,
                        target_weight=target_weight
                    )
                    running_ei_loss += ei_loss.item()
                    total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight
                    train_loader_tqdm.set_postfix(
                        mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                    )
                else:
                    total_loss = mc_loss
                    train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
                if torch.isnan(total_loss):
                    print(
                        "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                    )
                    raise RuntimeError("total_loss is NaN")
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                if epoch % save_interval == 0:
                    plot_reconstruction_sample(
                        x_recon,
                        f"Training Sample - Epoch {epoch}",
                        f"train_sample_epoch_{epoch}",
                        output_dir,
                        grasp_img
                    )
                    plot_enhancement_curve(
                        x_recon,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'train_sample_enhancement_curve_epoch_{epoch}.png'))
                    plot_enhancement_curve(
                        grasp_img,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'grasp_sample_enhancement_curve_epoch_{epoch}.png'))
                    if use_ei_loss:
                        plot_reconstruction_sample(
                            t_img,
                            f"Transformed Train Sample - Epoch {epoch}",
                            f"transforms/transform_train_sample_epoch_{epoch}",
                            output_dir,
                            x_recon,
                            transform=True
                        )
            epoch_train_mc_loss = running_mc_loss / len(train_loader)
            train_mc_losses.append(epoch_train_mc_loss)
            weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
            if use_ei_loss:
                epoch_train_ei_loss = running_ei_loss / len(train_loader)
                train_ei_losses.append(epoch_train_ei_loss)
                weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
            else:
                train_ei_losses.append(0.0)
                weighted_train_ei_losses.append(0.0)
            model.eval()
            val_running_mc_loss = 0.0
            val_running_ei_loss = 0.0
            val_loader_tqdm = tqdm(
                val_loader,
                desc=f"Epoch {epoch}/{epochs}  Validation",
                unit="batch",
                leave=False,
            )
            with torch.no_grad():
                for val_kspace_batch, val_csmap, val_grasp_img in val_loader_tqdm:
                    val_x_recon = model(val_kspace_batch.to(device), physics, val_csmap)
                    val_y_meas = val_kspace_batch
                    val_mc_loss = mc_loss_fn(val_y_meas.to(device), val_x_recon, physics, val_csmap)
                    val_running_mc_loss += val_mc_loss.item()
                    if use_ei_loss:
                        val_ei_loss, val_t_img = ei_loss_fn(
                            val_x_recon, physics, model, val_csmap
                        )
                        val_running_ei_loss += val_ei_loss.item()
                        val_loader_tqdm.set_postfix(
                            val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                        )
                    else:
                        val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                if epoch % save_interval == 0:
                    plot_reconstruction_sample(
                        val_x_recon,
                        f"Validation Sample - Epoch {epoch}",
                        f"val_sample_epoch_{epoch}",
                        output_dir,
                        val_grasp_img
                    )
                    plot_enhancement_curve(
                        val_x_recon,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
                    plot_enhancement_curve(
                        val_grasp_img,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
                    if use_ei_loss:
                        plot_reconstruction_sample(
                            val_t_img,
                            f"Transformed Validation Sample - Epoch {epoch}",
                            f"transforms/transform_val_sample_epoch_{epoch}",
                            output_dir,
                            val_x_recon,
                            transform=True
                        )
                    train_curves = dict(
                        train_mc_losses=train_mc_losses,
                        train_ei_losses=train_ei_losses,
                        weighted_train_mc_losses=weighted_train_mc_losses,
                        weighted_train_ei_losses=weighted_train_ei_losses,
                    )
                    val_curves = dict(
                        val_mc_losses=val_mc_losses,
                        val_ei_losses=val_ei_losses,
                    )
                    model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
                    save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, model_save_path)
                    print(f'Model saved to {model_save_path}')
            epoch_val_mc_loss = val_running_mc_loss / len(val_loader)
            val_mc_losses.append(epoch_val_mc_loss)
            if use_ei_loss:
                epoch_val_ei_loss = val_running_ei_loss / len(val_loader)
                val_ei_losses.append(epoch_val_ei_loss)
            else:
                val_ei_losses.append(0.0)
            if epoch % save_interval == 0:
                plt.figure()
                plt.plot(train_mc_losses, label="Training MC Loss")
                plt.plot(val_mc_losses, label="Validation MC Loss")
                plt.xlabel("Epoch")
                plt.ylabel("MC Loss")
                plt.title("Measurement Consistency Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "mc_losses.png"))
                plt.close()
                if use_ei_loss:
                    plt.figure()
                    plt.plot(train_ei_losses, label="Training EI Loss")
                    plt.plot(val_ei_losses, label="Validation EI Loss")
                    plt.xlabel("Epoch")
                    plt.ylabel("EI Loss")
                    plt.title("Equivariant Imaging Loss")
                    plt.legend()
                    plt.grid(True)
                    plt.savefig(os.path.join(output_dir, "ei_losses.png"))
                    plt.close()
                    plt.figure()
                    plt.plot(weighted_train_mc_losses, label="MC Loss")
                    plt.plot(weighted_train_ei_losses, label="EI Loss")
                    plt.xlabel("Epoch")
                    plt.ylabel("Loss")
                    plt.title("Weighted Training Losses")
                    plt.legend()
                    plt.grid(True)
                    plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
                    plt.close()
            print(
                f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
            )
            if use_ei_loss:
                print(
                    f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
                )
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, model_save_path)
print(f'Model saved to {model_save_path}')

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Splits each volume into Z separate examples (one per slice).
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx=41,
        N_time = 8,
        N_coils=16
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset (e.g. "kspace").
            file_pattern (str): Glob pattern to match your HDF5 files (default "*.h5").
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.N_time = N_time
        self.N_coils = N_coils
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
    def load_dynamic_img(self, patient_id):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{self.slice_idx:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; "
                                f"expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data) 
    def load_csmaps(self, patient_id):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{self.slice_idx:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.file_list)
    def __getitem__(self, idx):
        """
        Returns a single slice of k-space as a torch.Tensor.
        The output shape will be the standard (C=2, T, S, I) where C is [real, imag].
        """
        file_path = self.file_list[idx]
        patient_id = file_path.split('/')[-1].strip('.h5')
        grasp_img = self.load_dynamic_img(patient_id)
        csmap = self.load_csmaps(patient_id)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[self.slice_idx]
        if self.N_coils == 1:
            kspace_slice = kspace_slice[:, 0, :, :]  # Shape: (T, S, I)
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        return kspace_final, csmap, grasp_img
if __name__ == "__main__":
    root_dir = "/ess/scratch/scratch1/rachelgordon/dce-12tf/binned_kspace"
    dataset_key = "ktspace"  # change if your HDF5 group/dataset is named differently
    def to_complex(x_np: "np.ndarray") -> "np.ndarray":
        """
        If x_np.shape = (C, H, W, 2) or similar where the last dim is [real, imag],
        convert to complex64 with shape (C, H, W).
        Adjust slicing logic if your real/imag channels are elsewhere.
        """
        real = x_np[..., 0].astype("float32")
        imag = x_np[..., 1].astype("float32")
        return (real + 1j * imag).astype("complex64")
    dataset = SliceDataset(
        root_dir=root_dir, dataset_key=dataset_key, file_pattern="*.h5"
    )
    loader = DataLoader(
        dataset,
        batch_size=1,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    for batch_idx, kspace_batch in enumerate(loader):
        print(
            f"Batch {batch_idx}: k-space batch shape = {kspace_batch.shape}, dtype = {kspace_batch.dtype}"
        )
        break

