=== train_aug.py ===
import argparse
import json
import os
import subprocess
import matplotlib.pyplot as plt
import torch
import yaml
from crnn import CRNN, ArtifactRemovalCRNN
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset, SliceDatasetAug
from deepinv.transform import Transform
from einops import rearrange
from radial import RadialDCLayer, to_torch_complex, MCNUFFT_CRNN
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex
from eval import eval_grasp, eval_sample
import csv
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
eval_dir = os.path.join(output_dir, "eval_results")
os.makedirs(eval_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                   'lambda_S': config['model']['lambda_S'], 
                   'lambda_spatial_L': config['model']['lambda_spatial_L'],
                   'lambda_spatial_S': config['model']['lambda_spatial_S'],
                   'gamma': config['model']['gamma'],
                   'lambda_step': config['model']['lambda_step']}
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
start_epoch = 1
model_type = config["model"]["name"]
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils, N_time_eval = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"],
    config["data"]["eval_timeframes"]
)
N_spokes = int(config["data"]["total_spokes"] / N_time)
os.makedirs(os.path.join(output_dir, 'enhancement_curves'), exist_ok=True)
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
else:
    train_patient_ids = splits["train"]
val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]
if config['data']['spf_aug']:
    train_dataset = SliceDatasetAug(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        N_coils=N_coils
    )
else:
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        N_time=N_time,
        N_coils=N_coils
    )
val_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=val_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
    N_coils=N_coils
)
val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_loader = DataLoader(
    val_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)
eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], lambdas=initial_lambdas, channels=config['model']['channels'])
model = ArtifactRemovalLSFPNet(lsfp_backbone, output_dir).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
else:
    start_epoch = 1
if config['model']['losses']['mc_loss']['metric'] == "MSE":
    mc_loss_fn = MCLoss(model_type=model_type)
elif config['model']['losses']['mc_loss']['metric'] == "MAE":
    mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
else:
    raise(ValueError, "Unsupported MC Loss Metric.")
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(subsample, model_type=model_type)
        else:
            ei_loss_fn = EILoss(subsample | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "monophasic":
        ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "reverse":
        ei_loss_fn = EILoss(time_reverse | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "all":
        ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise | time_reverse) | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
            ei_loss_fn = EILoss(rotate, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
            ei_loss_fn = EILoss(diffeo, model_type=model_type)
        else:
            ei_loss_fn = EILoss(rotate | diffeo, model_type=model_type)
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    train_adj_losses = train_curves["train_adj_losses"]
    val_adj_losses = val_curves["val_adj_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
    weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
    eval_ssims = eval_curves["ssim"]
    eval_psnrs = eval_curves["psnr"]
    eval_mses = eval_curves["mse"]
    eval_dcs = eval_curves["dc"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    train_adj_losses = []
    val_adj_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
    weighted_train_adj_losses = []
    eval_ssims = []
    eval_psnrs = []
    eval_mses = []
    eval_dcs = []
grasp_ssims = []
grasp_psnrs = []
grasp_mses = []
grasp_dcs = []
lambda_Ls = []
lambda_Ss = []
lambda_spatial_Ls = []
lambda_spatial_Ss = []
gammas = []
lambda_steps = []
iteration_count = 0
if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    initial_train_adj_loss = 0.0
    initial_val_adj_loss = 0.0
    with torch.no_grad():
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            if model_type == "LSFPNet":
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                csmap = csmap.to(device).to(measured_kspace.dtype)
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, epoch="train0", norm=config['model']['norm']
                )
                initial_train_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
        train_adj_losses.append(step0_train_adj_loss)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        for measured_kspace, csmap, ground_truth, grasp_img, *_ in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
            if model_type == "LSFPNet":
                measured_kspace = measured_kspace.to(device)
                csmap = csmap.to(device)
                measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
                csmap = csmap.squeeze(0).to(device)   # Remove batch dim
                x_recon, adj_loss, *_ = model(
                    measured_kspace.to(device), eval_physics, csmap, epoch="val0", norm=config['model']['norm']
                )
                initial_val_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), eval_physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, eval_physics, model, csmap
                )
                initial_val_ei_loss += ei_loss.item()
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device)
            grasp_ssims.append(ssim_grasp)
            grasp_psnrs.append(psnr_grasp)
            grasp_mses.append(mse_grasp)
            grasp_dcs.append(dc_grasp)
        step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
        val_ei_losses.append(step0_val_ei_loss)
        step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
        val_adj_losses.append(step0_val_adj_loss)
    print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
    print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        running_adj_loss = 0.0
        epoch_eval_ssims = []
        epoch_eval_psnrs = []
        epoch_eval_mses = []
        epoch_eval_dcs = []
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
            ktraj = ktraj.to(device)
            dcomp = dcomp.to(device)
            nufft_ob = nufft_ob.to(device)
            adjnufft_ob = adjnufft_ob.to(device)
            physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            iteration_count += 1
            optimizer.zero_grad()
            if model_type == "LSFPNet":
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                csmap = csmap.to(device).to(measured_kspace.dtype)
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, epoch=f"train{epoch}", norm=config['model']['norm']
                )
                running_adj_loss += adj_loss.item()
            else:
                x_recon = model(
                    measured_kspace.to(device), physics, csmap
                )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            running_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap
                )
                ei_loss_weight = get_cosine_ei_weight(
                    current_epoch=epoch,
                    warmup_epochs=warmup,
                    schedule_duration=duration,
                    target_weight=target_weight
                )
                running_ei_loss += ei_loss.item()
                total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(
                    mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                )
            else:
                total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
            if torch.isnan(total_loss):
                print(
                    "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                )
                raise RuntimeError("total_loss is NaN")
            total_loss.backward()
            if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                log_gradient_stats(
                    model=model,
                    epoch=epoch,
                    iteration=iteration_count,
                    output_dir=output_dir,
                    log_filename="gradient_stats.csv"
                )
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_train_adj_loss = running_adj_loss / len(train_loader)
            train_adj_losses.append(epoch_train_adj_loss)
            weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
        else:
            train_adj_losses.append(0.0)
            weighted_train_adj_losses.append(0.0)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_running_adj_loss = 0.0
        val_loader_tqdm = tqdm(
            val_dro_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask in tqdm(val_dro_loader):
                if model_type == "LSFPNet":
                    val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                    val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                    val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                    val_grasp_img = torch.flip(val_grasp_img, dims=[-3])
                    val_grasp_img = torch.rot90(val_grasp_img, k=3, dims=[-3,-1])
                    val_grasp_img_tensor = val_grasp_img.to(device)
                    val_x_recon, val_adj_loss, *_ = model(
                        val_kspace_batch.to(device), eval_physics, val_csmap, epoch=f"val{epoch}", norm=config['model']['norm']
                    )
                    val_running_adj_loss += val_adj_loss.item()
                else:
                    val_x_recon = model(val_kspace_batch.to(device), eval_physics, val_csmap)
                val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                val_running_mc_loss += val_mc_loss.item()
                if use_ei_loss:
                    val_ei_loss, val_t_img = ei_loss_fn(
                        val_x_recon, eval_physics, model, val_csmap
                    )
                    val_running_ei_loss += val_ei_loss.item()
                    val_loader_tqdm.set_postfix(
                        val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                    )
                else:
                    val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                ssim, psnr, mse, dc = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, eval_dir, f'epoch{epoch}', device)
                epoch_eval_ssims.append(ssim)
                epoch_eval_psnrs.append(psnr)
                epoch_eval_mses.append(mse)
                epoch_eval_dcs.append(dc)
        epoch_eval_ssim = np.mean(epoch_eval_ssims)
        epoch_eval_psnr = np.mean(epoch_eval_psnrs)
        epoch_eval_mse = np.mean(epoch_eval_mses)
        epoch_eval_dc = np.mean(epoch_eval_dcs)
        eval_ssims.append(epoch_eval_ssim)
        eval_psnrs.append(epoch_eval_psnr)
        eval_mses.append(epoch_eval_mse)
        eval_dcs.append(epoch_eval_dc)    
        if epoch % save_interval == 0:
            val_x_recon_rot = torch.rot90(val_x_recon, k=2, dims=[-3,-2])
            plot_reconstruction_sample(
                val_x_recon_rot,
                f"Validation Sample - Epoch {epoch}",
                f"val_sample_epoch_{epoch}",
                output_dir,
                val_grasp_img
            )
            val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                val_x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
            plot_enhancement_curve(
                val_grasp_img,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch}",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
        epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            val_adj_losses.append(epoch_val_adj_loss)
        else:
            val_adj_losses.append(0.0)
        if epoch % save_interval == 0:
            train_curves = dict(
                train_mc_losses=train_mc_losses,
                train_ei_losses=train_ei_losses,
                weighted_train_mc_losses=weighted_train_mc_losses,
                weighted_train_ei_losses=weighted_train_ei_losses,
            )
            val_curves = dict(
                val_mc_losses=val_mc_losses,
                val_ei_losses=val_ei_losses,
            )
            eval_curves = dict(
                eval_ssims=eval_ssims,
                eval_psnrs=eval_psnrs,
                eval_mses=eval_mses,
                eval_dcs=eval_dcs
            )
            model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
            save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, model_save_path)
            print(f'Model saved to {model_save_path}')
            plt.figure()
            plt.plot(train_mc_losses, label="Training MC Loss")
            plt.plot(val_mc_losses, label="Validation MC Loss")
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(train_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Training Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "train_mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(val_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Validation Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "val_mc_losses.png"))
            plt.close()
            if use_ei_loss:
                plt.figure()
                plt.plot(train_ei_losses, label="Training EI Loss")
                plt.plot(val_ei_losses, label="Validation EI Loss")
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(train_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Training Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "train_ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(val_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Validation Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "val_ei_losses.png"))
                plt.close()
            if model_type == "LSFPNet":
                plt.figure()
                plt.plot(train_adj_losses, label="Training Adjoint Loss")
                plt.plot(val_adj_losses, label="Validation Adjoint Loss")
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(train_adj_losses)
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("Training CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "train_adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(val_adj_losses)
                plt.xlabel("Epoch")
                plt.ylabel("Adjoint Loss")
                plt.title("Validation CNN Adjoint Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "val_adj_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_Ls)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_L Value")
                plt.title("Lambda_L During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_L.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_Ss)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_S Value")
                plt.title("Lambda_S During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_S.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_spatial_Ls)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_spatial_L Value")
                plt.title("Lambda_spatial_L During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_spatial_L.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_spatial_Ss)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_spatial_S Value")
                plt.title("Lambda_spatial_S During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_spatial_S.png"))
                plt.close()
                plt.figure()
                plt.plot(gammas)
                plt.xlabel("Epoch")
                plt.ylabel("Gamma Value")
                plt.title("Step Size During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "gamma.png"))
                plt.close()
                plt.figure()
                plt.plot(lambda_steps)
                plt.xlabel("Epoch")
                plt.ylabel("Lambda_step Value")
                plt.title("Lambda_step During Training")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "lambda_step.png"))
                plt.close()
            plt.figure()
            plt.plot(weighted_train_mc_losses, label="MC Loss")
            plt.plot(weighted_train_ei_losses, label="EI Loss")
            plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Weighted Training Losses")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_ssims)
            plt.xlabel("Epoch")
            plt.ylabel("SSIM")
            plt.title("Evaluation SSIM")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_ssims.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_psnrs)
            plt.xlabel("Epoch")
            plt.ylabel("PSNR")
            plt.title("Evaluation PSNR")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_psnrs.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_mses)
            plt.xlabel("Epoch")
            plt.ylabel("MSE")
            plt.title("Evaluation MSE")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_mses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dcs)
            plt.xlabel("Epoch")
            plt.ylabel("DC")
            plt.title("Evaluation DC")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dcs.png"))
            plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
        if model_type == "LSFPNet":
            print(
                f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
            )
        print(f"--- Evaluation Metrics: Epoch {epoch} ---")
        print(f"Recon SSIM: {epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}")
        print(f"Recon PSNR: {epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}")
        print(f"Recon MSE: {epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}")
        print(f"Recon DC: {epoch_eval_dc:.4f} ± {np.std(epoch_eval_dcs):.4f}")
        print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}")
        print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}")
        print(f"GRASP MSE: {np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}")
        print(f"GRASP DC: {np.mean(grasp_dcs):.6f} ± {np.std(grasp_dcs):.4f}")
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    train_adj_losses=train_adj_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
    weighted_train_adj_losses=weighted_train_adj_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
    val_adj_losses=val_adj_losses,
)
eval_curves = dict(
    eval_ssims=eval_ssims,
    eval_psnrs=eval_psnrs,
    eval_mses=eval_mses,
    eval_dcs=eval_dcs,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, model_save_path)
print(f'Model saved to {model_save_path}')
metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
with open(metrics_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'DC'])
    writer.writerow(['DL', f'{epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}', f'{epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}', f'{epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}', f'{epoch_eval_dc:.4f} ± {np.std(epoch_eval_dcs):.4f}'])
    writer.writerow(['GRASP', f'{np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}', f'{np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}', f'{np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}', f'{np.mean(grasp_dcs):.4f} ± {np.std(grasp_dcs):.4f}'])
MAIN_EVALUATION_PLAN = [
    {
        "spokes_per_frame": 16,
        "num_frames": 20, # 16 * 20 = 320 total spokes
        "slice": slice(1, 21), # Wide window
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 20,
        "num_frames": 16, # 20 * 16 = 320 total spokes
        "slice": slice(3, 19), # Medium-wide window
        "description": "Good temporal resolution"
    },
    {
        "spokes_per_frame": 32,
        "num_frames": 10, # 32 * 10 = 320 total spokes
        "slice": slice(5, 15), # Narrow window centered on enhancement
        "description": "Standard temporal resolution"
    },
    {
        "spokes_per_frame": 40,
        "num_frames": 8,  # 40 * 8 = 320 total spokes
        "slice": slice(5, 13), # Very narrow window on peak
        "description": "Low temporal resolution"
    }
]
STRESS_TEST_PLAN = [
    {
        "spokes_per_frame": 8,
        "num_frames": 22, # 8 * 22 = 176 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 8 spokes"
    },
    {
        "spokes_per_frame": 4,
        "num_frames": 22, # 4 * 22 = 88 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 4 spokes"
    },
]
eval_spf_dataset = SimulatedSPFDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    )
eval_spf_loader = DataLoader(
    eval_spf_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
with torch.no_grad():
    spf_recon_ssim = {}
    spf_recon_psnr = {}
    spf_recon_mse = {}
    spf_recon_dc = {}
    spf_grasp_ssim = {}
    spf_grasp_psnr = {}
    spf_grasp_mse = {}
    spf_grasp_dc = {}
    print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
    for eval_config in STRESS_TEST_PLAN:
        stress_test_ssims = []
        stress_test_psnrs = []
        stress_test_mses = []
        stress_test_dcs = []
        stress_test_grasp_ssims = []
        stress_test_grasp_psnrs = []
        stress_test_grasp_mses = []
        stress_test_grasp_dcs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_img = grasp_img.to(device)
            grasp_img_plot = torch.flip(grasp_img, dims=[-3])
            grasp_img_plot = torch.rot90(grasp_img_plot, k=3, dims=[-3,-1])
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(640, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            x_recon, *_ = model(
                kspace.to(device), physics, csmap, epoch, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, dc = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img_plot, eval_dir, f"{spokes}spf", device)
            stress_test_ssims.append(ssim)
            stress_test_psnrs.append(psnr)
            stress_test_mses.append(mse)
            stress_test_dcs.append(dc)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device)
            stress_test_grasp_ssims.append(ssim_grasp)
            stress_test_grasp_psnrs.append(psnr_grasp)
            stress_test_grasp_mses.append(mse_grasp)
            stress_test_grasp_dcs.append(dc_grasp)
            spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
            spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
            spf_recon_mse[spokes] = np.mean(stress_test_mses)
            spf_recon_dc[spokes] = np.mean(stress_test_dcs)
            spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
            spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
            spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
            spf_grasp_dc[spokes] = np.mean(stress_test_grasp_dcs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'DC'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(stress_test_ssims):.4f} ± {np.std(stress_test_ssims):.4f}', 
            f'{np.mean(stress_test_psnrs):.4f} ± {np.std(stress_test_psnrs):.4f}', 
            f'{np.mean(stress_test_mses):.4f} ± {np.std(stress_test_mses):.4f}', 
            f'{np.mean(stress_test_dcs):.4f} ± {np.std(stress_test_dcs):.4f}'])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(stress_test_grasp_ssims):.4f} ± {np.std(stress_test_grasp_ssims):.4f}', 
            f'{np.mean(stress_test_grasp_psnrs):.4f} ± {np.std(stress_test_grasp_psnrs):.4f}', 
            f'{np.mean(stress_test_grasp_mses):.4f} ± {np.std(stress_test_grasp_mses):.4f}', 
            f'{np.mean(stress_test_grasp_dcs):.4f} ± {np.std(stress_test_grasp_dcs):.4f}'])
    print("--- Running Main Evaluation (Budget: 320 spokes) ---")
    for eval_config in MAIN_EVALUATION_PLAN:
        spf_eval_ssims = []
        spf_eval_psnrs = []
        spf_eval_mses = []
        spf_eval_dcs = []
        spf_grasp_ssims = []
        spf_grasp_psnrs = []
        spf_grasp_mses = []
        spf_grasp_dcs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_img = grasp_img.to(device)
            grasp_img_plot = torch.flip(grasp_img, dims=[-3])
            grasp_img_plot = torch.rot90(grasp_img_plot, k=3, dims=[-3,-1])
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(640, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            x_recon, _ = model(
                kspace.to(device), physics, csmap, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, dc = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img_plot, eval_dir, f'{spokes}spf', device)
            spf_eval_ssims.append(ssim)
            spf_eval_psnrs.append(psnr)
            spf_eval_mses.append(mse)
            spf_eval_dcs.append(dc)
            ssim_grasp, psnr_grasp, mse_grasp, dc_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device)
            spf_grasp_ssims.append(ssim_grasp)
            spf_grasp_psnrs.append(psnr_grasp)
            spf_grasp_mses.append(mse_grasp)
            spf_grasp_dcs.append(dc_grasp)
        spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
        spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
        spf_recon_mse[spokes] = np.mean(spf_eval_mses)
        spf_recon_dc[spokes] = np.mean(spf_eval_dcs)
        spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
        spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
        spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
        spf_grasp_dc[spokes] = np.mean(spf_grasp_dcs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'DC'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(spf_eval_ssims):.4f} ± {np.std(spf_eval_ssims):.4f}', 
            f'{np.mean(spf_eval_psnrs):.4f} ± {np.std(spf_eval_psnrs):.4f}', 
            f'{np.mean(spf_eval_mses):.4f} ± {np.std(spf_eval_mses):.4f}', 
            f'{np.mean(spf_eval_dcs):.4f} ± {np.std(spf_eval_dcs):.4f}'])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(spf_grasp_ssims):.4f} ± {np.std(spf_grasp_ssims):.4f}', 
            f'{np.mean(spf_grasp_psnrs):.4f} ± {np.std(spf_grasp_psnrs):.4f}', 
            f'{np.mean(spf_grasp_mses):.4f} ± {np.std(spf_grasp_mses):.4f}', 
            f'{np.mean(spf_grasp_dcs):.4f} ± {np.std(spf_grasp_dcs):.4f}'])
plt.figure()
plt.plot(list(spf_recon_ssim.keys()), list(spf_recon_ssim.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_ssim.keys()), list(spf_grasp_ssim.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("SSIM")
plt.title("Evaluation SSIM vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_ssim.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_psnr.keys()), list(spf_recon_psnr.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_psnr.keys()), list(spf_grasp_psnr.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("PSNR")
plt.title("Evaluation PSNR vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_psnr.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_mse.keys()), list(spf_recon_mse.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_mse.keys()), list(spf_grasp_mse.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("MSE")
plt.title("Evaluation Image MSE vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_mse.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_dc.keys()), list(spf_recon_dc.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_dc.keys()), list(spf_grasp_dc.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("k-space MSE")
plt.title("Data Consistency Evaluation (MSE) vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_dc.png"))
plt.close()

=== lsfpnet.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
import matplotlib.pyplot as plt
import numpy as np
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class BasicBlock(nn.Module):
    def __init__(self, lambdas, channels=32):
        super(BasicBlock, self).__init__()
        self.channels = channels
        self.lambda_L = nn.Parameter(torch.tensor([lambdas['lambda_L']]))
        self.lambda_S = nn.Parameter(torch.tensor([lambdas['lambda_S']]))
        self.lambda_spatial_L = nn.Parameter(torch.tensor([lambdas['lambda_spatial_L']]))
        self.lambda_spatial_S = nn.Parameter(torch.tensor([lambdas['lambda_spatial_S']]))
        self.gamma = nn.Parameter(torch.tensor([lambdas['gamma']]))
        self.lambda_step = nn.Parameter(torch.tensor([lambdas['lambda_step']]))
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmaps):
        c = self.lambda_step / self.gamma
        nx, ny, nt = M0.size()
        temp_data = torch.reshape(L + S, [nx, ny, nt])
        temp_data = param_E(inv=False, data=temp_data, smaps=csmaps).to(param_d.device)
        gradient = param_E(inv=True, data=temp_data - param_d, smaps=csmaps)
        gradient = torch.reshape(gradient, [nx * ny, nt]).to(param_d.device)
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(torch.squeeze(pb_L), [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        svd_input_complex = c * y_L + pt_L
        svd_input_mag = svd_input_complex.abs() + 1e-8
        original_phase = svd_input_complex / svd_input_mag
        epsilon = 1e-7  # A small constant. Tune if necessary.
        noise_std = 1e-5 # A small standard deviation for the noise
        noise = torch.randn_like(svd_input_mag) * noise_std
        stable_svd_input = svd_input_mag + noise #epsilon
        Ut, St, Vt = torch.linalg.svd(stable_svd_input, full_matrices=False)
        St_shrunk = Project_inf(St, self.lambda_L, to_complex=False)
        pt_L_mag = Ut @ torch.diag_embed(St_shrunk) @ Vt
        pt_L = pt_L_mag * original_phase
        temp_y_L_input = torch.cat((torch.real(y_L), torch.imag(y_L)), 0).to(torch.float32)
        temp_y_L_input = torch.reshape(temp_y_L_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_L = F.conv3d(temp_y_L_input, self.conv1_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L = F.conv3d(temp_y_L, self.conv2_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L_output = F.conv3d(temp_y_L, self.conv3_forward_l, padding=1)
        temp_y_L = temp_y_L_output + p_L
        temp_y_L = temp_y_L[0, :, :, :, :] + 1j * temp_y_L[1, :, :, :, :]
        p_L = Project_inf(c * temp_y_L, self.lambda_spatial_L)
        p_L = torch.cat((torch.real(p_L), torch.imag(p_L)), 0).to(torch.float32)
        p_L = torch.reshape(p_L, [2, self.channels, nx, ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L_output = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(pb_L_output, [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        adjloss_L = temp_y_L_output * p_L - pb_L_output * temp_y_L_input
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, self.lambda_S)
        temp_y_S_input = torch.cat((torch.real(y_S), torch.imag(y_S)), 0).to(torch.float32)
        temp_y_S_input = torch.reshape(temp_y_S_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_S = F.conv3d(temp_y_S_input, self.conv1_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S = F.conv3d(temp_y_S, self.conv2_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S_output = F.conv3d(temp_y_S, self.conv3_forward_s, padding=1)
        temp_y_Sp = temp_y_S_output + p_S
        temp_y_Sp = temp_y_Sp[0, :, :, :, :] + 1j * temp_y_Sp[1, :, :, :, :]
        p_S = Project_inf(c * temp_y_Sp, self.lambda_spatial_S)
        p_S = torch.cat((torch.real(p_S), torch.imag(p_S)), 0).to(torch.float32)
        p_S = torch.reshape(p_S, [2, self.channels, nx, ny, nt])
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S_output = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S_output, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        adjloss_S = temp_y_S_output * p_S - pb_S_output * temp_y_S_input
        return [L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S, self.lambda_L, self.lambda_S, self.lambda_spatial_L, self.lambda_spatial_S, self.gamma, self.lambda_step]
class LSFPNet(nn.Module):
    def __init__(self, LayerNo, lambdas, channels=32):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        self.channels = channels
        for ii in range(LayerNo):
            onelayer.append(BasicBlock(lambdas, channels=self.channels))
        self.fcs = nn.ModuleList(onelayer)
    def plot_block_output(self, M0, L, S, iter, epoch, output_dir):
        time_frame_index = 3
        nx, ny, nt = M0.size()
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        output_image = L + S
        fig, axes = plt.subplots(1, 4, figsize=(24, 6))
        fig.suptitle(f"Basic Block Output at Time Frame {time_frame_index} and Iteration {iter}", fontsize=20)
        axes[0].imshow(np.abs(M0[..., time_frame_index].cpu().numpy()), cmap='gray')
        axes[0].set_title("Input Image")
        axes[0].axis("off")
        axes[1].imshow(np.abs(L[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[1].set_title("Background Component (L)")
        axes[1].axis("off")
        axes[2].imshow(np.abs(S[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[2].set_title("Dynamic Component (S)")
        axes[2].axis("off")
        axes[3].imshow(np.abs(output_image[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[3].set_title("Combined Image (L + S)")
        axes[3].axis("off")
        filename = os.path.join(output_dir, f'basic_block_{iter}_output_{epoch}.png')
        plt.savefig(filename)
        plt.close()
    def forward(self, M0, param_E, param_d, csmap, epoch, output_dir):
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmap)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
            self.plot_block_output(M0, L, S, iter=ii, epoch=epoch, output_dir=output_dir)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step]
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, output_dir, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
        self.output_dir = output_dir
    @staticmethod
    def _normalise_both(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max()                       # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_indep(x: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = torch.quantile(x.abs(), 0.99) + 1e-6
        if scale < 1e-6: # Handle case where input is all zeros
             scale = 1.0
        return x / scale, scale
    def forward(self, y, E, csmap, epoch, norm="both", **kwargs):
        x_init = E(inv=True, data=y, smaps=csmap)
        if norm =="both":
            x_init_norm, y_norm, scale = self._normalise_both(x_init, y)
        elif norm == "independent":
            x_init_norm, scale = self._normalise_indep(x_init)
            y_norm, scale_y = self._normalise_indep(y)
        elif norm == "none":
            x_init_norm = x_init
            y_norm = y
            scale = 1.0
        L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir)
        loss_constraint_L = torch.square(torch.mean(loss_layers_adj_L[0])) / self.backbone_net.LayerNo
        loss_constraint_S = torch.square(torch.mean(loss_layers_adj_S[0])) / self.backbone_net.LayerNo
        for k in range(self.backbone_net.LayerNo - 1):
            loss_constraint_S += torch.square(torch.mean(loss_layers_adj_S[k + 1])) / self.backbone_net.LayerNo
            loss_constraint_L += torch.square(torch.mean(loss_layers_adj_L[k + 1])) / self.backbone_net.LayerNo
        recon = (L + S) * scale                 # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat, loss_constraint_L + loss_constraint_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Splits each volume into Z separate examples (one per slice).
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx=41,
        N_time = 8,
        N_coils=16
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset (e.g. "kspace").
            file_pattern (str): Glob pattern to match your HDF5 files (default "*.h5").
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.N_time = N_time
        self.N_coils = N_coils
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
    def load_dynamic_img(self, patient_id):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{self.slice_idx:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; "
                                f"expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data) 
    def load_csmaps(self, patient_id):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{self.slice_idx:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.file_list)
    def __getitem__(self, idx):
        """
        Returns a single slice of k-space as a torch.Tensor.
        The output shape will be the standard (C=2, T, S, I) where C is [real, imag].
        """
        file_path = self.file_list[idx]
        patient_id = file_path.split('/')[-1].strip('.h5')
        grasp_img = self.load_dynamic_img(patient_id)
        csmap = self.load_csmaps(patient_id)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[self.slice_idx]
        if self.N_coils == 1:
            kspace_slice = kspace_slice[:, 0, :, :]  # Shape: (T, S, I)
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        N_samples = kspace_final.shape[-1]
        spokes_per_frame = kspace_final.shape[-2]
        return kspace_final, csmap, grasp_img, N_samples, spokes_per_frame, self.N_time
class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.sample_paths = sorted(glob.glob(os.path.join(root_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {root_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {root_dir} for this dataset.")
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        kspace_complex = np.load(os.path.join(sample_dir, 'simulated_kspace.npy'))
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_recon = np.load(os.path.join(sample_dir, 'grasp_recon.npy'))
        ground_truth_complex = dro['ground_truth_images']
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        if self.model_type == "CRNN":
            kspace_real_imag = np.stack([kspace_complex.real, kspace_complex.imag]) # (2, C, Samples, T)
            kspace_torch = torch.from_numpy(kspace_real_imag)
            kspace_torch = kspace_torch.permute(2, 0, 1, 3).unsqueeze(0) # (B, C, 2, Samples, T)
        elif self.model_type == "LSFPNet":
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            raise ValueError(f"Unsupported model_type for SimulatedDataset: {self.model_type}")
        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask#, parMap, aif, S0, T10, mask
class SimulatedSPFDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.sample_paths = sorted(glob.glob(os.path.join(root_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {root_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {root_dir} for this dataset.")
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
        self.spokes_per_frame = 16
        self.num_frames = 20
        self.window = slice(1, 21)
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        print(f"  Testing {self.spokes_per_frame} spokes/frame with {self.num_frames} frames.")
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_recon = np.load(os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy'))
        ground_truth_complex = dro['ground_truth_images']
        ground_truth_complex = ground_truth_complex[..., self.window]
        smap_torch = rearrange(torch.tensor(csmaps), 'h w c -> c h w').unsqueeze(0)
        simImg_torch = torch.tensor(ground_truth_complex).to(torch.cfloat)
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        return smap_torch, simImg_torch, grasp_recon_torch, mask #, parMap, aif, S0, T10, mask
class SliceDatasetAug(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Splits each volume into Z separate examples (one per slice).
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx=41,
        N_time = 8,
        N_coils=16
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset (e.g. "kspace").
            file_pattern (str): Glob pattern to match your HDF5 files (default "*.h5").
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.N_time = N_time
        self.N_coils = N_coils
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
        self.spokes_range = [12, 16, 24, 32, 36, 48]
    def load_dynamic_img(self, patient_id):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{self.slice_idx:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; "
                                f"expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data) 
    def load_csmaps(self, patient_id):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{self.slice_idx:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.file_list)
    def __getitem__(self, idx):
        """
        Returns a single slice of k-space as a torch.Tensor.
        The output shape will be the standard (C=2, T, S, I) where C is [real, imag].
        """
        file_path = self.file_list[idx]
        patient_id = file_path.split('/')[-1].strip('.h5')
        grasp_img = self.load_dynamic_img(patient_id)
        csmap = self.load_csmaps(patient_id)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[self.slice_idx]
        total_spokes = kspace_slice.shape[0] * kspace_slice.shape[2]
        N_samples = kspace_slice.shape[-1]
        kspace = rearrange(kspace_slice, 't c sp sam -> t sp c sam')
        kspace_flat = kspace.contiguous().view(total_spokes, self.N_coils, N_samples)
        spokes_per_frame = random.choice(self.spokes_range)
        N_time = total_spokes // spokes_per_frame
        kspace_binned = kspace_flat.view(N_time, spokes_per_frame, self.N_coils, N_samples)
        kspace_binned = rearrange(kspace_binned, 't sp c sam -> t c sp sam')
        real_part = kspace_binned.real
        imag_part = kspace_binned.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        return kspace_final, csmap, grasp_img, N_samples, spokes_per_frame, N_time

=== lsp.py ===
import torch
import numpy as np
from time import time
dtype=torch.complex64
def Project_inf(x, c, to_complex=True):
    x_max = torch.maximum((abs(x) / c), torch.tensor(1))
    if to_complex:
        x_max = x_max.to(dtype)
    s = torch.div(x, x_max)
    return s
def Wxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0:x.shape[1]-2] = x[:, 1:x.shape[1]-1]
    temp_x[:, x.shape[1]-1] = temp_x[:, x.shape[1]-1]
    res = temp_x - x
    return res
def Wtxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0] = temp_x[:, 0]
    temp_x[:, 1:x.shape[1]-1] = x[:, 0:x.shape[1]-2]
    res = temp_x - x
    res[:, 0] = -x[:, 0]
    res[:, x.shape[1]-1] = x[:, x.shape[1]-2]
    return res
def LSP(param_E, param_d, param_lambda_L, param_lambda_S, param_nite, param_tol):
    '''
    :param param_E:
    :param param_d:
    :param param_lambda_L:
    :param param_lambda_S:
    :param param_nite:
    :param param_tol:
    :return:
    '''
    M = param_E(inv=True, data=param_d)
    nx, ny, nt = M.size()
    L = torch.zeros([nx * ny, nt], dtype=dtype)
    S = torch.zeros([nx * ny, nt], dtype=dtype)
    p_L = torch.zeros([nx * ny, nt], dtype=dtype)
    p_S = torch.zeros([nx * ny, nt], dtype=dtype)
    gamma = 0.5
    lambda_step = 1/10
    c = lambda_step/gamma
    loss = torch.zeros(param_nite, dtype=float)
    for itr in range(0, param_nite):
        temp_data = torch.reshape(L+S, [nx, ny, nt])
        gradient = param_E(inv=True, data=param_E(inv=False, data=temp_data) - param_d)
        gradient = torch.reshape(gradient, [nx * ny, nt])
        y_L = L - gamma * gradient - gamma * p_L
        Par_L = c * y_L + p_L
        Ut, St, Vt = torch.svd(Par_L)
        temp_St = torch.diag(Project_inf(St, param_lambda_L))
        p_L = Ut.mm(temp_St).mm(Vt.T)
        L = L - gamma * gradient - gamma * p_L
        y_S = S - gamma * gradient - gamma * Wtxs(p_S)
        Par_S = c * Wxs(y_S) + p_S
        p_S = Project_inf(Par_S, param_lambda_S)
        S = S - gamma * gradient - gamma * Wtxs(p_S)
        loss[itr] = 0
        print(' iteration: %d/%d, Loss: %f' % (itr+1, param_nite, loss[itr]))
    Ut, St, Vt = torch.svd(L)
    L = torch.matmul((Ut[:, 0]*St[0]).unsqueeze(1), Vt[:, 0].unsqueeze(0))
    L = torch.reshape(L, [nx, ny, nt])
    S = torch.reshape(S, [nx, ny, nt])
    return L, S, loss

=== radial_lsfp.py ===
import torch
import torch.nn as nn
import numpy as np
from time import time
dtype = torch.complex64
class MCNUFFT(nn.Module):
    def __init__(self, nufft_ob, adjnufft_ob, ktraj, dcomp):
        super(MCNUFFT, self).__init__()
        self.nufft_ob = nufft_ob
        self.adjnufft_ob = adjnufft_ob
        self.ktraj = torch.squeeze(ktraj)
        self.dcomp = torch.squeeze(dcomp)
    def forward(self, inv, data, smaps):
        data = torch.squeeze(data)  # delete redundant dimension
        Nx = smaps.shape[2]
        Ny = smaps.shape[3]
        if inv:  # adjoint nufft
            smaps = smaps.to(dtype)
            if len(data.shape) > 2:  # multi-frame
                x = torch.zeros([Nx, Ny, data.shape[2]], dtype=dtype)
                for ii in range(0, data.shape[2]):
                    kd = data[:, :, ii]
                    k = self.ktraj[:, :, ii]
                    d = self.dcomp[:, ii]
                    kd = kd.unsqueeze(0)
                    d = d.unsqueeze(0).unsqueeze(0)
                    tt1 = time()
                    x_temp = self.adjnufft_ob(kd * d, k, smaps=smaps)
                    x[:, :, ii] = torch.squeeze(x_temp) / np.sqrt(Nx * Ny)
                    tt2 = time()
            else:  # single frame
                kd = data.unsqueeze(0)
                d = self.dcomp.unsqueeze(0).unsqueeze(0)
                x = self.adjnufft_ob(kd * d, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        else:  # forward nufft
            if len(data.shape) > 2:  # multi-frame
                x = torch.zeros([smaps.shape[1], self.ktraj.shape[1], data.shape[-1]], dtype=dtype)
                for ii in range(0, data.shape[-1]):
                    image = data[:, :, ii]
                    k = self.ktraj[:, :, ii]
                    image = image.unsqueeze(0).unsqueeze(0)
                    x_temp = self.nufft_ob(image, k, smaps=smaps)
                    x[:, :, ii] = torch.squeeze(x_temp) / np.sqrt(Nx * Ny)
            else:  # single frame
                image = data.unsqueeze(0).unsqueeze(0)
                x = self.nufft_ob(image, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        return x

=== mc.py ===
from typing import Union
import torch
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from radial import to_torch_complex
class MCLoss(Loss):
    r"""
    Measurement consistency loss
    This loss enforces that the reconstructions are measurement-consistent, i.e., :math:`y=\forw{\inverse{y}}`.
    The measurement consistency loss is defined as
    .. math::
        \|y-\forw{\inverse{y}}\|^2
    where :math:`\inverse{y}` is the reconstructed signal and :math:`A` is a forward operator.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param Metric, torch.nn.Module metric: metric used for computing data consistency, which is set as the mean squared error by default.
    """
    def __init__(self, model_type, metric: Union[Metric, torch.nn.Module] = torch.nn.MSELoss()):
        super(MCLoss, self).__init__()
        self.name = "mc"
        self.metric = metric
        self.device = torch.device("cuda")
        self.model_type = model_type
    def forward(self, y, x_net, physics, csmap, **kwargs):
        r"""
        Computes the measurement splitting loss
        :param torch.Tensor y: measurements.
        :param torch.Tensor x_net: reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: forward operator associated with the measurements.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.model_type == "CRNN":
            return self.metric(physics.A(x_net, csmap), y)
        elif self.model_type == "LSFPNet":
            x_net = to_torch_complex(x_net)
            y_hat = physics(inv=False, data=x_net, smaps=csmap).to(self.device)
            y_hat = torch.stack([y_hat.real, y_hat.imag], dim=-1)
            y = torch.stack([y.real, y.imag], dim=-1)
            return self.metric(y_hat, y)

