=== train.py ===
import argparse
import json
import os
import matplotlib.pyplot as plt
import torch
import yaml
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet_encoding import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex, GRASPRecon, sliding_window_inference
from eval import eval_grasp, eval_sample
import csv
import math
import random
import time 
import seaborn as sns
from loss_metrics import LPIPSVideoMetric, SSIMVideoMetric
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
eval_dir = os.path.join(output_dir, "eval_results")
os.makedirs(eval_dir, exist_ok=True)
block_dir = os.path.join(output_dir, "block_outputs")
os.makedirs(block_dir, exist_ok=True)
ec_dir = os.path.join(output_dir, 'enhancement_curves')
os.makedirs(ec_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                   'lambda_S': config['model']['lambda_S'], 
                   'lambda_spatial_L': config['model']['lambda_spatial_L'],
                   'lambda_spatial_S': config['model']['lambda_spatial_S'],
                   'gamma': config['model']['gamma'],
                   'lambda_step': config['model']['lambda_step']}
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
model_type = config["model"]["name"]
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"]
)
Ng = config["data"]["fpg"] 
N_spokes = int(config["data"]["total_spokes"] / N_time)
N_full = config['data']['height'] * math.pi / 2
eval_chunk_size = config["evaluation"]["chunk_size"]
eval_chunk_overlap = config["evaluation"]["chunk_overlap"]
if config["data"]["train_spokes_per_frame"] != "None":
    train_spokes_per_frame = config["data"]["train_spokes_per_frame"]
else:
    train_spokes_per_frame = None
curriculum_enabled = config['training']['curriculum_learning']['enabled']
curriculum_phases = config['training']['curriculum_learning']['phases']
print(curriculum_enabled)
print(curriculum_phases)
initial_train_spokes_range = [2, 4, 8, 16, 24, 36]
if curriculum_enabled:
    if not curriculum_phases:
        raise ValueError("Curriculum learning enabled but no phases defined in config.yaml")
    initial_train_spokes_range = curriculum_phases[0]['train_spokes_range']
    print(f"Curriculum Learning Enabled. Initial training with spokes range: {initial_train_spokes_range}")
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
else:
    train_patient_ids = splits["train"]
val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]
for val_id in val_patient_ids:
    if val_id in train_patient_ids:
        raise ValueError(f"Data Leakage encountered! Duplicate sample in train and val patient IDs: {val_id}")
if config['dataloader']['slice_range_start'] == "None" or config['dataloader']['slice_range_end'] == "None":
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        num_random_slices=config["dataloader"].get("num_random_slices", None),
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
        spokes_per_frame=train_spokes_per_frame,
        weight_accelerations=config['data']['weight_accelerations'],
        initial_spokes_range=initial_train_spokes_range
    )
else:
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=range(config['dataloader']['slice_range_start'], config['dataloader']['slice_range_end']),
        num_random_slices=config["dataloader"].get("num_random_slices", None),
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
        spokes_per_frame=train_spokes_per_frame,
        weight_accelerations=config['data']['weight_accelerations'],
        initial_spokes_range=initial_train_spokes_range
    )
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
    pin_memory=True,
)
if curriculum_enabled:
    N_spokes_eval = curriculum_phases[0]['eval_spokes_per_frame']
    N_time_eval = curriculum_phases[0]['eval_num_frames']
else:
    N_time_eval, N_spokes_eval = config["data"]["eval_timeframes"], config["data"]["eval_spokes"]
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)
eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    spokes_per_frame=N_spokes_eval,
    num_frames=N_time_eval)
val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
    pin_memory=True,
)
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], lambdas=initial_lambdas, channels=config['model']['channels'])
model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, target_w_ei, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
else:
    start_epoch = 1
    target_w_ei = 0.0
if config['model']['losses']['mc_loss']['metric'] == "MSE":
    mc_loss_fn = MCLoss(model_type=model_type)
elif config['model']['losses']['mc_loss']['metric'] == "MAE":
    mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
else:
    raise(ValueError, "Unsupported MC Loss Metric.")
if config['model']['losses']['ei_loss']['metric'] == "LPIPS":
    ei_loss_metric = LPIPSVideoMetric(net_type='alex') 
elif config['model']['losses']['ei_loss']['metric'] == "SSIM":
    ei_loss_metric = SSIMVideoMetric()
else:
    ei_loss_metric = torch.nn.MSELoss()
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(subsample, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(subsample | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(monophasic_warp, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise, metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp_subsample":
        ei_loss_fn = EILoss((subsample | monophasic_warp) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
            ei_loss_fn = EILoss(rotate, metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
            ei_loss_fn = EILoss(diffeo, metric=ei_loss_metric, model_type=model_type)
        else:
            ei_loss_fn = EILoss(rotate | diffeo, metric=ei_loss_metric, model_type=model_type)
    elif config['model']['losses']['ei_loss']['spatial_transform'] == "all":
        if config['model']['losses']['ei_loss']['temporal_transform'] == "all":
            ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    train_adj_losses = train_curves["train_adj_losses"]
    val_adj_losses = val_curves["val_adj_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
    weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
    eval_ssims = eval_curves["eval_ssims"]
    eval_psnrs = eval_curves["eval_psnrs"]
    eval_mses = eval_curves["eval_mses"]
    eval_lpipses = eval_curves["eval_lpipses"]
    eval_dc_mses = eval_curves["eval_dc_mses"]
    eval_dc_maes = eval_curves["eval_dc_maes"]
    eval_curve_corrs = eval_curves["eval_curve_corrs"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    train_adj_losses = []
    val_adj_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
    weighted_train_adj_losses = []
    eval_ssims = []
    eval_lpipses = []
    eval_psnrs = []
    eval_mses = []
    eval_dc_mses = []
    eval_dc_maes = []
    eval_curve_corrs = []
grasp_ssims = []
grasp_psnrs = []
grasp_mses = []
grasp_lpipses = []
grasp_dc_mses = []
grasp_dc_maes = []
grasp_curve_corrs = []
lambda_Ls = []
lambda_Ss = []
lambda_spatial_Ls = []
lambda_spatial_Ss = []
gammas = []
lambda_steps = []
iteration_count = 0
if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    initial_train_adj_loss = 0.0
    initial_val_adj_loss = 0.0
    initial_eval_ssims = []
    initial_eval_psnrs = []
    initial_eval_mses = []
    initial_eval_lpipses = []
    initial_eval_dc_mses = []
    initial_eval_dc_maes = []
    initial_eval_curve_corrs = []
    with torch.no_grad():
        for measured_kspace, csmap, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            if N_time > Ng:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, Ng)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
            else:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                measured_kspace.to(device), physics, csmap, acceleration_encoding, epoch="train0", norm=config['model']['norm']
            )
            initial_train_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap, acceleration_encoding
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
        train_adj_losses.append(step0_train_adj_loss)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        for measured_kspace, csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            if type(measured_kspace) is list:
                ground_truth_for_physics = rearrange(to_torch_complex(ground_truth), 'b t h w -> b h w t')
                kspace_path = measured_kspace[0]
                measured_kspace = eval_physics(False, ground_truth_for_physics, csmap)
                np.save(kspace_path, measured_kspace.cpu().numpy())
            measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                grasp_img = GRASPRecon(csmap, measured_kspace, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            N_spokes = eval_ktraj.shape[1] / config['data']['samples']
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if N_time_eval > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, adj_loss = sliding_window_inference(H, W, N_samples, int(N_spokes), N_time_eval, eval_chunk_size, eval_chunk_overlap, measured_kspace, csmap, acceleration_encoding, model, epoch="val0", device=device)  
            else:
                x_recon, adj_loss, *_ = model(
                measured_kspace.to(device), eval_physics, csmap, acceleration_encoding, epoch="val0", norm=config['model']['norm']
                )
            initial_val_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, eval_physics, model, csmap, acceleration_encoding
                )
                initial_val_ei_loss += ei_loss.item()
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device, eval_dir)
            grasp_ssims.append(ssim_grasp)
            grasp_psnrs.append(psnr_grasp)
            grasp_mses.append(mse_grasp)
            grasp_lpipses.append(lpips_grasp)
            grasp_dc_mses.append(dc_mse_grasp)
            grasp_dc_maes.append(dc_mae_grasp)
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(measured_kspace, csmap, ground_truth, x_recon, eval_physics, mask, grasp_recon, acceleration, eval_dir, label=None, device=device)
            initial_eval_ssims.append(ssim)
            initial_eval_psnrs.append(psnr)
            initial_eval_mses.append(mse)
            initial_eval_lpipses.append(lpips)
            initial_eval_dc_mses.append(dc_mse)
            initial_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                initial_eval_curve_corrs.append(recon_corr)
                grasp_curve_corrs.append(grasp_corr)
        step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
        val_ei_losses.append(step0_val_ei_loss)
        step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
        val_adj_losses.append(step0_val_adj_loss)
        initial_eval_ssim = np.mean(initial_eval_ssims)
        initial_eval_psnr = np.mean(initial_eval_psnrs)
        initial_eval_mse = np.mean(initial_eval_mses)
        initial_eval_lpips = np.mean(initial_eval_lpipses)
        initial_eval_dc_mse = np.mean(initial_eval_dc_mses)
        initial_eval_dc_mae = np.mean(initial_eval_dc_maes)
        initial_eval_curve_corr = np.mean(initial_eval_curve_corrs)
        eval_ssims.append(initial_eval_ssim)
        eval_psnrs.append(initial_eval_psnr)
        eval_mses.append(initial_eval_mse)
        eval_lpipses.append(initial_eval_lpips)
        eval_dc_mses.append(initial_eval_dc_mse) 
        eval_dc_maes.append(initial_eval_dc_mae) 
        eval_curve_corrs.append(initial_eval_curve_corr)
    print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
    print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    current_curriculum_phase_idx = -1
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        running_adj_loss = 0.0
        epoch_eval_ssims = []
        epoch_eval_psnrs = []
        epoch_eval_mses = []
        epoch_eval_lpipses = []
        epoch_eval_dc_mses = []
        epoch_eval_dc_maes = []
        epoch_eval_curve_corrs = []
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        if hasattr(train_dataset, 'resample_slices'):
            print(f"Epoch {epoch}: Resampling training slices...")
            train_dataset.resample_slices()
        if curriculum_enabled:
            for i, phase in enumerate(curriculum_phases):
                if epoch >= phase['start_epoch']:
                    if i > current_curriculum_phase_idx: # Transition to a new phase
                        print(f"\n--- Entering Curriculum Phase: {phase['name']} at Epoch {epoch} ---")
                        train_dataset.spokes_range = phase['train_spokes_range']
                        train_dataset.update_spokes_weights()
                        current_curriculum_phase_idx = i
                        N_spokes_eval = phase['eval_spokes_per_frame']
                        N_time_eval = phase['eval_num_frames']
                        eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
                        eval_ktraj = eval_ktraj.to(device)
                        eval_dcomp = eval_dcomp.to(device)
                        eval_nufft_ob = eval_nufft_ob.to(device)
                        eval_adjnufft_ob = eval_adjnufft_ob.to(device)
                        eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
                        val_dro_dataset.spokes_per_frame = N_spokes_eval
                        val_dro_dataset.num_frames = N_time_eval
                        val_dro_dataset._update_sample_paths()
                        val_dro_loader = DataLoader(
                            val_dro_dataset,
                            batch_size=config["dataloader"]["batch_size"],
                            shuffle=config["dataloader"]["shuffle"],
                            num_workers=config["dataloader"]["num_workers"],
                            pin_memory=True,
                        )
        if use_ei_loss:
            if epoch < warmup + 1:
                target_w_ei = 0.0
            elif epoch == warmup + 1:
                mc_loss_at_transition = epoch_train_mc_loss
                print(f"Transitioning at Epoch {epoch}. MC Loss: {mc_loss_at_transition:.4e}")
                if step0_train_ei_loss > 0:
                    target_w_ei = mc_loss_at_transition / step0_train_ei_loss
                else:
                    target_w_ei = 0.0 # Prevent division by zero
                print(f"Dynamically calculated target EI weight: {target_w_ei:.4f}")
        for measured_kspace, csmap, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            print("spokes per frame: ", N_spokes)
            start = time.time()
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            if N_time > Ng:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, Ng)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
            else:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            iteration_count += 1
            optimizer.zero_grad()
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                measured_kspace.to(device), physics, csmap, acceleration_encoding, epoch=f"train{epoch}", norm=config['model']['norm']
            )
            running_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            running_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap, acceleration_encoding
                )
                ei_loss_weight = get_cosine_ei_weight(
                    current_epoch=epoch,
                    warmup_epochs=warmup,
                    schedule_duration=duration,
                    target_weight=target_w_ei
                )
                running_ei_loss += ei_loss.item()
                total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(
                    mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                )
            else:
                total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
            if torch.isnan(total_loss):
                print(
                    "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                )
                raise RuntimeError("total_loss is NaN")
            total_loss.backward()
            if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                log_gradient_stats(
                    model=model,
                    epoch=epoch,
                    iteration=iteration_count,
                    output_dir=output_dir,
                    log_filename="gradient_stats.csv"
                )
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            end = time.time()
            print("time for one iteration: ", end-start)
        if epoch % save_interval == 0:
            plot_reconstruction_sample(
                x_recon,
                f"Training Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)})",
                f"train_sample_epoch_{epoch}",
                output_dir,
            )
            x_recon_reshaped = rearrange(x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'train_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    t_img,
                    f"Transformed Train Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)})",
                    f"transforms/transform_train_sample_epoch_{epoch}",
                    output_dir,
                    x_recon,
                    transform=True
                )
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        epoch_train_adj_loss = running_adj_loss / len(train_loader)
        train_adj_losses.append(epoch_train_adj_loss)
        weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_running_adj_loss = 0.0
        val_loader_tqdm = tqdm(
            val_dro_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask, grasp_path in tqdm(val_dro_loader):
                val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                if type(val_kspace_batch) is list:
                    ground_truth_for_physics = rearrange(to_torch_complex(val_ground_truth), 'b t h w -> b h w t')
                    kspace_path = val_kspace_batch[0]
                    print("ground_truth_for_physics: ", ground_truth_for_physics.shape)
                    print("val_csmap: ", val_csmap.shape)
                    print("eval_physics.ktraj: ", eval_physics.ktraj.shape)
                    print("spokes_per_frame: ", val_dro_dataset.spokes_per_frame)
                    print("num_frames: ", val_dro_dataset.num_frames)
                    print("N_time_eval: ", N_time_eval)
                    print("N_spokes_eval: ", N_spokes_eval)
                    val_kspace_batch = eval_physics(False, ground_truth_for_physics, val_csmap)
                val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                if type(val_grasp_img) is int or len(val_grasp_img.shape) == 1:
                    print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                    val_grasp_img = GRASPRecon(val_csmap, val_kspace_batch, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                    val_grasp_img = torch.from_numpy(val_grasp_img).permute(2, 0, 1) # T, H, W
                    val_grasp_img = torch.stack([val_grasp_img.real, val_grasp_img.imag], dim=0)
                    val_grasp_img = torch.flip(val_grasp_img, dims=[-3])
                    val_grasp_img = torch.rot90(val_grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                val_grasp_img_tensor = val_grasp_img.to(device)
                N_spokes = eval_ktraj.shape[1] / config['data']['samples']
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                if N_time_eval > eval_chunk_size:
                    print("Performing sliding window eval...")
                    val_x_recon, val_adj_loss = sliding_window_inference(H, W, N_samples, int(N_spokes), N_time_eval, eval_chunk_size, eval_chunk_overlap, val_kspace_batch, val_csmap, acceleration_encoding, model, epoch=f"val{epoch}", device=device)  
                else:
                    val_x_recon, val_adj_loss, *_ = model(
                    val_kspace_batch.to(device), eval_physics, val_csmap, acceleration_encoding, epoch=f"val{epoch}", norm=config['model']['norm']
                    )
                val_running_adj_loss += val_adj_loss.item()
                val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                val_running_mc_loss += val_mc_loss.item()
                if use_ei_loss:
                    val_ei_loss, val_t_img = ei_loss_fn(
                        val_x_recon, eval_physics, model, val_csmap, acceleration_encoding
                    )
                    val_running_ei_loss += val_ei_loss.item()
                    val_loader_tqdm.set_postfix(
                        val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                    )
                else:
                    val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, _ = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, acceleration, eval_dir, f'epoch{epoch}', device)
                epoch_eval_ssims.append(ssim)
                epoch_eval_psnrs.append(psnr)
                epoch_eval_mses.append(mse)
                epoch_eval_lpipses.append(lpips)
                epoch_eval_dc_mses.append(dc_mse)
                epoch_eval_dc_maes.append(dc_mae)
                if recon_corr is not None:
                    epoch_eval_curve_corrs.append(recon_corr)
        epoch_eval_ssim = np.mean(epoch_eval_ssims)
        epoch_eval_psnr = np.mean(epoch_eval_psnrs)
        epoch_eval_mse = np.mean(epoch_eval_mses)
        epoch_eval_lpips = np.mean(epoch_eval_lpipses)
        epoch_eval_dc_mse = np.mean(epoch_eval_dc_mses)
        epoch_eval_dc_mae = np.mean(epoch_eval_dc_maes)
        epoch_eval_curve_corr = np.mean(epoch_eval_curve_corrs)
        eval_ssims.append(epoch_eval_ssim)
        eval_psnrs.append(epoch_eval_psnr)
        eval_mses.append(epoch_eval_mse)
        eval_lpipses.append(epoch_eval_lpips)
        eval_dc_mses.append(epoch_eval_dc_mse) 
        eval_dc_maes.append(epoch_eval_dc_mae)    
        eval_curve_corrs.append(epoch_eval_curve_corr)  
        if epoch % save_interval == 0:
            plot_reconstruction_sample(
                val_x_recon,
                f"Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)})",
                f"val_sample_epoch_{epoch}",
                output_dir,
                val_grasp_img
            )
            val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                val_x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
            plot_enhancement_curve(
                val_grasp_img,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)})",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
        epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            val_adj_losses.append(epoch_val_adj_loss)
        else:
            val_adj_losses.append(0.0)
        if epoch % save_interval == 0:
            train_curves = dict(
                train_mc_losses=train_mc_losses,
                train_ei_losses=train_ei_losses,
                weighted_train_mc_losses=weighted_train_mc_losses,
                weighted_train_ei_losses=weighted_train_ei_losses,
            )
            val_curves = dict(
                val_mc_losses=val_mc_losses,
                val_ei_losses=val_ei_losses,
            )
            eval_curves = dict(
                eval_ssims=eval_ssims,
                eval_psnrs=eval_psnrs,
                eval_mses=eval_mses,
                eval_lpipses=eval_lpipses,
                eval_dc_mses=eval_dc_mses,
                eval_dc_maes=eval_dc_maes,
                eval_curve_corrs=eval_curve_corrs
            )
            model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
            save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, target_w_ei, model_save_path)
            print(f'Model saved to {model_save_path}')
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            sns.lineplot(x=range(len(train_adj_losses)), y=train_adj_losses, ax=axes[0, 0])
            axes[0, 0].set_title("Training Adjoint Loss")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("Adjoint Loss")
            sns.lineplot(x=range(len(train_mc_losses)), y=train_mc_losses, ax=axes[0, 1])
            axes[0, 1].set_title("Training MC Loss")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("MC Loss")
            sns.lineplot(x=range(len(train_ei_losses)), y=train_ei_losses, ax=axes[0, 2])
            axes[0, 2].set_title("Training EI Loss")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("EI Loss")
            sns.lineplot(x=range(len(val_adj_losses)), y=val_adj_losses, ax=axes[1, 0], color='orange')
            axes[1, 0].set_title(f"Validation Adjoint Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("Adjoint Loss")
            sns.lineplot(x=range(len(val_mc_losses)), y=val_mc_losses, ax=axes[1, 1], color='orange')
            axes[1, 1].set_title(f"Validation MC Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("MC Loss")
            sns.lineplot(x=range(len(val_ei_losses)), y=val_ei_losses, ax=axes[1, 2], color='orange')
            axes[1, 2].set_title(f"Validation EI Loss ({N_spokes_eval} spokes/frame)")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("EI Loss")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "losses.png"))
            plt.close()
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            sns.lineplot(x=range(len(lambda_Ls)), y=lambda_Ls, ax=axes[0, 0])
            axes[0, 0].set_title("Lambda_L Parameter Value")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("Lambda_L")
            sns.lineplot(x=range(len(lambda_Ss)), y=lambda_Ss, ax=axes[0, 1])
            axes[0, 1].set_title("Lambda_S Parameter Value")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("Lambda_S")
            sns.lineplot(x=range(len(lambda_spatial_Ls)), y=lambda_spatial_Ls, ax=axes[0, 2])
            axes[0, 2].set_title("Spatial Lambda_L Parameter Value")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("Spatial Lambda_L")
            sns.lineplot(x=range(len(lambda_spatial_Ss)), y=lambda_spatial_Ss, ax=axes[1, 0])
            axes[1, 0].set_title("Spatial Lambda_S Parameter Value")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("Spatial Lambda_S")
            sns.lineplot(x=range(len(gammas)), y=gammas, ax=axes[1, 1])
            axes[1, 1].set_title("Gamma Parameter Value")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("Gamma")
            sns.lineplot(x=range(len(lambda_steps)), y=lambda_steps, ax=axes[1, 2])
            axes[1, 2].set_title("Lambda Step Parameter Value")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("Lambda Step")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "parameters.png"))
            plt.close()
            plt.figure()
            plt.plot(weighted_train_mc_losses, label="MC Loss")
            plt.plot(weighted_train_ei_losses, label="EI Loss")
            plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Weighted Training Losses")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
            plt.close()
            sns.set_style("whitegrid")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle(f'Evaluation Metrics Over Epochs ({N_spokes_eval} spokes/frame)', fontsize=20)
            sns.lineplot(x=range(len(eval_ssims)), y=eval_ssims, ax=axes[0, 0])
            axes[0, 0].set_title("Evaluation SSIM")
            axes[0, 0].set_xlabel("Epoch")
            axes[0, 0].set_ylabel("SSIM")
            sns.lineplot(x=range(len(eval_psnrs)), y=eval_psnrs, ax=axes[0, 1])
            axes[0, 1].set_title("Evaluation PSNR")
            axes[0, 1].set_xlabel("Epoch")
            axes[0, 1].set_ylabel("PSNR")
            sns.lineplot(x=range(len(eval_mses)), y=eval_mses, ax=axes[0, 2])
            axes[0, 2].set_title("Evaluation Image MSE")
            axes[0, 2].set_xlabel("Epoch")
            axes[0, 2].set_ylabel("MSE")
            sns.lineplot(x=range(len(eval_lpipses)), y=eval_lpipses, ax=axes[1, 0])
            axes[1, 0].set_title("Evaluation LPIPS")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("LPIPS")
            sns.lineplot(x=range(len(eval_dc_maes)), y=eval_dc_maes, ax=axes[1, 1])
            axes[1, 1].set_title("Evaluation k-space MAE")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("MAE")
            sns.lineplot(x=range(len(eval_curve_corrs)), y=eval_curve_corrs, ax=axes[1, 2])
            axes[1, 2].set_title("Tumor Enhancement Curve Correlation")
            axes[1, 2].set_xlabel("Epoch")
            axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.savefig(os.path.join(output_dir, "eval_metrics.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dc_mses)
            plt.xlabel("Epoch")
            plt.ylabel("k-space MSE")
            plt.title("Evaluation Data Consistency (MSE)")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dc_mses.png"))
            plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
        if model_type == "LSFPNet":
            print(
                f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
            )
        print(f"--- Evaluation Metrics: Epoch {epoch} ---")
        print(f"Recon SSIM: {epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}")
        print(f"Recon PSNR: {epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}")
        print(f"Recon MSE: {epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}")
        print(f"Recon LPIPS: {epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}")
        print(f"Recon DC MSE: {epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}")
        print(f"Recon DC MAE: {epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}")
        print(f"Recon Enhancement Curve Correlation: {epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}")
        print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}")
        print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}")
        print(f"GRASP MSE: {np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}")
        print(f"GRASP LPIPS: {np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}")
        print(f"GRASP DC MSE: {np.mean(grasp_dc_mses):.6f} ± {np.std(grasp_dc_mses):.4f}")
        print(f"GRASP DC MAE: {np.mean(grasp_dc_maes):.6f} ± {np.std(grasp_dc_maes):.4f}")
        print(f"GRASP Enhancement Curve Correlation: {np.mean(grasp_curve_corrs):.6f} ± {np.std(grasp_curve_corrs):.4f}")
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    train_adj_losses=train_adj_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
    weighted_train_adj_losses=weighted_train_adj_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
    val_adj_losses=val_adj_losses,
)
eval_curves = dict(
    eval_ssims=eval_ssims,
    eval_psnrs=eval_psnrs,
    eval_mses=eval_mses,
    eval_lpipses=eval_lpipses,
    eval_dc_mses=eval_dc_mses,
    eval_dc_maes=eval_dc_maes,
    eval_curve_corrs=eval_curve_corrs,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, target_w_ei, model_save_path)
print(f'Model saved to {model_save_path}')
metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
with open(metrics_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
    writer.writerow(['DL', 
                     f'{epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}', 
                     f'{epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}', 
                     f'{epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}',
                     f'{epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}',  
                     f'{epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}', 
                     f'{epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}', 
                     f'{epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}'])
    writer.writerow(['GRASP', 
                     f'{np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}', 
                     f'{np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}', 
                     f'{np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}', 
                     f'{np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}', 
                     f'{np.mean(grasp_dc_mses):.4f} ± {np.std(grasp_dc_mses):.4f}', 
                     f'{np.mean(grasp_dc_maes):.4f} ± {np.std(grasp_dc_maes):.4f}', 
                     f'{np.mean(grasp_curve_corrs):.4f} ± {np.std(grasp_curve_corrs):.4f}'])
MAIN_EVALUATION_PLAN = [
    {
        "spokes_per_frame": 8,
        "num_frames": 36, # 8 * 36 = 288 total spokes
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 16,
        "num_frames": 18, # 16 * 18 = 288 total spokes
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 24,
        "num_frames": 12, # 24 * 12 = 288 total spokes
        "description": "Good temporal resolution"
    },
    {
        "spokes_per_frame": 32,
        "num_frames": 8, # 36 * 8 = 288 total spokes
        "description": "Standard temporal resolution"
    },
]
STRESS_TEST_PLAN = [
    {
        "spokes_per_frame": 2,
        "num_frames": 144, # 2 * 144 = 288 total spokes
        "description": "Stress test: max temporal points, 2 spokes"
    },
    {
        "spokes_per_frame": 4,
        "num_frames": 72, # 4 * 72 = 288 total spokes
        "description": "Stress test: max temporal points, 4 spokes"
    },
]
eval_spf_dataset = SimulatedSPFDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    )
eval_spf_loader = DataLoader(
    eval_spf_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
with torch.no_grad():
    spf_recon_ssim = {}
    spf_recon_psnr = {}
    spf_recon_mse = {}
    spf_recon_lpips = {}
    spf_recon_dc_mse = {}
    spf_recon_dc_mae = {}
    spf_recon_corr = {}
    spf_grasp_ssim = {}
    spf_grasp_psnr = {}
    spf_grasp_mse = {}
    spf_grasp_lpips = {}
    spf_grasp_dc_mse = {}
    spf_grasp_dc_mae = {}
    spf_grasp_corr = {}
    print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
    for eval_config in STRESS_TEST_PLAN:
        stress_test_ssims = []
        stress_test_psnrs = []
        stress_test_mses = []
        stress_test_lpipses = []
        stress_test_dc_mses = []
        stress_test_dc_maes = []
        stress_test_corrs = []
        stress_test_grasp_ssims = []
        stress_test_grasp_psnrs = []
        stress_test_grasp_mses = []
        stress_test_grasp_lpipses = []
        stress_test_grasp_dc_mses = []
        stress_test_grasp_dc_maes = []
        stress_test_grasp_corrs = []
        spokes = eval_config["spokes_per_frame"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.num_frames = num_frames
        eval_spf_dataset._update_sample_paths()
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            if num_frames > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, _ = sliding_window_inference(H, W, N_samples, int(spokes), num_frames, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, model, epoch=None, device=device)  
            else:
                x_recon, *_ = model(
                    kspace.to(device), physics, csmap, acceleration_encoding, epoch=None, norm=config['model']['norm']
                )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, eval_dir, f"{spokes}spf", device)
            stress_test_ssims.append(ssim)
            stress_test_psnrs.append(psnr)
            stress_test_mses.append(mse)
            stress_test_lpipses.append(lpips)
            stress_test_dc_mses.append(dc_mse)
            stress_test_dc_maes.append(dc_mae)
            if recon_corr is not None:
                stress_test_corrs.append(recon_corr)
                stress_test_grasp_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            stress_test_grasp_ssims.append(ssim_grasp)
            stress_test_grasp_psnrs.append(psnr_grasp)
            stress_test_grasp_mses.append(mse_grasp)
            stress_test_grasp_lpipses.append(lpips_grasp)
            stress_test_grasp_dc_mses.append(dc_mse_grasp)
            stress_test_grasp_dc_maes.append(dc_mae_grasp)
            spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
            spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
            spf_recon_mse[spokes] = np.mean(stress_test_mses)
            spf_recon_lpips[spokes] = np.mean(stress_test_lpipses)
            spf_recon_dc_mse[spokes] = np.mean(stress_test_dc_mses)
            spf_recon_dc_mae[spokes] = np.mean(stress_test_dc_maes)
            spf_recon_corr[spokes] = np.mean(stress_test_corrs)
            spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
            spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
            spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
            spf_grasp_lpips[spokes] = np.mean(stress_test_grasp_lpipses)
            spf_grasp_dc_mse[spokes] = np.mean(stress_test_grasp_dc_mses)
            spf_grasp_dc_mae[spokes] = np.mean(stress_test_grasp_dc_maes)
            spf_grasp_corr[spokes] = np.mean(stress_test_grasp_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', "LPIPS", 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(stress_test_ssims):.4f} ± {np.std(stress_test_ssims):.4f}', 
            f'{np.mean(stress_test_psnrs):.4f} ± {np.std(stress_test_psnrs):.4f}', 
            f'{np.mean(stress_test_mses):.4f} ± {np.std(stress_test_mses):.4f}', 
            f'{np.mean(stress_test_lpipses):.4f} ± {np.std(stress_test_lpipses):.4f}', 
            f'{np.mean(stress_test_dc_mses):.4f} ± {np.std(stress_test_dc_mses):.4f}',
            f'{np.mean(stress_test_dc_maes):.4f} ± {np.std(stress_test_dc_maes):.4f}',
            f'{np.mean(stress_test_corrs):.4f} ± {np.std(stress_test_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(stress_test_grasp_ssims):.4f} ± {np.std(stress_test_grasp_ssims):.4f}', 
            f'{np.mean(stress_test_grasp_psnrs):.4f} ± {np.std(stress_test_grasp_psnrs):.4f}', 
            f'{np.mean(stress_test_grasp_mses):.4f} ± {np.std(stress_test_grasp_mses):.4f}', 
            f'{np.mean(stress_test_grasp_lpipses):.4f} ± {np.std(stress_test_grasp_lpipses):.4f}', 
            f'{np.mean(stress_test_grasp_dc_mses):.4f} ± {np.std(stress_test_grasp_dc_mses):.4f}',
            f'{np.mean(stress_test_grasp_dc_maes):.4f} ± {np.std(stress_test_grasp_dc_maes):.4f}',
            f'{np.mean(stress_test_grasp_corrs):.4f} ± {np.std(stress_test_grasp_corrs):.4f}',
            ])
    print("--- Running Main Evaluation (Budget: 320 spokes) ---")
    for eval_config in MAIN_EVALUATION_PLAN:
        spf_eval_ssims = []
        spf_eval_psnrs = []
        spf_eval_mses = []
        spf_eval_lpipses = []
        spf_eval_dc_mses = []
        spf_eval_dc_maes = []
        spf_eval_curve_corrs = []
        spf_grasp_ssims = []
        spf_grasp_psnrs = []
        spf_grasp_mses = []
        spf_grasp_lpipses = []
        spf_grasp_dc_mses = []
        spf_grasp_dc_maes = []
        spf_grasp_curve_corrs = []
        spokes = eval_config["spokes_per_frame"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.num_frames = num_frames
        eval_spf_dataset._update_sample_paths()
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            if num_frames > eval_chunk_size:
                print("Performing sliding window eval...")
                x_recon, _ = sliding_window_inference(H, W, N_samples, int(spokes), num_frames, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, model, epoch=None, device=device)  
            else:
                x_recon, *_ = model(
                kspace.to(device), physics, csmap, acceleration_encoding, epoch=None, norm=config['model']['norm']
                )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, eval_dir, f'{spokes}spf', device)
            spf_eval_ssims.append(ssim)
            spf_eval_psnrs.append(psnr)
            spf_eval_mses.append(mse)
            spf_eval_lpipses.append(lpips)
            spf_eval_dc_mses.append(dc_mse)
            spf_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                spf_eval_curve_corrs.append(recon_corr)
                spf_grasp_curve_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            spf_grasp_ssims.append(ssim_grasp)
            spf_grasp_psnrs.append(psnr_grasp)
            spf_grasp_mses.append(mse_grasp)
            spf_grasp_lpipses.append(lpips_grasp)
            spf_grasp_dc_mses.append(dc_mse_grasp)
            spf_grasp_dc_maes.append(dc_mae_grasp)
        spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
        spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
        spf_recon_mse[spokes] = np.mean(spf_eval_mses)
        spf_recon_lpips[spokes] = np.mean(spf_eval_lpipses)
        spf_recon_dc_mse[spokes] = np.mean(spf_eval_dc_mses)
        spf_recon_dc_mae[spokes] = np.mean(spf_eval_dc_maes)
        spf_recon_corr[spokes] = np.mean(spf_eval_curve_corrs)
        spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
        spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
        spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
        spf_grasp_lpips[spokes] = np.mean(spf_grasp_lpipses)
        spf_grasp_dc_mse[spokes] = np.mean(spf_grasp_dc_mses)
        spf_grasp_dc_mae[spokes] = np.mean(spf_grasp_dc_maes)
        spf_grasp_corr[spokes] = np.mean(spf_grasp_curve_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(spf_eval_ssims):.4f} ± {np.std(spf_eval_ssims):.4f}', 
            f'{np.mean(spf_eval_psnrs):.4f} ± {np.std(spf_eval_psnrs):.4f}', 
            f'{np.mean(spf_eval_mses):.4f} ± {np.std(spf_eval_mses):.4f}', 
            f'{np.mean(spf_eval_lpipses):.4f} ± {np.std(spf_eval_lpipses):.4f}', 
            f'{np.mean(spf_eval_dc_mses):.4f} ± {np.std(spf_eval_dc_mses):.4f}',
            f'{np.mean(spf_eval_dc_maes):.4f} ± {np.std(spf_eval_dc_maes):.4f}',
            f'{np.mean(spf_eval_curve_corrs):.4f} ± {np.std(spf_eval_curve_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(spf_grasp_ssims):.4f} ± {np.std(spf_grasp_ssims):.4f}', 
            f'{np.mean(spf_grasp_psnrs):.4f} ± {np.std(spf_grasp_psnrs):.4f}', 
            f'{np.mean(spf_grasp_mses):.4f} ± {np.std(spf_grasp_mses):.4f}', 
            f'{np.mean(spf_grasp_lpipses):.4f} ± {np.std(spf_grasp_lpipses):.4f}', 
            f'{np.mean(spf_grasp_dc_mses):.4f} ± {np.std(spf_grasp_dc_mses):.4f}',
            f'{np.mean(spf_grasp_dc_maes):.4f} ± {np.std(spf_grasp_dc_maes):.4f}',
            f'{np.mean(spf_grasp_curve_corrs):.4f} ± {np.std(spf_grasp_curve_corrs):.4f}'
            ])
sns.set_style("whitegrid")
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
sns.lineplot(x=list(spf_recon_ssim.keys()), 
             y=list(spf_recon_ssim.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 0])
sns.lineplot(x=list(spf_grasp_ssim.keys()), 
             y=list(spf_grasp_ssim.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 0])
axes[0, 0].set_title("Evaluation SSIM vs Spokes/Frame")
axes[0, 0].set_xlabel("Epoch")
axes[0, 0].set_ylabel("SSIM")
sns.lineplot(x=list(spf_recon_psnr.keys()), 
             y=list(spf_recon_psnr.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 1])
sns.lineplot(x=list(spf_grasp_psnr.keys()), 
             y=list(spf_grasp_psnr.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 1])
axes[0, 1].set_title("Evaluation PSNR vs Spokes/Frame")
axes[0, 1].set_xlabel("Epoch")
axes[0, 1].set_ylabel("PSNR")
sns.lineplot(x=list(spf_recon_mse.keys()), 
             y=list(spf_recon_mse.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[0, 2])
sns.lineplot(x=list(spf_grasp_mse.keys()), 
             y=list(spf_grasp_mse.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[0, 2])
axes[0, 2].set_title("Evaluation Image MSE vs Spokes/Frame")
axes[0, 2].set_xlabel("Epoch")
axes[0, 2].set_ylabel("MSE")
sns.lineplot(x=list(spf_recon_lpips.keys()), 
             y=list(spf_recon_lpips.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 0])
sns.lineplot(x=list(spf_grasp_lpips.keys()), 
             y=list(spf_grasp_lpips.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 0])
axes[1, 0].set_title("Evaluation LPIPS vs Spokes/Frame")
axes[1, 0].set_xlabel("Epoch")
axes[1, 0].set_ylabel("LPIPS")
sns.lineplot(x=list(spf_recon_dc_mae.keys()), 
             y=list(spf_recon_dc_mae.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 1])
sns.lineplot(x=list(spf_grasp_dc_mae.keys()), 
             y=list(spf_grasp_dc_mae.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 1])
axes[1, 1].set_title("Evaluation k-space MAE vs Spokes/Frame")
axes[1, 1].set_xlabel("Epoch")
axes[1, 1].set_ylabel("MAE")
sns.lineplot(x=list(spf_recon_corr.keys()), 
             y=list(spf_recon_corr.values()), 
             label="DL Recon", 
             marker='o',
             ax=axes[1, 2])
sns.lineplot(x=list(spf_grasp_corr.keys()), 
             y=list(spf_grasp_corr.values()), 
             label="Standard Recon", 
             marker='o',
             ax=axes[1, 2])
axes[1, 2].set_title("Tumor Enhancement Curve Correlation vs Spokes/Frame")
axes[1, 2].set_xlabel("Epoch")
axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "spf_eval_metrics.png"))
plt.close()

=== lsfpnet_encoding.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
import matplotlib.pyplot as plt
import numpy as np
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class MappingNetwork(nn.Module):
    """Maps a scalar input to a style vector using a simple MLP."""
    def __init__(self, style_dim, num_layers=4):
        super().__init__()
        layers = [nn.Linear(1, style_dim), nn.ReLU(True)]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(style_dim, style_dim), nn.ReLU(True)])
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        if x.dim() == 0:
            x = x.unsqueeze(0)
        if x.dim() == 1:
            x = x.unsqueeze(1)
        return self.net(x)
class BasicBlock(nn.Module):
    def __init__(self, lambdas, channels=32, style_dim=128):
        super(BasicBlock, self).__init__()
        self.channels = channels
        self.style_dim = style_dim
        self.lambda_L = nn.Parameter(torch.tensor([lambdas['lambda_L']]))
        self.lambda_S = nn.Parameter(torch.tensor([lambdas['lambda_S']]))
        self.lambda_spatial_L = nn.Parameter(torch.tensor([lambdas['lambda_spatial_L']]))
        self.lambda_spatial_S = nn.Parameter(torch.tensor([lambdas['lambda_spatial_S']]))
        self.gamma = nn.Parameter(torch.tensor([lambdas['gamma']]))
        self.lambda_step = nn.Parameter(torch.tensor([lambdas['lambda_step']]))
        self.style_injector_L = nn.Linear(self.style_dim, self.channels * 2) # *2 for scale and bias
        self.style_injector_S = nn.Linear(self.style_dim, self.channels * 2)
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmaps, style_embedding=None):
        c = self.lambda_step / self.gamma
        nx, ny, nt = M0.size()
        temp_data = torch.reshape(L + S, [nx, ny, nt])
        temp_data = param_E(inv=False, data=temp_data, smaps=csmaps).to(param_d.device)
        gradient = param_E(inv=True, data=temp_data - param_d, smaps=csmaps)
        gradient = torch.reshape(gradient, [nx * ny, nt]).to(param_d.device)
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(torch.squeeze(pb_L), [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        svd_input_complex = c * y_L + pt_L
        svd_input_mag = svd_input_complex.abs() + 1e-8
        original_phase = svd_input_complex / svd_input_mag
        noise_std = 1e-3 # A small standard deviation for the noise
        noise = torch.randn_like(svd_input_mag) * noise_std
        stable_svd_input = svd_input_mag + noise #epsilon
        if torch.isnan(stable_svd_input).any() or torch.isinf(stable_svd_input).any():
            print("!!! SVD input contains NaN or Inf values. Halting. !!!")
            import pdb; pdb.set_trace()
        Ut, St, Vt = torch.linalg.svd(stable_svd_input, full_matrices=False)
        St_shrunk = Project_inf(St, self.lambda_L, to_complex=False)
        pt_L_mag = Ut @ torch.diag_embed(St_shrunk) @ Vt
        pt_L = pt_L_mag * original_phase
        St_shrunk = Project_inf(St, self.lambda_L, to_complex=False)
        pt_L_mag = Ut @ torch.diag_embed(St_shrunk) @ Vt
        pt_L = pt_L_mag * original_phase
        temp_y_L_input = torch.cat((torch.real(y_L), torch.imag(y_L)), 0).to(torch.float32)
        temp_y_L_input = torch.reshape(temp_y_L_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_L = F.conv3d(temp_y_L_input, self.conv1_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L = F.conv3d(temp_y_L, self.conv2_forward_l, padding=1)
        if style_embedding is not None:
            style_params_L = self.style_injector_L(style_embedding)
            scale_L, bias_L = style_params_L.chunk(2, dim=-1) # Split into [1, channels] each
            scale_L = scale_L.view(1, self.channels, 1, 1, 1)
            bias_L = bias_L.view(1, self.channels, 1, 1, 1)
            temp_y_L = F.relu(temp_y_L * (scale_L + 1) + bias_L)
        else: 
            temp_y_L = F.relu(temp_y_L)
        temp_y_L_output = F.conv3d(temp_y_L, self.conv3_forward_l, padding=1)
        temp_y_L = temp_y_L_output + p_L
        temp_y_L = temp_y_L[0, :, :, :, :] + 1j * temp_y_L[1, :, :, :, :]
        p_L = Project_inf(c * temp_y_L, self.lambda_spatial_L)
        p_L = torch.cat((torch.real(p_L), torch.imag(p_L)), 0).to(torch.float32)
        p_L = torch.reshape(p_L, [2, self.channels, nx, ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L_output = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(pb_L_output, [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        adjloss_L = temp_y_L_output * p_L - pb_L_output * temp_y_L_input
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, self.lambda_S)
        temp_y_S_input = torch.cat((torch.real(y_S), torch.imag(y_S)), 0).to(torch.float32)
        temp_y_S_input = torch.reshape(temp_y_S_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_S = F.conv3d(temp_y_S_input, self.conv1_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S = F.conv3d(temp_y_S, self.conv2_forward_s, padding=1)
        if style_embedding is not None:
            style_params_S = self.style_injector_S(style_embedding)
            scale_S, bias_S = style_params_S.chunk(2, dim=-1)
            scale_S = scale_S.view(1, self.channels, 1, 1, 1)
            bias_S = bias_S.view(1, self.channels, 1, 1, 1)
            temp_y_S = F.relu(temp_y_S * (scale_S + 1) + bias_S)
        else:
            temp_y_S = F.relu(temp_y_S)
        temp_y_S_output = F.conv3d(temp_y_S, self.conv3_forward_s, padding=1)
        temp_y_Sp = temp_y_S_output + p_S
        temp_y_Sp = temp_y_Sp[0, :, :, :, :] + 1j * temp_y_Sp[1, :, :, :, :]
        p_S = Project_inf(c * temp_y_Sp, self.lambda_spatial_S)
        p_S = torch.cat((torch.real(p_S), torch.imag(p_S)), 0).to(torch.float32)
        p_S = torch.reshape(p_S, [2, self.channels, nx, ny, nt])
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S_output = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S_output, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        adjloss_S = temp_y_S_output * p_S - pb_S_output * temp_y_S_input
        return [L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S, self.lambda_L, self.lambda_S, self.lambda_spatial_L, self.lambda_spatial_S, self.gamma, self.lambda_step]
class LSFPNet(nn.Module):
    def __init__(self, LayerNo, lambdas, channels=32, style_dim=128):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        self.channels = channels
        self.style_dim = style_dim
        for ii in range(LayerNo):
            onelayer.append(BasicBlock(lambdas, channels=self.channels, style_dim=style_dim))
        self.fcs = nn.ModuleList(onelayer)
    def plot_block_output(self, M0, L, S, iter, epoch, output_dir):
        time_frame_index = 3
        nx, ny, nt = M0.size()
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        output_image = L + S
        fig, axes = plt.subplots(1, 4, figsize=(24, 6))
        fig.suptitle(f"Basic Block Output at Time Frame {time_frame_index} and Iteration {iter}", fontsize=20)
        axes[0].imshow(np.abs(M0[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[0].set_title("Input Image")
        axes[0].axis("off")
        axes[1].imshow(np.abs(L[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[1].set_title("Background Component (L)")
        axes[1].axis("off")
        axes[2].imshow(np.abs(S[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[2].set_title("Dynamic Component (S)")
        axes[2].axis("off")
        axes[3].imshow(np.abs(output_image[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[3].set_title("Combined Image (L + S)")
        axes[3].axis("off")
        filename = os.path.join(output_dir, f'basic_block_output_{epoch}_iter{iter}.png')
        plt.savefig(filename)
        plt.close()
    def forward(self, M0, param_E, param_d, csmap, epoch, output_dir, style_embedding=None):
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmap, style_embedding)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
            self.plot_block_output(M0, L, S, iter=ii, epoch=epoch, output_dir=output_dir)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step]
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, output_dir, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
        self.output_dir = output_dir
        self.style_dim = 128  # You can tune this hyperparameter
        self.mapping_network = MappingNetwork(style_dim=self.style_dim)
    @staticmethod
    def _normalise_both(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max() + 1e-8                     # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_indep(x: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = torch.quantile(x.abs(), 0.99) + 1e-6
        if scale < 1e-6: # Handle case where input is all zeros
             scale = 1.0
        return x / scale, scale
    def forward(self, y, E, csmap, acceleration=None, epoch=None, norm="both", **kwargs):
        x_init = E(inv=True, data=y, smaps=csmap)
        if norm =="both":
            x_init_norm, y_norm, scale = self._normalise_both(x_init, y)
        elif norm == "independent":
            x_init_norm, scale = self._normalise_indep(x_init)
            y_norm, scale_y = self._normalise_indep(y)
        elif norm == "none":
            x_init_norm = x_init
            y_norm = y
            scale = 1.0
        if acceleration:
            style_embedding = self.mapping_network(acceleration)
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir, style_embedding)
        else:
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir)
        loss_constraint_L = torch.square(torch.mean(loss_layers_adj_L[0])) / self.backbone_net.LayerNo
        loss_constraint_S = torch.square(torch.mean(loss_layers_adj_S[0])) / self.backbone_net.LayerNo
        for k in range(self.backbone_net.LayerNo - 1):
            loss_constraint_S += torch.square(torch.mean(loss_layers_adj_S[k + 1])) / self.backbone_net.LayerNo
            loss_constraint_L += torch.square(torch.mean(loss_layers_adj_L[k + 1])) / self.backbone_net.LayerNo
        recon = (L + S) * scale                 # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat, loss_constraint_L + loss_constraint_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step

=== radial_lsfp.py ===
import torch
import torch.nn as nn
import numpy as np
from time import time
dtype = torch.complex64
class MCNUFFT(nn.Module):
    def __init__(self, nufft_ob, adjnufft_ob, ktraj, dcomp):
        super(MCNUFFT, self).__init__()
        self.nufft_ob = nufft_ob
        self.adjnufft_ob = adjnufft_ob
        self.ktraj = torch.squeeze(ktraj)
        self.dcomp = torch.squeeze(dcomp)
    def forward(self, inv, data, smaps):
        data = torch.squeeze(data)
        Nx, Ny = smaps.shape[2], smaps.shape[3]
        if len(data.shape) > 2:  # multi-frame
            is_complex_data = torch.is_complex(data)
            if inv: # Adjoint NUFFT (k-space -> image)
                kd = data.permute(2, 0, 1) # -> [time, coils, samples]
                d = self.dcomp.permute(1, 0) # -> [time, samples]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                d = d.unsqueeze(1) 
                x_temp = self.adjnufft_ob(kd * d, k, smaps=smaps.to(dtype))
                x = x_temp.squeeze(1).permute(1, 2, 0) / np.sqrt(Nx * Ny)
            else: # Forward NUFFT (image -> k-space)
                image = data.permute(2, 0, 1).unsqueeze(1) # -> [time, 1, Nx, Ny]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                x_temp = self.nufft_ob(image, k, smaps=smaps)
                x = x_temp.permute(1, 2, 0) / np.sqrt(Nx * Ny)
        else:  # single frame (original logic is fine)
            if inv:
                kd = data.unsqueeze(0)
                d = self.dcomp.unsqueeze(0).unsqueeze(0)
                x = self.adjnufft_ob(kd * d, self.ktraj, smaps=smaps.to(dtype))
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
            else:
                image = data.unsqueeze(0).unsqueeze(0)
                x = self.nufft_ob(image, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        return x

=== eval.py ===
import os
import matplotlib.pyplot as plt
import torch
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import torchmetrics
import time
from dataloader import SimulatedDataset
from lsfpnet import to_torch_complex, from_torch_complex
import numpy as np
from scipy.optimize import curve_fit
from scipy.interpolate import PchipInterpolator
from tqdm import tqdm # A library for a nice progress bar
from scipy.stats import mannwhitneyu
from skimage.metrics import structural_similarity as ssim_map_func
import matplotlib.gridspec as gridspec
from skimage.measure import find_contours
from typing import List, Dict
from scipy.stats import pearsonr
def normalize_for_lpips(image, data_range):
    """Normalizes an image tensor to the [-1, 1] range for LPIPS."""
    min_val, max_val = data_range
    image_0_1 = (image - min_val) / (max_val - min_val)
    image_minus1_1 = 2 * image_0_1 - 1
    return image_minus1_1
def calc_image_metrics(input, reference, data_range, device, filename):
    """
    Calculates image metrics for a given input and reference image.
    """
    ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range).to(device)
    psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range).to(device)
    mse = torchmetrics.MeanSquaredError().to(device)
    lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=False).to(device)
    ssim = ssim(input, reference)
    psnr = psnr(input, reference)
    mse = mse(input, reference)
    if input.dim() == 5:
        num_slices = input.shape[2]
        lpips_scores = []
        for i in range(num_slices):
            input_slice = input[:, :, i, :, :]
            reference_slice = reference[:, :, i, :, :]
            input_lpips = normalize_for_lpips(input_slice.clone(), data_range)
            reference_lpips = normalize_for_lpips(reference_slice.clone(), data_range)
            if input_lpips.shape[1] == 1:
                input_lpips = input_lpips.repeat(1, 3, 1, 1)
                reference_lpips = reference_lpips.repeat(1, 3, 1, 1)
            input_lpips = input_lpips.to(reference_lpips.dtype)
            lpips_scores.append(lpips_metric(input_lpips, reference_lpips).item())
        final_lpips = sum(lpips_scores) / len(lpips_scores)
    return ssim.item(), psnr.item(), mse.item(), final_lpips
def calc_dc(input, reference, device):
    """
    Calculates data consistency MSE for a given input and reference k-space tensor.
    """
    mse = torchmetrics.MeanSquaredError().to(device)
    mae = torchmetrics.MeanAbsoluteError().to(device)
    input = from_torch_complex(input).to(device)
    reference = from_torch_complex(reference).to(device)
    mse = mse(input, reference)
    mae = mae(input, reference)
    return mse.item(), mae.item()
def evaluate_reconstruction_fidelity(
    ground_truth_params: np.ndarray,
    estimated_params: np.ndarray,
    masks: dict,
    param_names: list = None,
    regions_to_evaluate: list = None,
    display_plots: bool = True,
    filename: str = 'pk_param_maps.png'
) -> dict:
    """
    Evaluates the fidelity of reconstructed pharmacokinetic (PK) parameters against ground truth.
    This function performs a quantitative and visual comparison, mimicking the evaluation
    methods described in the research paper (e.g., Figures 6 and 8).
    Args:
        ground_truth_params (np.ndarray): The ground truth PK parameter map, typically a
                                          (H, W, 4) array from the `gen_dro` output.
        estimated_params (np.ndarray): The PK parameter map estimated from your reconstructed
                                       images, with the same shape as ground_truth_params.
        masks (dict): A dictionary of boolean masks for different tissue regions, typically
                      from the `gen_dro` output (e.g., dro_results['mask']).
        param_names (list, optional): A list of names for the 4 parameters.
                                      Defaults to ['ve', 'vp', 'Fp', 'PS'].
        regions_to_evaluate (list, optional): A list of region names (keys in the `masks`
                                            dict) to analyze. Defaults to all available masks.
        display_plots (bool): If True, generates and shows summary plots.
    Returns:
        dict: A nested dictionary containing the evaluation results (median error and p-value)
              for each region and each parameter.
    """
    if param_names is None:
        param_names = ['ve', 'vp', 'Fp (F_p)', 'PS'] # As ordered in gen_dro
    if regions_to_evaluate is None:
        regions_to_evaluate = [name for name, mask in masks.items() if mask.any()]
    if ground_truth_params.shape != estimated_params.shape:
        raise ValueError("Ground truth and estimated parameter maps must have the same shape.")
    evaluation_results = {}
    print("--- Reconstruction Fidelity Evaluation ---")
    print("-" * 40)
    for region in regions_to_evaluate:
        if region not in masks or not masks[region].any():
            continue
        print(f"Region: {region.capitalize()}")
        evaluation_results[region] = {}
        mask = masks[region]
        for i, p_name in enumerate(param_names):
            print("ground_truth_params: ", type(ground_truth_params))
            gt_values = ground_truth_params[:, :, i][mask]
            est_values = estimated_params[:, :, i][mask]
            print("gt_values: ", type(gt_values))
            gt_values_safe = gt_values.copy()
            gt_values_safe[gt_values_safe == 0] = 1e-9 # Add a small epsilon
            relative_error = (est_values - gt_values) / gt_values_safe
            median_err = np.median(relative_error)
            try:
                stat, p_value = mannwhitneyu(gt_values, est_values, alternative='two-sided')
            except ValueError: # Happens if all values are identical
                stat, p_value = 0, 1.0
            evaluation_results[region][p_name] = {
                'median_relative_error': median_err,
                'p_value': p_value
            }
            print(f"  - {p_name:<10}: Median Error = {median_err:+.2%}, p-value = {p_value:.4f}")
    if not display_plots:
        return evaluation_results
    num_params = ground_truth_params.shape[2]
    fig, axes = plt.subplots(num_params, 3, figsize=(15, 4 * num_params), sharex=True, sharey=True)
    fig.suptitle("Visual Comparison of PK Parameter Maps", fontsize=16)
    for i in range(num_params):
        p_name = param_names[i]
        gt_map = ground_truth_params[:, :, i]
        est_map = estimated_params[:, :, i]
        error_map = est_map - gt_map
        vmax = np.percentile(gt_map[gt_map > 0], 99) if (gt_map > 0).any() else 1.0
        vmin = 0
        im_gt = axes[i, 0].imshow(gt_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 0].set_title(f"Ground Truth: {p_name}")
        axes[i, 0].axis('off')
        fig.colorbar(im_gt, ax=axes[i, 0])
        im_est = axes[i, 1].imshow(est_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 1].set_title(f"Your Estimation: {p_name}")
        axes[i, 1].axis('off')
        fig.colorbar(im_est, ax=axes[i, 1])
        err_vmax = np.percentile(np.abs(error_map), 99)
        im_err = axes[i, 2].imshow(error_map, vmin=-err_vmax, vmax=err_vmax, cmap='coolwarm')
        axes[i, 2].set_title(f"Error Map (Est - GT)")
        axes[i, 2].axis('off')
        fig.colorbar(im_err, ax=axes[i, 2])
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return evaluation_results
def plot_spatial_quality(
    recon_img: np.ndarray,
    gt_img: np.ndarray,
    grasp_img: np.ndarray,
    time_frame_index: int,
    filename: str,
    grasp_comparison_filename: str,
    data_range: float, 
    acceleration: float,
):
    """
    Generates a comparison plot for a single time frame in a 2x4 grid.
    Each row includes: Ground Truth, Reconstruction, Error Map, and SSIM Map.
    Args:
        recon_img (np.ndarray): Your model's reconstructed image for this frame.
        gt_img (np.ndarray): The ground truth image for this frame.
        grasp_img (np.ndarray): The GRASP reconstruction image for this frame.
        time_frame_index (int): The index of the time frame for titling.
        filename (str): The path to save the output plot.
    """
    error_map_dl = recon_img - gt_img
    error_map_grasp = grasp_img - gt_img
    ssim_dl, ssim_map_dl = ssim_map_func(gt_img, recon_img, data_range=data_range, full=True)
    ssim_grasp, ssim_map_grasp = ssim_map_func(gt_img, grasp_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(2, 4, figsize=(24, 12))
    fig.suptitle(f"Spatial Quality Comparison at Time Frame {time_frame_index} with AF {acceleration}", fontsize=20)
    axes[0, 0].imshow(gt_img, cmap='gray')
    axes[0, 0].set_title("Ground Truth")
    axes[0, 1].imshow(recon_img, cmap='gray')
    axes[0, 1].set_title("DL Reconstruction")
    im_err_dl = axes[0, 2].imshow(error_map_dl, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[0, 2].set_title("DL Error Map (Recon - GT)")
    fig.colorbar(im_err_dl, ax=axes[0, 2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[0, 3].imshow(ssim_map_dl, cmap='viridis', vmin=0, vmax=1)
    axes[0, 3].set_title(f"DL SSIM Map (SSIM Recon vs GT: {round(ssim_dl, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[0, 3], fraction=0.046, pad=0.04)
    axes[1, 0].imshow(gt_img, cmap='gray')
    axes[1, 0].set_title("Ground Truth")
    axes[1, 1].imshow(grasp_img, cmap='gray')
    axes[1, 1].set_title("GRASP Reconstruction")
    im_err_grasp = axes[1, 2].imshow(error_map_grasp, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[1, 2].set_title("GRASP Error Map (Recon - GT)")
    fig.colorbar(im_err_grasp, ax=axes[1, 2], fraction=0.046, pad=0.04)
    im_ssim_grasp = axes[1, 3].imshow(ssim_map_grasp, cmap='viridis', vmin=0, vmax=1)
    axes[1, 3].set_title(f"GRASP SSIM Map (SSIM Recon vs GT: {round(ssim_grasp, 3)})")
    fig.colorbar(im_ssim_grasp, ax=axes[1, 3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    plt.savefig(filename)
    plt.close()
    error_map = recon_img - grasp_img
    ssim, ssim_map = ssim_map_func(grasp_img, recon_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(1, 4, figsize=(24, 6))
    fig.suptitle(f"DL vs GRASP Comparison at Time Frame {time_frame_index} with AF {acceleration}", fontsize=20)
    axes[0].imshow(grasp_img, cmap='gray')
    axes[0].set_title("GRASP Reconstruction")
    axes[1].imshow(recon_img, cmap='gray')
    axes[1].set_title("DL Reconstruction")
    im_err_dl = axes[2].imshow(error_map, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[2].set_title("Error Map (DL Recon - GRASP)")
    fig.colorbar(im_err_dl, ax=axes[2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[3].imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)
    axes[3].set_title(f"SSIM Map (SSIM between DL and GRASP Recons: {round(ssim, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    print("SSIM between GRASP and DL Recon: ", ssim)
    plt.savefig(grasp_comparison_filename)
    plt.close()
def plot_temporal_curves(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    masks: dict,
    time_points: np.ndarray,
    filename: str, 
    acceleration: float,
):
    """
    Plots the mean signal intensity vs. time for different tissue regions.
    This is CRITICAL for debugging PK model fitting.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        masks (dict): Dictionary of boolean NumPy masks for different regions.
        time_points (np.ndarray): The time vector for the x-axis.
        filename (str): The path to save the output plot.
    """
    regions = [r for r in ['malignant', 'glandular', 'muscle'] if r in masks and masks[r].any()]
    if not regions:
        print("No relevant regions found in mask to plot temporal curves.")
        return
    fig, axes = plt.subplots(1, len(regions), figsize=(7 * len(regions), 5), sharey=True)
    if len(regions) == 1: axes = [axes] # Ensure axes is always a list
    fig.suptitle(f"Temporal Fidelity: Mean Signal vs. Time (AF = {acceleration})", fontsize=16)
    for i, region in enumerate(regions):
        mask = masks[region]
        gt_curve = [gt_img_stack[:, :, t][mask].mean() for t in range(gt_img_stack.shape[2])]
        recon_curve = [recon_img_stack[:, :, t][mask].mean() for t in range(recon_img_stack.shape[2])]
        grasp_curve = [grasp_img_stack[:, :, t][mask].mean() for t in range(grasp_img_stack.shape[2])]
        if region == 'malignant':
            recon_correlation, _ = pearsonr(recon_curve, gt_curve)
            grasp_correlation, _ = pearsonr(grasp_curve, gt_curve)
        axes[i].plot(time_points, gt_curve, 'k-', label='Ground Truth', linewidth=2, marker='o')
        axes[i].plot(time_points, recon_curve, 'r--', label='DL Recon', marker='o')
        axes[i].plot(time_points, grasp_curve, 'b:', label='GRASP Recon', marker='o')
        axes[i].set_title(f"Region: {region.capitalize()}")
        axes[i].set_xlabel("Time (s)")
        axes[i].grid(True)
        axes[i].legend()
    axes[0].set_ylabel("Mean Signal Intensity")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return recon_correlation, grasp_correlation
def plot_single_temporal_curve(
    img_stack: np.ndarray,
    masks: Dict[str, np.ndarray],
    time_points: np.ndarray,
    num_frames: int,
    filename: str,
    acceleration: float,
    frames_to_show: List[int] = None,
):
    """
    Generates a comprehensive analysis plot for a single sample, showing the
    Tumor Contrast Enhancement Curve (CEC) and corresponding image frames with
    the tumor Region of Interest (ROI) highlighted.
    This function is modified to produce a detailed analysis plot for the
    'malignant' tissue type, using the ground truth data.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        grasp_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        masks (dict): Dictionary of boolean NumPy masks. Expects a 'malignant' key.
        time_points (np.ndarray): The time vector for the x-axis (e.g., frame numbers).
        filename (str): The path to save the output plot.
        sample_name (str): The name of the sample for the main plot title.
        frames_to_show (List[int]): A list of 4 frame indices to display in the
                                    image grid and highlight on the curve.
                                    If None, defaults to [0, 6, 13, 20].
    """
    region_key = 'malignant'
    if region_key not in masks or not masks[region_key].any():
        print(f"'{region_key}' mask not found or is empty. Skipping plot generation.")
        return
    tumor_mask = masks[region_key]
    if frames_to_show is None:
        interval = round(num_frames / 4)
        frames_to_show = [0, interval, 2*interval, num_frames-1]
    if len(frames_to_show) != 4:
        raise ValueError(f"This function is designed to show exactly 4 frames, but {len(frames_to_show)} were provided.")
    fig = plt.figure(figsize=(20, 8.5))
    fig.suptitle(f"Tumor Enhancement Over Time (AF = {acceleration})")
    gs = gridspec.GridSpec(2, 4, figure=fig, hspace=0.1, wspace=0.1)
    ax_curve = fig.add_subplot(gs[:, 0:2])
    ax_imgs = [
        fig.add_subplot(gs[0, 2]), fig.add_subplot(gs[0, 3]),
        fig.add_subplot(gs[1, 2]), fig.add_subplot(gs[1, 3])
    ]
    mean_curve = [img_stack[:, :, t][tumor_mask].mean() for t in range(img_stack.shape[2])]
    ax_curve.plot(time_points, mean_curve, 'o-', label='Mean Tumor Signal', linewidth=2, markersize=6)
    highlight_times = [time_points[i] for i in frames_to_show]
    highlight_vals = [mean_curve[i] for i in frames_to_show]
    ax_curve.plot(highlight_times, highlight_vals, 'r*', markersize=18, zorder=10) # zorder to ensure stars are on top
    ax_curve.set_title("Tumor Contrast Enhancement Curve (CEC)", fontsize=18, pad=10)
    ax_curve.set_xlabel("Time Frame", fontsize=16)
    ax_curve.set_ylabel("Mean Signal Intensity", fontsize=16)
    ax_curve.legend(fontsize=14)
    ax_curve.grid(True, linestyle='--')
    ax_curve.tick_params(axis='both', which='major', labelsize=14)
    contours = find_contours(tumor_mask, 0.5)
    for i, frame_idx in enumerate(frames_to_show):
        ax = ax_imgs[i]
        image = img_stack[:, :, frame_idx]
        ax.imshow(image, cmap='gray')#, vmin=vmin, vmax=vmax)
        for contour in contours:
            ax.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='red')
        ax.set_title(f"Frame {frame_idx}", fontsize=16)
        ax.axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect for suptitle
    plt.savefig(filename, bbox_inches='tight', dpi=150)
    plt.close(fig)
def plot_time_series(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    filename: str,
    acceleration: float,
):
    """
    Plots the middle 5 time points for Ground Truth, DL Recon, and GRASP.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        filename (str): The path to save the output plot.
    """
    num_frames = gt_img_stack.shape[2]
    indices = np.linspace(0, num_frames - 1, 5, dtype=int)
    fig, axes = plt.subplots(3, 5, figsize=(25, 15))
    fig.suptitle(f"Temporal Series Comparison (AF = {acceleration})", fontsize=20)
    for i, frame_idx in enumerate(indices):
        img = gt_img_stack[:, :, frame_idx]
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f"GT: Frame {frame_idx}")
        axes[0, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = recon_img_stack[:, :, frame_idx]
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].set_title(f"DL: Frame {frame_idx}")
        axes[1, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = grasp_img_stack[:, :, frame_idx]
        axes[2, i].imshow(img, cmap='gray')
        axes[2, i].set_title(f"GRASP: Frame {frame_idx}")
        axes[2, i].axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(filename)
    plt.close()
def tofts_model(t, Ktrans, ve, aif_t, aif_c):
    """Calculates the tissue concentration curve using the standard Tofts model."""
    ve = max(ve, 1e-6)
    interp_func = PchipInterpolator(aif_t, aif_c, extrapolate=True)
    aif_interp = interp_func(t)
    impulse_response = Ktrans * np.exp(-t * (Ktrans / ve))
    dt = t[1] - t[0] if len(t) > 1 else 1.0
    Ct = np.convolve(aif_interp, impulse_response, mode='full')[:len(t)] * dt
    return Ct
def signal_to_concentration(signal_curve, S0_pixel, T10_pixel, TR, r1, flip_angle_rad):
    """Converts an MRI signal curve S(t) to a concentration curve C(t)."""
    if S0_pixel < 1e-6:
        return np.zeros_like(signal_curve)
    norm_signal = signal_curve / S0_pixel
    sin_fa, cos_fa = np.sin(flip_angle_rad), np.cos(flip_angle_rad)
    denominator = sin_fa - norm_signal * cos_fa
    E1 = (sin_fa - norm_signal) / np.maximum(denominator, 1e-9)
    E1 = np.maximum(E1, 1e-9)
    R1_t = -np.log(E1) / TR
    R10 = 1.0 / T10_pixel
    concentration_curve = (R1_t - R10) / r1
    return np.maximum(0, concentration_curve)
def estimate_pk_parameters(
    reconstructed_images: np.ndarray,
    aif_t: np.ndarray,
    aif_c: np.ndarray,
    S0_map: np.ndarray,
    T10_map: np.ndarray,
    TR: float = 4.87e-3,
    r1: float = 4.3,
    flip_angle_deg: float = 10.0
) -> np.ndarray:
    """
    Estimates pharmacokinetic parameters (Ktrans, ve) from reconstructed DCE-MRI images.
    Args:
        reconstructed_images (np.ndarray): A (H, W, Time) array of dynamic images,
                                           THIS IS THE OUTPUT FROM YOUR DL MODEL.
        aif_t, aif_c (np.ndarray): The time points and concentrations for the AIF.
        S0_map, T10_map (np.ndarray): Baseline maps from the ground truth DRO.
        TR, r1, flip_angle_deg: Sequence parameters.
    Returns:
        np.ndarray: A (H, W, 4) array containing the estimated [ve, Ktrans, 0, 0] maps.
    """
    height, width, num_frames = reconstructed_images.shape
    flip_angle_rad = np.deg2rad(flip_angle_deg)
    time_points = aif_t
    ktrans_map = np.zeros((height, width))
    ve_map = np.zeros((height, width))
    fitting_func = lambda t, Ktrans, ve: tofts_model(t, Ktrans, ve, aif_t, aif_c)
    DEBUG_PIXEL_R, DEBUG_PIXEL_C = 150, 150
    print("Estimating PK parameters from the reconstructed images...")
    for r in tqdm(range(height), desc="Fitting PK Model"):
        for c in range(width):
            if S0_map[r, c] < np.mean(S0_map) * 0.1:
                continue
            signal_curve = np.abs(reconstructed_images[r, c, :])
            concentration_curve = signal_to_concentration(
                signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
            )
            try:
                initial_guess = [0.1 / 60, 0.2] # Ktrans in s^-1
                bounds = ([0, 0], [2.0 / 60, 1.0])
                params, _ = curve_fit(
                    fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                )
                ktrans_map[r, c] = params[0] * 60 # Convert from s^-1 to min^-1
                ve_map[r, c] = params[1]
            except RuntimeError:
                pass # Fit failed, leave as 0
            if r == DEBUG_PIXEL_R and c == DEBUG_PIXEL_C:
                print(f"\n--- DEBUGGING PIXEL ({r}, {c}) ---")
                signal_curve = np.abs(reconstructed_images[r, c, :])
                concentration_curve = signal_to_concentration(
                    signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
                )
                plt.figure(figsize=(10, 6))
                plt.plot(time_points, concentration_curve, 'bo', label='Measured Concentration (from DL Recon)')
                try:
                    params, _ = curve_fit(
                        fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                    )
                    ktrans_fit, ve_fit = params
                    fitted_curve = tofts_model(time_points, ktrans_fit, ve_fit, aif_t, aif_c)
                    plt.plot(time_points, fitted_curve, 'r-', label=f'Tofts Fit (Ktrans={ktrans_fit*60:.3f}, ve={ve_fit:.3f})')
                except RuntimeError:
                    plt.title(f"DEBUG: Curve fit FAILED for pixel ({r}, {c})")
                plt.xlabel("Time (s)")
                plt.ylabel("Concentration")
                plt.legend()
                plt.grid(True)
                plt.savefig(f"debug_pixel_fit_{r}_{c}.png")
                plt.close()
                print("--- DEBUG PLOT SAVED ---")
    zeros_map = np.zeros_like(ktrans_map)
    estimated_pk_map = np.stack([ve_map, ktrans_map, zeros_map, zeros_map], axis=-1)
    return estimated_pk_map
def eval_grasp(kspace, csmap, ground_truth, grasp_recon, physics, device, output_dir):
    grasp_recon_complex = rearrange(to_torch_complex(grasp_recon).squeeze(), 'h t w -> h w t')
    kspace = kspace.squeeze()
    grasp_kspace = physics(False, grasp_recon_complex.to(csmap.dtype), csmap)
    dc_mse_grasp, dc_mae_grasp = calc_dc(grasp_kspace, kspace, device)
    grasp_recon_np = grasp_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    c = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_recon = torch.tensor(c * grasp_recon_np, device=device)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    grasp_mag = torch.sqrt(grasp_recon[:, 0, ...]**2 + grasp_recon[:, 1, ...]**2)
    grasp_mag = rearrange(grasp_mag, 'c h t w -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp = calc_image_metrics(grasp_mag.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    return ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp
def eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, output_dir, label, device):
    acceleration = round(acceleration.item(), 1)
    x_recon_complex = to_torch_complex(x_recon).squeeze()
    kspace = kspace.squeeze()
    recon_kspace = physics(False, x_recon_complex, csmap)
    dc_mse, dc_mae = calc_dc(recon_kspace, kspace, device)
    x_recon_np = x_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    grasp_recon_np = grasp_img.cpu().numpy()
    c = np.dot(x_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(x_recon_np.flatten(), x_recon_np.flatten())
    recon_complex_scaled = torch.tensor(c * x_recon_np, device=device)
    c_grasp = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_img = torch.tensor(c_grasp * grasp_recon_np, device=device)
    recon_mag_scaled = torch.sqrt(recon_complex_scaled[:, 0, ...]**2 + recon_complex_scaled[:, 1, ...]**2)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    recon_mag_scaled = rearrange(recon_mag_scaled, 'c h w t -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim, psnr, mse, lpips = calc_image_metrics(recon_mag_scaled.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    grasp_recon_complex_np = rearrange(to_torch_complex(grasp_img).squeeze(), 'h t w -> h w t').cpu().numpy()
    grasp_mag_np = np.abs(grasp_recon_complex_np)
    x_recon_complex_np = to_torch_complex(recon_complex_scaled).squeeze().cpu().numpy()
    gt_squeezed = ground_truth.squeeze()  # Shape: (C, T, H, W) -> (2, 22, 320, 320)
    gt_rearranged = rearrange(gt_squeezed, 'c t h w -> t c h w') # Shape: (22, 320, 320, 2)
    gt_complex_tensor = to_torch_complex(gt_rearranged) # Shape: (22, 320, 320)
    gt_final_tensor = rearrange(gt_complex_tensor, 't h w -> h w t') # Shape: (320, 320, 22)
    gt_complex_np = gt_final_tensor.cpu().numpy()
    recon_mag_np = np.abs(x_recon_complex_np)
    gt_mag_np = np.abs(gt_complex_np)
    masks_np = {key: val.cpu().numpy().squeeze().astype(bool) for key, val in mask.items()}
    num_frames = recon_mag_np.shape[2]
    aif_time_points = np.linspace(0, 150, num_frames)
    print("\nGenerating diagnostic plots...")
    if mask['malignant'].any() and label is not None:
        peak_frame = num_frames // 3
        data_range = gt_mag_np[:, :, peak_frame].max() - gt_mag_np[:, :, peak_frame].min()
        plot_spatial_quality(
            recon_img=recon_mag_np[:, :, peak_frame],
            gt_img=gt_mag_np[:, :, peak_frame],
            grasp_img=grasp_mag_np[:, :, peak_frame],
            time_frame_index=peak_frame,
            filename=os.path.join(output_dir, f"spatial_quality_{label}.png"),
            grasp_comparison_filename=os.path.join(output_dir, f"grasp_comparison_{label}.png"),
            data_range=data_range,
            acceleration=acceleration,
        )
        recon_corr, grasp_corr = plot_temporal_curves(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            filename=os.path.join(output_dir, f"temporal_curves_{label}.png"),
            acceleration=acceleration,
        )
        plot_single_temporal_curve(
            img_stack=recon_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            num_frames=num_frames,
            filename=os.path.join(output_dir, f"recon_temporal_curve_{label}.png"),
            acceleration=acceleration,
        )
        plot_time_series(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            filename=os.path.join(output_dir, f"time_points_{label}.png"),
            acceleration=acceleration,
        )
        print("Diagnostic plots saved.")
    else:
        recon_corr, grasp_corr = None, None
    return ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr

=== mc.py ===
from typing import Union
import torch
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from radial import to_torch_complex
class MCLoss(Loss):
    r"""
    Measurement consistency loss
    This loss enforces that the reconstructions are measurement-consistent, i.e., :math:`y=\forw{\inverse{y}}`.
    The measurement consistency loss is defined as
    .. math::
        \|y-\forw{\inverse{y}}\|^2
    where :math:`\inverse{y}` is the reconstructed signal and :math:`A` is a forward operator.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param Metric, torch.nn.Module metric: metric used for computing data consistency, which is set as the mean squared error by default.
    """
    def __init__(self, model_type, metric: Union[Metric, torch.nn.Module] = torch.nn.MSELoss()):
        super(MCLoss, self).__init__()
        self.name = "mc"
        self.metric = metric
        self.device = torch.device("cuda")
        self.model_type = model_type
    def forward(self, y, x_net, physics, csmap, **kwargs):
        r"""
        Computes the measurement splitting loss
        :param torch.Tensor y: measurements.
        :param torch.Tensor x_net: reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: forward operator associated with the measurements.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.model_type == "CRNN":
            return self.metric(physics.A(x_net, csmap), y)
        elif self.model_type == "LSFPNet":
            x_net = to_torch_complex(x_net)
            y_hat = physics(inv=False, data=x_net, smaps=csmap).to(self.device)
            y_hat = torch.stack([y_hat.real, y_hat.imag], dim=-1)
            y = torch.stack([y.real, y.imag], dim=-1)
            return self.metric(y_hat, y)

=== ei.py ===
from typing import Union
import torch
import torch.nn as nn
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from einops import rearrange
from radial import to_torch_complex
class EILoss(Loss):
    r"""
    Equivariant imaging self-supervised loss.
    Assumes that the set of signals is invariant to a group of transformations (rotations, translations, etc.)
    in order to learn from incomplete measurement data alone https://https://arxiv.org/pdf/2103.14756.pdf.
    The EI loss is defined as
    .. math::
        \| T_g \hat{x} - \inverse{\forw{T_g \hat{x}}}\|^2
    where :math:`\hat{x}=\inverse{y}` is a reconstructed signal and
    :math:`T_g` is a transformation sampled at random from a group :math:`g\sim\group`.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param deepinv.transform.Transform transform: Transform to generate the virtually augmented measurement.
        It can be any torch-differentiable function (e.g., a :class:`torch.nn.Module`)
        including `torchvision transforms <https://pytorch.org/vision/stable/transforms.html>`_.
    :param Metric, torch.nn.Module metric: Metric used to compute the error between the reconstructed augmented measurement and the reference
        image.
    :param bool apply_noise: if ``True``, the augmented measurement is computed with the full sensing model
        :math:`\sensor{\noise{\forw{\hat{x}}}}` (i.e., noise and sensor model),
        otherwise is generated as :math:`\forw{\hat{x}}`.
    :param float weight: Weight of the loss.
    :param bool no_grad: if ``True``, the gradient does not propagate through :math:`T_g`. Default: ``False``.
        This option is useful for super-resolution problems, see https://arxiv.org/abs/2312.11232.
    """
    def __init__(
        self,
        transform: Transform,
        model_type: str,
        metric: Union[Metric, nn.Module] = torch.nn.MSELoss(),
        apply_noise=True,
        weight=1.0,
        no_grad=False,
        *args,
        **kwargs,
    ):
        super(EILoss, self).__init__(*args, **kwargs)
        self.name = "ei"
        self.metric = metric
        self.weight = weight
        self.T = transform
        self.noise = apply_noise
        self.no_grad = no_grad
        self.model_type = model_type
    def forward(self, x_net, physics, model, csmap, acceleration, **kwargs):
        r"""
        Computes the EI loss
        :param torch.Tensor x_net: Reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: Forward operator associated with the measurements.
        :param torch.nn.Module model: Reconstruction function.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.no_grad:
            with torch.no_grad():
                x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
                x2_rearranged = self.T(x_net_rearranged)
                x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
                x2 = x2.detach()
        else:
            x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
            x2_rearranged = self.T(x_net_rearranged)
            x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
        x2_complex = to_torch_complex(x2)
        y = physics(inv=False, data=x2_complex, smaps=csmap).to(csmap.device)
        x3, *_ = model(y, physics, csmap, acceleration, epoch=None)
        loss_ei = self.weight * self.metric(x3, x2)
        return loss_ei, x2

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
from typing import Union, List, Optional
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Can either use a fixed set of slices or randomly sample N slices per volume
        at the start of each epoch.
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx: Optional[Union[int, range]] = 41,
        num_random_slices: Optional[int] = None,  # New parameter for random sampling
        N_time=8,
        N_coils=16,
        spf_aug=False,
        spokes_per_frame=None,
        weight_accelerations=False, 
        initial_spokes_range=[2, 4, 8, 16, 24, 36]
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            patient_ids (list): List of patient IDs to filter the files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset.
            file_pattern (str): Glob pattern to match your HDF5 files.
            slice_idx (int, range, optional): A fixed slice index or range of indices to use.
                                              This is ignored if num_random_slices is set.
            num_random_slices (int, optional): If provided, the dataset will randomly sample
                                               this many slices from each volume at the beginning
                                               of each epoch.
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.num_random_slices = num_random_slices
        self.N_time = N_time
        self.N_coils = N_coils
        self.spf_aug = spf_aug
        self.weight_acc = weight_accelerations
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
        if self.num_random_slices is not None:
            print(f"Initializing in random slice sampling mode with N={self.num_random_slices} slices per volume.")
            self.volume_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    num_slices = f[self.dataset_key].shape[0]
                    self.volume_map.append((fp, num_slices))
            self.resample_slices()
        else:
            print(f"Initializing in fixed slice mode with slice_idx={self.slice_idx}.")
            self.slice_index_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    ds = f[self.dataset_key]
                    num_slices = ds.shape[0]
                slices_to_add = []
                if isinstance(self.slice_idx, int):
                    if self.slice_idx < num_slices:
                        slices_to_add = [self.slice_idx]
                    else:
                        print(f"Warning: slice_idx {self.slice_idx} is out of bounds for {fp} "
                              f"(size {num_slices}). Skipping this file for this slice.")
                elif isinstance(self.slice_idx, range):
                    slices_to_add = [s for s in self.slice_idx if s < num_slices]
                    if len(slices_to_add) < len(self.slice_idx):
                        print(f"Warning: Some requested slices were out of bounds for {fp}. "
                              f"Using only the valid slice indices from the provided range.")
                else:
                    raise TypeError(f"slice_idx must be an int, range, or None, but got {type(self.slice_idx)}")
                for z in slices_to_add:
                    self.slice_index_map.append((fp, z))
        print(f"Dataset initialized with {len(self.slice_index_map)} total slice examples.")
        self.spokes_per_frame = spokes_per_frame
        self.spokes_range = initial_spokes_range
        self.update_spokes_weights()
    def update_spokes_weights(self):
        if self.weight_acc:
            self.spf_weights = [1.0 / spf for spf in self.spokes_range]
        else:
            self.spf_weights = [1.0 for spf in self.spokes_range]
    def resample_slices(self):
        """
        Resamples N unique slices from each volume. This should be called at the
        beginning of each training epoch to ensure the model sees different data.
        """
        if self.num_random_slices is None:
            return
        self.slice_index_map = []
        for file_path, num_slices in self.volume_map:
            if num_slices >= self.num_random_slices:
                selected_slices = random.sample(range(num_slices), self.num_random_slices)
            else:
                print(f"Warning: Volume {os.path.basename(file_path)} has only {num_slices} slices, "
                      f"which is less than the requested {self.num_random_slices}. Using all available slices.")
                selected_slices = list(range(num_slices))
            for z in selected_slices:
                self.slice_index_map.append((file_path, z))
    def load_dynamic_img(self, patient_id, slice):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{slice:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data)
    def load_csmaps(self, patient_id, slice):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{slice:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.slice_index_map)
    def __getitem__(self, idx):
        file_path, current_slice_idx = self.slice_index_map[idx]
        current_slice_idx = int(current_slice_idx)
        patient_id = file_path.split('/')[-1].strip('.h5')
        csmap = self.load_csmaps(patient_id, current_slice_idx)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[current_slice_idx]
        if self.spf_aug or self.spokes_per_frame:
            total_spokes = kspace_slice.shape[0] * kspace_slice.shape[2]
            N_samples = kspace_slice.shape[-1]
            kspace = rearrange(kspace_slice, 't c sp sam -> t sp c sam')
            kspace_flat = kspace.contiguous().view(total_spokes, self.N_coils, N_samples)
            if self.spf_aug:
                print("setting random spokes per frame...")
                spokes_per_frame = random.choices(self.spokes_range, self.spf_weights, k=1)[0]
            else:
                spokes_per_frame = self.spokes_per_frame
                print(f"training with fixed spokes per frame ({spokes_per_frame})")
            N_time = total_spokes // spokes_per_frame
            kspace_binned = kspace_flat.view(N_time, spokes_per_frame, self.N_coils, N_samples)
            kspace_slice = rearrange(kspace_binned, 't sp c sam -> t c sp sam')
        else:
            N_time = self.N_time
            N_samples = kspace_slice.shape[-1]
            spokes_per_frame = kspace_slice.shape[-2]
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        kspace_final = torch.flip(kspace_final, dims=[-1])
        csmap_tensor = torch.from_numpy(csmap)
        csmap_tensor = torch.rot90(csmap_tensor, k=2, dims=[-2, -1])
        csmap = csmap_tensor.numpy()
        return kspace_final, csmap, N_samples, spokes_per_frame, N_time
class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids, spokes_per_frame=36, num_frames=8):
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.model_type = model_type
        self.spokes_per_frame = spokes_per_frame
        self.num_frames = num_frames
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            print("loading grasp image from ", grasp_path)
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            print("setting grasp img to zero")
            grasp_recon_torch = 0
        kspace_path = os.path.join(sample_dir, f'simulated_kspace_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(kspace_path):
            kspace_complex = np.load(kspace_path, allow_pickle=True)
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            kspace_torch = kspace_path
        ground_truth_complex = dro['ground_truth_images']
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask
class SimulatedSPFDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.spokes_per_frame = 16
        self.num_frames = 18
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        print(f"  Testing {self.spokes_per_frame} spokes/frame with {self.num_frames} frames.")
        print("loading data from ", sample_dir)
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            grasp_recon_torch = 0
        ground_truth_complex = dro['ground_truth_images']
        smap_torch = rearrange(torch.tensor(csmaps), 'h w c -> c h w').unsqueeze(0)
        simImg_torch = torch.tensor(ground_truth_complex).to(torch.cfloat)
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        return smap_torch, simImg_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask

