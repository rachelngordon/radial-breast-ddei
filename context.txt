=== train_fpg.py ===
import argparse
import json
import os
import matplotlib.pyplot as plt
import torch
import yaml
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset
from deepinv.transform import Transform
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet_encoding import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex, GRASPRecon
from eval import eval_grasp, eval_sample
import csv
import math
import random
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
    with open(args.config, "r") as file:
        new_config = yaml.safe_load(file)
    epochs = new_config['training']["epochs"]
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
    epochs = config['training']["epochs"]
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
eval_dir = os.path.join(output_dir, "eval_results")
os.makedirs(eval_dir, exist_ok=True)
block_dir = os.path.join(output_dir, "block_outputs")
os.makedirs(block_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                   'lambda_S': config['model']['lambda_S'], 
                   'lambda_spatial_L': config['model']['lambda_spatial_L'],
                   'lambda_spatial_S': config['model']['lambda_spatial_S'],
                   'gamma': config['model']['gamma'],
                   'lambda_step': config['model']['lambda_step']}
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
start_epoch = 1
model_type = config["model"]["name"]
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils, N_time_eval = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"],
    config["data"]["eval_timeframes"]
)
Ng = config["data"]["fpg"] 
N_spokes = int(config["data"]["total_spokes"] / N_time)
N_full = config['data']['height'] * math.pi / 2
os.makedirs(os.path.join(output_dir, 'enhancement_curves'), exist_ok=True)
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
else:
    train_patient_ids = splits["train"]
val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]
if config['dataloader']['slice_range_start'] == "None" or config['dataloader']['slice_range_end'] == "None":
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=config["dataloader"]["slice_idx"],
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
    )
else:
    train_dataset = SliceDataset(
        root_dir=config["data"]["root_dir"],
        patient_ids=train_patient_ids,
        dataset_key=config["data"]["dataset_key"],
        file_pattern="*.h5",
        slice_idx=range(config['dataloader']['slice_range_start'], config['dataloader']['slice_range_end']),
        N_time=N_time,
        N_coils=N_coils,
        spf_aug=config['data']['spf_aug'],
    )
val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)
eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], lambdas=initial_lambdas, channels=config['model']['channels'])
model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
if args.from_checkpoint == True:
    checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
    model, optimizer, start_epoch, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
else:
    start_epoch = 1
if config['model']['losses']['mc_loss']['metric'] == "MSE":
    mc_loss_fn = MCLoss(model_type=model_type)
elif config['model']['losses']['mc_loss']['metric'] == "MAE":
    mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
else:
    raise(ValueError, "Unsupported MC Loss Metric.")
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
            ei_loss_fn = EILoss(subsample, model_type=model_type)
        else:
            ei_loss_fn = EILoss(subsample | (diffeo | rotate), model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp":
        ei_loss_fn = EILoss(monophasic_warp, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise, model_type=model_type)
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
        if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
            ei_loss_fn = EILoss(rotate, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
            ei_loss_fn = EILoss(diffeo, model_type=model_type)
        else:
            ei_loss_fn = EILoss(rotate | diffeo, model_type=model_type)
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
if args.from_checkpoint:
    train_mc_losses = train_curves["train_mc_losses"]
    val_mc_losses = val_curves["val_mc_losses"]
    train_ei_losses = train_curves["train_ei_losses"]
    val_ei_losses = val_curves["val_ei_losses"]
    train_adj_losses = train_curves["train_adj_losses"]
    val_adj_losses = val_curves["val_adj_losses"]
    weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
    weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
    weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
    eval_ssims = eval_curves["eval_ssims"]
    eval_psnrs = eval_curves["eval_psnrs"]
    eval_mses = eval_curves["eval_mses"]
    eval_lpipses = eval_curves["eval_lpipses"]
    eval_dc_mses = eval_curves["eval_dc_mses"]
    eval_dc_maes = eval_curves["eval_dc_maes"]
    eval_curve_corrs = eval_curves["eval_curve_corrs"]
else:
    train_mc_losses = []
    val_mc_losses = []
    train_ei_losses = []
    val_ei_losses = []
    train_adj_losses = []
    val_adj_losses = []
    weighted_train_mc_losses = []
    weighted_train_ei_losses = []
    weighted_train_adj_losses = []
    eval_ssims = []
    eval_lpipses = []
    eval_psnrs = []
    eval_mses = []
    eval_dc_mses = []
    eval_dc_maes = []
    eval_curve_corrs = []
grasp_ssims = []
grasp_psnrs = []
grasp_mses = []
grasp_lpipses = []
grasp_dc_mses = []
grasp_dc_maes = []
grasp_curve_corrs = []
lambda_Ls = []
lambda_Ss = []
lambda_spatial_Ls = []
lambda_spatial_Ss = []
gammas = []
lambda_steps = []
iteration_count = 0
if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    initial_train_adj_loss = 0.0
    initial_val_adj_loss = 0.0
    initial_eval_ssims = []
    initial_eval_psnrs = []
    initial_eval_mses = []
    initial_eval_lpipses = []
    initial_eval_dc_mses = []
    initial_eval_dc_maes = []
    initial_eval_curve_corrs = []
    with torch.no_grad():
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            if N_time > Ng:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, Ng)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
            else:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                measured_kspace.to(device), physics, csmap, acceleration_encoding, epoch="train0", norm=config['model']['norm']
            )
            initial_train_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap, acceleration_encoding
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
        train_adj_losses.append(step0_train_adj_loss)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        for measured_kspace, csmap, ground_truth, grasp_img, mask, *_ in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
            measured_kspace = measured_kspace.to(device)
            csmap = csmap.to(device)
            measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            N_spokes = eval_ktraj.shape[1] / config['data']['samples']
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            x_recon, adj_loss, *_ = model(
                measured_kspace.to(device), eval_physics, csmap, acceleration_encoding, epoch="val0", norm=config['model']['norm']
            )
            initial_val_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, eval_physics, model, csmap, acceleration_encoding
                )
                initial_val_ei_loss += ei_loss.item()
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device, eval_dir)
            grasp_ssims.append(ssim_grasp)
            grasp_psnrs.append(psnr_grasp)
            grasp_mses.append(mse_grasp)
            grasp_lpipses.append(lpips_grasp)
            grasp_dc_mses.append(dc_mse_grasp)
            grasp_dc_maes.append(dc_mae_grasp)
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(measured_kspace, csmap, ground_truth, x_recon, eval_physics, mask, grasp_recon, acceleration, eval_dir, label=None, device=device)
            initial_eval_ssims.append(ssim)
            initial_eval_psnrs.append(psnr)
            initial_eval_mses.append(mse)
            initial_eval_lpipses.append(lpips)
            initial_eval_dc_mses.append(dc_mse)
            initial_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                initial_eval_curve_corrs.append(recon_corr)
                grasp_curve_corrs.append(grasp_corr)
        step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
        val_ei_losses.append(step0_val_ei_loss)
        step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
        val_adj_losses.append(step0_val_adj_loss)
        initial_eval_ssim = np.mean(initial_eval_ssims)
        initial_eval_psnr = np.mean(initial_eval_psnrs)
        initial_eval_mse = np.mean(initial_eval_mses)
        initial_eval_lpips = np.mean(initial_eval_lpipses)
        initial_eval_dc_mse = np.mean(initial_eval_dc_mses)
        initial_eval_dc_mae = np.mean(initial_eval_dc_maes)
        initial_eval_curve_corr = np.mean(initial_eval_curve_corrs)
        eval_ssims.append(initial_eval_ssim)
        eval_psnrs.append(initial_eval_psnr)
        eval_mses.append(initial_eval_mse)
        eval_lpipses.append(initial_eval_lpips)
        eval_dc_mses.append(initial_eval_dc_mse) 
        eval_dc_maes.append(initial_eval_dc_mae) 
        eval_curve_corrs.append(initial_eval_curve_corr)
    print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
    print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
if (epochs + 1) == start_epoch:
    raise(ValueError("Full training epochs already complete."))
else: 
    for epoch in range(start_epoch, epochs + 1):
        model.train()
        running_mc_loss = 0.0
        running_ei_loss = 0.0
        running_adj_loss = 0.0
        epoch_eval_ssims = []
        epoch_eval_psnrs = []
        epoch_eval_mses = []
        epoch_eval_lpipses = []
        epoch_eval_dc_mses = []
        epoch_eval_dc_maes = []
        epoch_eval_curve_corrs = []
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        if use_ei_loss:
            if epoch < warmup + 1:
                target_w_ei = 0.0
            elif epoch == warmup + 1:
                mc_loss_at_transition = epoch_train_mc_loss
                print(f"Transitioning at Epoch {epoch}. MC Loss: {mc_loss_at_transition:.4e}")
                if step0_train_ei_loss > 0:
                    target_w_ei = mc_loss_at_transition / step0_train_ei_loss
                else:
                    target_w_ei = 0.0 # Prevent division by zero
                print(f"Dynamically calculated target EI weight: {target_w_ei:.4f}")
        for measured_kspace, csmap, grasp_img, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            measured_kspace = to_torch_complex(measured_kspace).squeeze()
            measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
            if N_time > Ng:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, Ng)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                max_idx = N_time - Ng
                random_index = random.randint(0, max_idx - 1) 
                measured_kspace = measured_kspace[..., random_index:random_index + Ng]
            else:
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
            optimizer.zero_grad()
            csmap = csmap.to(device).to(measured_kspace.dtype)
            acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
            print("AF: ", acceleration)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                measured_kspace.to(device), physics, csmap, acceleration_encoding, epoch=f"train{epoch}", norm=config['model']['norm']
            )
            running_adj_loss += adj_loss.item()
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
            running_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model, csmap, acceleration_encoding
                )
                ei_loss_weight = get_cosine_ei_weight(
                    current_epoch=epoch,
                    warmup_epochs=warmup,
                    schedule_duration=duration,
                    target_weight=target_w_ei
                )
                running_ei_loss += ei_loss.item()
                total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(
                    mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                )
            else:
                total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
            if torch.isnan(total_loss):
                print(
                    "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                )
                raise RuntimeError("total_loss is NaN")
            total_loss.backward()
            if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                log_gradient_stats(
                    model=model,
                    epoch=epoch,
                    iteration=iteration_count,
                    output_dir=output_dir,
                    log_filename="gradient_stats.csv"
                )
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        epoch_train_adj_loss = running_adj_loss / len(train_loader)
        train_adj_losses.append(epoch_train_adj_loss)
        weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
        lambda_Ls.append(lambda_L.item())
        lambda_Ss.append(lambda_S.item())
        lambda_spatial_Ls.append(lambda_spatial_L.item())
        lambda_spatial_Ss.append(lambda_spatial_S.item())
        gammas.append(gamma.item())
        lambda_steps.append(lambda_step.item())
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_running_adj_loss = 0.0
        val_loader_tqdm = tqdm(
            val_dro_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask in tqdm(val_dro_loader):
                val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                val_grasp_img_tensor = val_grasp_img.to(device)
                N_spokes = eval_ktraj.shape[1] / config['data']['samples']
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                val_x_recon, val_adj_loss, *_ = model(
                    val_kspace_batch.to(device), eval_physics, val_csmap, acceleration_encoding, epoch=f"val{epoch}", norm=config['model']['norm']
                )
                val_running_adj_loss += val_adj_loss.item()
                val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                val_running_mc_loss += val_mc_loss.item()
                if use_ei_loss:
                    val_ei_loss, val_t_img = ei_loss_fn(
                        val_x_recon, eval_physics, model, val_csmap, acceleration_encoding
                    )
                    val_running_ei_loss += val_ei_loss.item()
                    val_loader_tqdm.set_postfix(
                        val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                    )
                else:
                    val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, _ = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, acceleration, eval_dir, f'epoch{epoch}', device)
                epoch_eval_ssims.append(ssim)
                epoch_eval_psnrs.append(psnr)
                epoch_eval_mses.append(mse)
                epoch_eval_lpipses.append(lpips)
                epoch_eval_dc_mses.append(dc_mse)
                epoch_eval_dc_maes.append(dc_mae)
                if recon_corr is not None:
                    epoch_eval_curve_corrs.append(recon_corr)
        epoch_eval_ssim = np.mean(epoch_eval_ssims)
        epoch_eval_psnr = np.mean(epoch_eval_psnrs)
        epoch_eval_mse = np.mean(epoch_eval_mses)
        epoch_eval_lpips = np.mean(epoch_eval_lpipses)
        epoch_eval_dc_mse = np.mean(epoch_eval_dc_mses)
        epoch_eval_dc_mae = np.mean(epoch_eval_dc_maes)
        epoch_eval_curve_corr = np.mean(epoch_eval_curve_corrs)
        eval_ssims.append(epoch_eval_ssim)
        eval_psnrs.append(epoch_eval_psnr)
        eval_mses.append(epoch_eval_mse)
        eval_lpipses.append(epoch_eval_lpips)
        eval_dc_mses.append(epoch_eval_dc_mse) 
        eval_dc_maes.append(epoch_eval_dc_mae)    
        eval_curve_corrs.append(epoch_eval_curve_corr)  
        if epoch % save_interval == 0:
            plot_reconstruction_sample(
                val_x_recon,
                f"Validation Sample - Epoch {epoch}",
                f"val_sample_epoch_{epoch}",
                output_dir,
                val_grasp_img
            )
            val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
            plot_enhancement_curve(
                val_x_recon_reshaped,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
            plot_enhancement_curve(
                val_grasp_img,
                output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
            if use_ei_loss:
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch}",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
        epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if model_type == "LSFPNet":
            epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            val_adj_losses.append(epoch_val_adj_loss)
        else:
            val_adj_losses.append(0.0)
        if epoch % save_interval == 0:
            train_curves = dict(
                train_mc_losses=train_mc_losses,
                train_ei_losses=train_ei_losses,
                weighted_train_mc_losses=weighted_train_mc_losses,
                weighted_train_ei_losses=weighted_train_ei_losses,
            )
            val_curves = dict(
                val_mc_losses=val_mc_losses,
                val_ei_losses=val_ei_losses,
            )
            eval_curves = dict(
                eval_ssims=eval_ssims,
                eval_psnrs=eval_psnrs,
                eval_mses=eval_mses,
                eval_lpipses=eval_lpipses,
                eval_dc_mses=eval_dc_mses,
                eval_dc_maes=eval_dc_maes,
                eval_curve_corrs=eval_curve_corrs
            )
            model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
            save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, model_save_path)
            print(f'Model saved to {model_save_path}')
            plt.figure()
            plt.plot(train_mc_losses, label="Training MC Loss")
            plt.plot(val_mc_losses, label="Validation MC Loss")
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(train_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Training Measurement Consistency Loss")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "train_mc_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(val_mc_losses)
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Validation Measurement Consistency Loss")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "val_mc_losses.png"))
            plt.close()
            if use_ei_loss:
                plt.figure()
                plt.plot(train_ei_losses, label="Training EI Loss")
                plt.plot(val_ei_losses, label="Validation EI Loss")
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(train_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Training Equivariant Imaging Loss")
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "train_ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(val_ei_losses)
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Validation Equivariant Imaging Loss")
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "val_ei_losses.png"))
                plt.close()
            plt.figure()
            plt.plot(train_adj_losses, label="Training Adjoint Loss")
            plt.plot(val_adj_losses, label="Validation Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Adjoint Loss")
            plt.title("CNN Adjoint Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "adj_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(train_adj_losses)
            plt.xlabel("Epoch")
            plt.ylabel("Adjoint Loss")
            plt.title("Training CNN Adjoint Loss")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "train_adj_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(val_adj_losses)
            plt.xlabel("Epoch")
            plt.ylabel("Adjoint Loss")
            plt.title("Validation CNN Adjoint Loss")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "val_adj_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(lambda_Ls)
            plt.xlabel("Epoch")
            plt.ylabel("Lambda_L Value")
            plt.title("Lambda_L During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "lambda_L.png"))
            plt.close()
            plt.figure()
            plt.plot(lambda_Ss)
            plt.xlabel("Epoch")
            plt.ylabel("Lambda_S Value")
            plt.title("Lambda_S During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "lambda_S.png"))
            plt.close()
            plt.figure()
            plt.plot(lambda_spatial_Ls)
            plt.xlabel("Epoch")
            plt.ylabel("Lambda_spatial_L Value")
            plt.title("Lambda_spatial_L During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "lambda_spatial_L.png"))
            plt.close()
            plt.figure()
            plt.plot(lambda_spatial_Ss)
            plt.xlabel("Epoch")
            plt.ylabel("Lambda_spatial_S Value")
            plt.title("Lambda_spatial_S During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "lambda_spatial_S.png"))
            plt.close()
            plt.figure()
            plt.plot(gammas)
            plt.xlabel("Epoch")
            plt.ylabel("Gamma Value")
            plt.title("Step Size During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "gamma.png"))
            plt.close()
            plt.figure()
            plt.plot(lambda_steps)
            plt.xlabel("Epoch")
            plt.ylabel("Lambda_step Value")
            plt.title("Lambda_step During Training")
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "lambda_step.png"))
            plt.close()
            plt.figure()
            plt.plot(weighted_train_mc_losses, label="MC Loss")
            plt.plot(weighted_train_ei_losses, label="EI Loss")
            plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("Weighted Training Losses")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_ssims)
            plt.xlabel("Epoch")
            plt.ylabel("SSIM")
            plt.title("Evaluation SSIM")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_ssims.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_psnrs)
            plt.xlabel("Epoch")
            plt.ylabel("PSNR")
            plt.title("Evaluation PSNR")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_psnrs.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_mses)
            plt.xlabel("Epoch")
            plt.ylabel("MSE")
            plt.title("Evaluation MSE")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_mses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_lpipses)
            plt.xlabel("Epoch")
            plt.ylabel("LPIPS")
            plt.title("Evaluation LPIPS")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_lpipses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dc_mses)
            plt.xlabel("Epoch")
            plt.ylabel("k-space MSE")
            plt.title("Evaluation Data Consistency (MSE)")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dc_mses.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_dc_maes)
            plt.xlabel("Epoch")
            plt.ylabel("k-space MAE")
            plt.title("Evaluation Data Consistency (MAE)")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_dc_maes.png"))
            plt.close()
            plt.figure()
            plt.plot(eval_curve_corrs)
            plt.xlabel("Epoch")
            plt.ylabel("Pearson Correlation of Tumor Enhancement Curve")
            plt.title("Pearson Correlation")
            plt.grid(True)
            plt.savefig(os.path.join(eval_dir, "eval_curve_correlations.png"))
            plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
        if model_type == "LSFPNet":
            print(
                f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
            )
        print(f"--- Evaluation Metrics: Epoch {epoch} ---")
        print(f"Recon SSIM: {epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}")
        print(f"Recon PSNR: {epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}")
        print(f"Recon MSE: {epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}")
        print(f"Recon LPIPS: {epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}")
        print(f"Recon DC MSE: {epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}")
        print(f"Recon DC MAE: {epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}")
        print(f"Recon Enhancement Curve Correlation: {epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}")
        print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}")
        print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}")
        print(f"GRASP MSE: {np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}")
        print(f"GRASP LPIPS: {np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}")
        print(f"GRASP DC MSE: {np.mean(grasp_dc_mses):.6f} ± {np.std(grasp_dc_mses):.4f}")
        print(f"GRASP DC MAE: {np.mean(grasp_dc_maes):.6f} ± {np.std(grasp_dc_maes):.4f}")
        print(f"GRASP Enhancement Curve Correlation: {np.mean(grasp_curve_corrs):.6f} ± {np.std(grasp_curve_corrs):.4f}")
train_curves = dict(
    train_mc_losses=train_mc_losses,
    train_ei_losses=train_ei_losses,
    train_adj_losses=train_adj_losses,
    weighted_train_mc_losses=weighted_train_mc_losses,
    weighted_train_ei_losses=weighted_train_ei_losses,
    weighted_train_adj_losses=weighted_train_adj_losses,
)
val_curves = dict(
    val_mc_losses=val_mc_losses,
    val_ei_losses=val_ei_losses,
    val_adj_losses=val_adj_losses,
)
eval_curves = dict(
    eval_ssims=eval_ssims,
    eval_psnrs=eval_psnrs,
    eval_mses=eval_mses,
    eval_lpipses=eval_lpipses,
    eval_dc_mses=eval_dc_mses,
    eval_dc_maes=eval_dc_maes,
    eval_curve_corrs=eval_curve_corrs,
)
model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, model_save_path)
print(f'Model saved to {model_save_path}')
metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
with open(metrics_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
    writer.writerow(['DL', 
                     f'{epoch_eval_ssim:.4f} ± {np.std(epoch_eval_ssims):.4f}', 
                     f'{epoch_eval_psnr:.4f} ± {np.std(epoch_eval_psnrs):.4f}', 
                     f'{epoch_eval_mse:.4f} ± {np.std(epoch_eval_mses):.4f}',
                     f'{epoch_eval_lpips:.4f} ± {np.std(epoch_eval_lpipses):.4f}',  
                     f'{epoch_eval_dc_mse:.4f} ± {np.std(epoch_eval_dc_mses):.4f}', 
                     f'{epoch_eval_dc_mae:.4f} ± {np.std(epoch_eval_dc_maes):.4f}', 
                     f'{epoch_eval_curve_corr:.4f} ± {np.std(epoch_eval_curve_corrs):.4f}'])
    writer.writerow(['GRASP', 
                     f'{np.mean(grasp_ssims):.4f} ± {np.std(grasp_ssims):.4f}', 
                     f'{np.mean(grasp_psnrs):.4f} ± {np.std(grasp_psnrs):.4f}', 
                     f'{np.mean(grasp_mses):.4f} ± {np.std(grasp_mses):.4f}', 
                     f'{np.mean(grasp_lpipses):.4f} ± {np.std(grasp_lpipses):.4f}', 
                     f'{np.mean(grasp_dc_mses):.4f} ± {np.std(grasp_dc_mses):.4f}', 
                     f'{np.mean(grasp_dc_maes):.4f} ± {np.std(grasp_dc_maes):.4f}', 
                     f'{np.mean(grasp_curve_corrs):.4f} ± {np.std(grasp_curve_corrs):.4f}'])
MAIN_EVALUATION_PLAN = [
    {
        "spokes_per_frame": 16,
        "num_frames": 20, # 16 * 20 = 320 total spokes
        "slice": slice(1, 21), # Wide window
        "description": "High temporal resolution"
    },
    {
        "spokes_per_frame": 20,
        "num_frames": 16, # 20 * 16 = 320 total spokes
        "slice": slice(3, 19), # Medium-wide window
        "description": "Good temporal resolution"
    },
    {
        "spokes_per_frame": 32,
        "num_frames": 10, # 32 * 10 = 320 total spokes
        "slice": slice(5, 15), # Narrow window centered on enhancement
        "description": "Standard temporal resolution"
    },
    {
        "spokes_per_frame": 40,
        "num_frames": 8,  # 40 * 8 = 320 total spokes
        "slice": slice(5, 13), # Very narrow window on peak
        "description": "Low temporal resolution"
    }
]
STRESS_TEST_PLAN = [
    {
        "spokes_per_frame": 2,
        "num_frames": 22, # 4 * 22 = 88 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 2 spokes"
    },
    {
        "spokes_per_frame": 4,
        "num_frames": 22, # 4 * 22 = 88 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 4 spokes"
    },
    {
        "spokes_per_frame": 8,
        "num_frames": 22, # 8 * 22 = 176 total spokes
        "slice": slice(0, 22), # The entire 22-frame duration
        "description": "Stress test: max temporal points, 8 spokes"
    },
]
eval_spf_dataset = SimulatedSPFDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    )
eval_spf_loader = DataLoader(
    eval_spf_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
with torch.no_grad():
    spf_recon_ssim = {}
    spf_recon_psnr = {}
    spf_recon_mse = {}
    spf_recon_lpips = {}
    spf_recon_dc_mse = {}
    spf_recon_dc_mae = {}
    spf_recon_corr = {}
    spf_grasp_ssim = {}
    spf_grasp_psnr = {}
    spf_grasp_mse = {}
    spf_grasp_lpips = {}
    spf_grasp_dc_mse = {}
    spf_grasp_dc_mae = {}
    spf_grasp_corr = {}
    print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
    for eval_config in STRESS_TEST_PLAN:
        stress_test_ssims = []
        stress_test_psnrs = []
        stress_test_mses = []
        stress_test_lpipses = []
        stress_test_dc_mses = []
        stress_test_dc_maes = []
        stress_test_corrs = []
        stress_test_grasp_ssims = []
        stress_test_grasp_psnrs = []
        stress_test_grasp_mses = []
        stress_test_grasp_lpipses = []
        stress_test_grasp_dc_mses = []
        stress_test_grasp_dc_maes = []
        stress_test_grasp_corrs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            x_recon, *_ = model(
                kspace.to(device), physics, csmap, acceleration_encoding, epoch=None, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, eval_dir, f"{spokes}spf", device)
            stress_test_ssims.append(ssim)
            stress_test_psnrs.append(psnr)
            stress_test_mses.append(mse)
            stress_test_lpipses.append(lpips)
            stress_test_dc_mses.append(dc_mse)
            stress_test_dc_maes.append(dc_mae)
            if recon_corr is not None:
                stress_test_corrs.append(recon_corr)
                stress_test_grasp_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            stress_test_grasp_ssims.append(ssim_grasp)
            stress_test_grasp_psnrs.append(psnr_grasp)
            stress_test_grasp_mses.append(mse_grasp)
            stress_test_grasp_lpipses.append(lpips_grasp)
            stress_test_grasp_dc_mses.append(dc_mse_grasp)
            stress_test_grasp_dc_maes.append(dc_mae_grasp)
            spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
            spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
            spf_recon_mse[spokes] = np.mean(stress_test_mses)
            spf_recon_lpips[spokes] = np.mean(stress_test_lpipses)
            spf_recon_dc_mse[spokes] = np.mean(stress_test_dc_mses)
            spf_recon_dc_mae[spokes] = np.mean(stress_test_dc_maes)
            spf_recon_corr[spokes] = np.mean(stress_test_corrs)
            spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
            spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
            spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
            spf_grasp_lpips[spokes] = np.mean(stress_test_grasp_lpipses)
            spf_grasp_dc_mse[spokes] = np.mean(stress_test_grasp_dc_mses)
            spf_grasp_dc_mae[spokes] = np.mean(stress_test_grasp_dc_maes)
            spf_grasp_corr[spokes] = np.mean(stress_test_grasp_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', "LPIPS", 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(stress_test_ssims):.4f} ± {np.std(stress_test_ssims):.4f}', 
            f'{np.mean(stress_test_psnrs):.4f} ± {np.std(stress_test_psnrs):.4f}', 
            f'{np.mean(stress_test_mses):.4f} ± {np.std(stress_test_mses):.4f}', 
            f'{np.mean(stress_test_lpipses):.4f} ± {np.std(stress_test_lpipses):.4f}', 
            f'{np.mean(stress_test_dc_mses):.4f} ± {np.std(stress_test_dc_mses):.4f}',
            f'{np.mean(stress_test_dc_maes):.4f} ± {np.std(stress_test_dc_maes):.4f}',
            f'{np.mean(stress_test_corrs):.4f} ± {np.std(stress_test_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(stress_test_grasp_ssims):.4f} ± {np.std(stress_test_grasp_ssims):.4f}', 
            f'{np.mean(stress_test_grasp_psnrs):.4f} ± {np.std(stress_test_grasp_psnrs):.4f}', 
            f'{np.mean(stress_test_grasp_mses):.4f} ± {np.std(stress_test_grasp_mses):.4f}', 
            f'{np.mean(stress_test_grasp_lpipses):.4f} ± {np.std(stress_test_grasp_lpipses):.4f}', 
            f'{np.mean(stress_test_grasp_dc_mses):.4f} ± {np.std(stress_test_grasp_dc_mses):.4f}',
            f'{np.mean(stress_test_grasp_dc_maes):.4f} ± {np.std(stress_test_grasp_dc_maes):.4f}',
            f'{np.mean(stress_test_grasp_corrs):.4f} ± {np.std(stress_test_grasp_corrs):.4f}',
            ])
    print("--- Running Main Evaluation (Budget: 320 spokes) ---")
    for eval_config in MAIN_EVALUATION_PLAN:
        spf_eval_ssims = []
        spf_eval_psnrs = []
        spf_eval_mses = []
        spf_eval_lpipses = []
        spf_eval_dc_mses = []
        spf_eval_dc_maes = []
        spf_eval_curve_corrs = []
        spf_grasp_ssims = []
        spf_grasp_psnrs = []
        spf_grasp_mses = []
        spf_grasp_lpipses = []
        spf_grasp_dc_mses = []
        spf_grasp_dc_maes = []
        spf_grasp_curve_corrs = []
        spokes = eval_config["spokes_per_frame"]
        time_slice = eval_config["slice"]
        num_frames = eval_config["num_frames"]
        eval_spf_dataset.spokes_per_frame = spokes
        eval_spf_dataset.window = time_slice
        eval_spf_dataset.num_frames = num_frames
        for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
            csmap = csmap.squeeze(0).to(device)   # Remove batch dim
            ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
            ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
            physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
            sim_kspace = physics(False, ground_truth, csmap)
            kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
            acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
            if config['model']['encode_acceleration']:
                acceleration_encoding = acceleration
            else: 
                acceleration_encoding = None
            if type(grasp_img) is int or len(grasp_img.shape) == 1:
                print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
            grasp_img = grasp_img.to(device)
            x_recon, *_ = model(
                kspace.to(device), physics, csmap, acceleration_encoding, epoch=None, norm=config['model']['norm']
            )
            ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
            ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, eval_dir, f'{spokes}spf', device)
            spf_eval_ssims.append(ssim)
            spf_eval_psnrs.append(psnr)
            spf_eval_mses.append(mse)
            spf_eval_lpipses.append(lpips)
            spf_eval_dc_mses.append(dc_mse)
            spf_eval_dc_maes.append(dc_mae)
            if recon_corr is not None:
                spf_eval_curve_corrs.append(recon_corr)
                spf_grasp_curve_corrs.append(grasp_corr)
            ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
            spf_grasp_ssims.append(ssim_grasp)
            spf_grasp_psnrs.append(psnr_grasp)
            spf_grasp_mses.append(mse_grasp)
            spf_grasp_lpipses.append(lpips_grasp)
            spf_grasp_dc_mses.append(dc_mse_grasp)
            spf_grasp_dc_maes.append(dc_mae_grasp)
        spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
        spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
        spf_recon_mse[spokes] = np.mean(spf_eval_mses)
        spf_recon_lpips[spokes] = np.mean(spf_eval_lpipses)
        spf_recon_dc_mse[spokes] = np.mean(spf_eval_dc_mses)
        spf_recon_dc_mae[spokes] = np.mean(spf_eval_dc_maes)
        spf_recon_corr[spokes] = np.mean(spf_eval_curve_corrs)
        spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
        spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
        spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
        spf_grasp_lpips[spokes] = np.mean(spf_grasp_lpipses)
        spf_grasp_dc_mse[spokes] = np.mean(spf_grasp_dc_mses)
        spf_grasp_dc_mae[spokes] = np.mean(spf_grasp_dc_maes)
        spf_grasp_corr[spokes] = np.mean(spf_grasp_curve_corrs)
        spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(spf_metrics_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', spokes, 
            f'{np.mean(spf_eval_ssims):.4f} ± {np.std(spf_eval_ssims):.4f}', 
            f'{np.mean(spf_eval_psnrs):.4f} ± {np.std(spf_eval_psnrs):.4f}', 
            f'{np.mean(spf_eval_mses):.4f} ± {np.std(spf_eval_mses):.4f}', 
            f'{np.mean(spf_eval_lpipses):.4f} ± {np.std(spf_eval_lpipses):.4f}', 
            f'{np.mean(spf_eval_dc_mses):.4f} ± {np.std(spf_eval_dc_mses):.4f}',
            f'{np.mean(spf_eval_dc_maes):.4f} ± {np.std(spf_eval_dc_maes):.4f}',
            f'{np.mean(spf_eval_curve_corrs):.4f} ± {np.std(spf_eval_curve_corrs):.4f}'
            ])
            writer.writerow(['GRASP', spokes, 
            f'{np.mean(spf_grasp_ssims):.4f} ± {np.std(spf_grasp_ssims):.4f}', 
            f'{np.mean(spf_grasp_psnrs):.4f} ± {np.std(spf_grasp_psnrs):.4f}', 
            f'{np.mean(spf_grasp_mses):.4f} ± {np.std(spf_grasp_mses):.4f}', 
            f'{np.mean(spf_grasp_lpipses):.4f} ± {np.std(spf_grasp_lpipses):.4f}', 
            f'{np.mean(spf_grasp_dc_mses):.4f} ± {np.std(spf_grasp_dc_mses):.4f}',
            f'{np.mean(spf_grasp_dc_maes):.4f} ± {np.std(spf_grasp_dc_maes):.4f}',
            f'{np.mean(spf_grasp_curve_corrs):.4f} ± {np.std(spf_grasp_curve_corrs):.4f}'
            ])
plt.figure()
plt.plot(list(spf_recon_ssim.keys()), list(spf_recon_ssim.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_ssim.keys()), list(spf_grasp_ssim.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("SSIM")
plt.title("Evaluation SSIM vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_ssim.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_psnr.keys()), list(spf_recon_psnr.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_psnr.keys()), list(spf_grasp_psnr.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("PSNR")
plt.title("Evaluation PSNR vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_psnr.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_mse.keys()), list(spf_recon_mse.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_mse.keys()), list(spf_grasp_mse.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("MSE")
plt.title("Evaluation Image MSE vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_mse.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_lpips.keys()), list(spf_recon_lpips.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_lpips.keys()), list(spf_grasp_lpips.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("LPIPS")
plt.title("Evaluation LPIPS vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_lpips.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_dc_mse.keys()), list(spf_recon_dc_mse.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_dc_mse.keys()), list(spf_grasp_dc_mse.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("k-space MSE")
plt.title("Data Consistency Evaluation (MSE) vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_dc_mse.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_dc_mae.keys()), list(spf_recon_dc_mae.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_dc_mae.keys()), list(spf_grasp_dc_mae.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("k-space MAE")
plt.title("Data Consistency Evaluation (MAE) vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_dc_mae.png"))
plt.close()
plt.figure()
plt.plot(list(spf_recon_corr.keys()), list(spf_recon_corr.values()), label="DL Recon", marker='o')
plt.plot(list(spf_grasp_corr.keys()), list(spf_grasp_corr.values()), label="GRASP Recon", marker='o')
plt.xlabel("Spokes per Frame")
plt.ylabel("Pearson Correlation")
plt.title("Pearson Correlation of Tumor Enhancement Curve vs Spokes per Frame")
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(eval_dir, "spf_eval_curve_correlations.png"))
plt.close()

=== radial_lsfp.py ===
import torch
import torch.nn as nn
import numpy as np
from time import time
dtype = torch.complex64
class MCNUFFT(nn.Module):
    def __init__(self, nufft_ob, adjnufft_ob, ktraj, dcomp):
        super(MCNUFFT, self).__init__()
        self.nufft_ob = nufft_ob
        self.adjnufft_ob = adjnufft_ob
        self.ktraj = torch.squeeze(ktraj)
        self.dcomp = torch.squeeze(dcomp)
    def forward(self, inv, data, smaps):
        data = torch.squeeze(data)  # delete redundant dimension
        Nx = smaps.shape[2]
        Ny = smaps.shape[3]
        if inv:  # adjoint nufft
            smaps = smaps.to(dtype)
            if len(data.shape) > 2:  # multi-frame
                x = torch.zeros([Nx, Ny, data.shape[2]], dtype=dtype)
                for ii in range(0, data.shape[2]):
                    kd = data[:, :, ii]
                    k = self.ktraj[:, :, ii]
                    d = self.dcomp[:, ii]
                    kd = kd.unsqueeze(0)
                    d = d.unsqueeze(0).unsqueeze(0)
                    tt1 = time()
                    x_temp = self.adjnufft_ob(kd * d, k, smaps=smaps)
                    x[:, :, ii] = torch.squeeze(x_temp) / np.sqrt(Nx * Ny)
                    tt2 = time()
            else:  # single frame
                kd = data.unsqueeze(0)
                d = self.dcomp.unsqueeze(0).unsqueeze(0)
                x = self.adjnufft_ob(kd * d, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        else:  # forward nufft
            if len(data.shape) > 2:  # multi-frame
                x = torch.zeros([smaps.shape[1], self.ktraj.shape[1], data.shape[-1]], dtype=dtype)
                for ii in range(0, data.shape[-1]):
                    image = data[:, :, ii]
                    k = self.ktraj[:, :, ii]
                    image = image.unsqueeze(0).unsqueeze(0)
                    x_temp = self.nufft_ob(image, k, smaps=smaps)
                    x[:, :, ii] = torch.squeeze(x_temp) / np.sqrt(Nx * Ny)
            else:  # single frame
                image = data.unsqueeze(0).unsqueeze(0)
                x = self.nufft_ob(image, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        return x

=== lsfpnet_encoding.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
import matplotlib.pyplot as plt
import numpy as np
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class MappingNetwork(nn.Module):
    """Maps a scalar input to a style vector using a simple MLP."""
    def __init__(self, style_dim, num_layers=4):
        super().__init__()
        layers = [nn.Linear(1, style_dim), nn.ReLU(True)]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(style_dim, style_dim), nn.ReLU(True)])
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        if x.dim() == 0:
            x = x.unsqueeze(0)
        if x.dim() == 1:
            x = x.unsqueeze(1)
        return self.net(x)
class BasicBlock(nn.Module):
    def __init__(self, lambdas, channels=32, style_dim=128):
        super(BasicBlock, self).__init__()
        self.channels = channels
        self.style_dim = style_dim
        self.lambda_L = nn.Parameter(torch.tensor([lambdas['lambda_L']]))
        self.lambda_S = nn.Parameter(torch.tensor([lambdas['lambda_S']]))
        self.lambda_spatial_L = nn.Parameter(torch.tensor([lambdas['lambda_spatial_L']]))
        self.lambda_spatial_S = nn.Parameter(torch.tensor([lambdas['lambda_spatial_S']]))
        self.gamma = nn.Parameter(torch.tensor([lambdas['gamma']]))
        self.lambda_step = nn.Parameter(torch.tensor([lambdas['lambda_step']]))
        self.style_injector_L = nn.Linear(self.style_dim, self.channels * 2) # *2 for scale and bias
        self.style_injector_S = nn.Linear(self.style_dim, self.channels * 2)
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmaps, style_embedding=None):
        c = self.lambda_step / self.gamma
        nx, ny, nt = M0.size()
        temp_data = torch.reshape(L + S, [nx, ny, nt])
        temp_data = param_E(inv=False, data=temp_data, smaps=csmaps).to(param_d.device)
        gradient = param_E(inv=True, data=temp_data - param_d, smaps=csmaps)
        gradient = torch.reshape(gradient, [nx * ny, nt]).to(param_d.device)
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(torch.squeeze(pb_L), [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        svd_input_complex = c * y_L + pt_L
        svd_input_mag = svd_input_complex.abs() + 1e-8
        original_phase = svd_input_complex / svd_input_mag
        noise_std = 1e-3 # A small standard deviation for the noise
        noise = torch.randn_like(svd_input_mag) * noise_std
        stable_svd_input = svd_input_mag + noise #epsilon
        A = stable_svd_input
        alpha = 1e-6 # This is a hyperparameter you can tune
        m, n = A.shape
        device = A.device
        dtype = A.dtype
        identity_aug = torch.sqrt(torch.tensor(alpha)) * torch.eye(n, device=device, dtype=dtype)
        A_aug = torch.cat([A, identity_aug], dim=0)
        U_aug, S, Vh = torch.linalg.svd(A_aug, full_matrices=False)
        Ut = U_aug[:m, :]
        Vt = Vh
        St = S #torch.diag(S) # or just use the vector S depending on your needs
        St_shrunk = Project_inf(St, self.lambda_L, to_complex=False)
        pt_L_mag = Ut @ torch.diag_embed(St_shrunk) @ Vt
        pt_L = pt_L_mag * original_phase
        temp_y_L_input = torch.cat((torch.real(y_L), torch.imag(y_L)), 0).to(torch.float32)
        temp_y_L_input = torch.reshape(temp_y_L_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_L = F.conv3d(temp_y_L_input, self.conv1_forward_l, padding=1)
        temp_y_L = F.relu(temp_y_L)
        temp_y_L = F.conv3d(temp_y_L, self.conv2_forward_l, padding=1)
        if style_embedding is not None:
            print("encoding acceleration...")
            style_params_L = self.style_injector_L(style_embedding)
            scale_L, bias_L = style_params_L.chunk(2, dim=-1) # Split into [1, channels] each
            scale_L = scale_L.view(1, self.channels, 1, 1, 1)
            bias_L = bias_L.view(1, self.channels, 1, 1, 1)
            temp_y_L = F.relu(temp_y_L * (scale_L + 1) + bias_L)
        else: 
            temp_y_L = F.relu(temp_y_L)
        temp_y_L_output = F.conv3d(temp_y_L, self.conv3_forward_l, padding=1)
        temp_y_L = temp_y_L_output + p_L
        temp_y_L = temp_y_L[0, :, :, :, :] + 1j * temp_y_L[1, :, :, :, :]
        p_L = Project_inf(c * temp_y_L, self.lambda_spatial_L)
        p_L = torch.cat((torch.real(p_L), torch.imag(p_L)), 0).to(torch.float32)
        p_L = torch.reshape(p_L, [2, self.channels, nx, ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1)
        pb_L = F.relu(pb_L)
        pb_L_output = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = torch.reshape(pb_L_output, [2, nx * ny, nt])
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - self.gamma * gradient - self.gamma * pt_L - self.gamma * pb_L
        adjloss_L = temp_y_L_output * p_L - pb_L_output * temp_y_L_input
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, self.lambda_S)
        temp_y_S_input = torch.cat((torch.real(y_S), torch.imag(y_S)), 0).to(torch.float32)
        temp_y_S_input = torch.reshape(temp_y_S_input, [2, nx, ny, nt]).unsqueeze(1)
        temp_y_S = F.conv3d(temp_y_S_input, self.conv1_forward_s, padding=1)
        temp_y_S = F.relu(temp_y_S)
        temp_y_S = F.conv3d(temp_y_S, self.conv2_forward_s, padding=1)
        if style_embedding is not None:
            print("encoding acceleration...")
            style_params_S = self.style_injector_S(style_embedding)
            scale_S, bias_S = style_params_S.chunk(2, dim=-1)
            scale_S = scale_S.view(1, self.channels, 1, 1, 1)
            bias_S = bias_S.view(1, self.channels, 1, 1, 1)
            temp_y_S = F.relu(temp_y_S * (scale_S + 1) + bias_S)
        else:
            temp_y_S = F.relu(temp_y_S)
        temp_y_S_output = F.conv3d(temp_y_S, self.conv3_forward_s, padding=1)
        temp_y_Sp = temp_y_S_output + p_S
        temp_y_Sp = temp_y_Sp[0, :, :, :, :] + 1j * temp_y_Sp[1, :, :, :, :]
        p_S = Project_inf(c * temp_y_Sp, self.lambda_spatial_S)
        p_S = torch.cat((torch.real(p_S), torch.imag(p_S)), 0).to(torch.float32)
        p_S = torch.reshape(p_S, [2, self.channels, nx, ny, nt])
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1)
        pb_S = F.relu(pb_S)
        pb_S_output = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = torch.reshape(pb_S_output, [2, nx * ny, nt])
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - self.gamma * gradient - self.gamma * Wtxs(pt_S) - self.gamma * pb_S
        adjloss_S = temp_y_S_output * p_S - pb_S_output * temp_y_S_input
        return [L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S, self.lambda_L, self.lambda_S, self.lambda_spatial_L, self.lambda_spatial_S, self.gamma, self.lambda_step]
class LSFPNet(nn.Module):
    def __init__(self, LayerNo, lambdas, channels=32, style_dim=128):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        self.channels = channels
        self.style_dim = style_dim
        for ii in range(LayerNo):
            onelayer.append(BasicBlock(lambdas, channels=self.channels, style_dim=style_dim))
        self.fcs = nn.ModuleList(onelayer)
    def plot_block_output(self, M0, L, S, iter, epoch, output_dir):
        time_frame_index = 3
        nx, ny, nt = M0.size()
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        output_image = L + S
        fig, axes = plt.subplots(1, 4, figsize=(24, 6))
        fig.suptitle(f"Basic Block Output at Time Frame {time_frame_index} and Iteration {iter}", fontsize=20)
        axes[0].imshow(np.abs(M0[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[0].set_title("Input Image")
        axes[0].axis("off")
        axes[1].imshow(np.abs(L[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[1].set_title("Background Component (L)")
        axes[1].axis("off")
        axes[2].imshow(np.abs(S[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[2].set_title("Dynamic Component (S)")
        axes[2].axis("off")
        axes[3].imshow(np.abs(output_image[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[3].set_title("Combined Image (L + S)")
        axes[3].axis("off")
        filename = os.path.join(output_dir, f'basic_block_output_{epoch}_iter{iter}.png')
        plt.savefig(filename)
        plt.close()
    def forward(self, M0, param_E, param_d, csmap, epoch, output_dir, style_embedding=None):
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmap, style_embedding)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
            print("L: ", L.shape)
            print("S: ", S.shape)
            if epoch is not None:
                self.plot_block_output(M0, L, S, iter=ii, epoch=epoch, output_dir=output_dir)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step]
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, output_dir, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
        self.output_dir = output_dir
        self.style_dim = 128  # You can tune this hyperparameter
        self.mapping_network = MappingNetwork(style_dim=self.style_dim)
    @staticmethod
    def _normalise_both(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max()                       # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_indep(x: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = torch.quantile(x.abs(), 0.99) + 1e-6
        if scale < 1e-6: # Handle case where input is all zeros
             scale = 1.0
        return x / scale, scale
    def forward(self, y, E, csmap, acceleration=None, epoch=None, norm="both", **kwargs):
        x_init = E(inv=True, data=y, smaps=csmap)
        if norm =="both":
            x_init_norm, y_norm, scale = self._normalise_both(x_init, y)
        elif norm == "independent":
            x_init_norm, scale = self._normalise_indep(x_init)
            y_norm, scale_y = self._normalise_indep(y)
        elif norm == "none":
            x_init_norm = x_init
            y_norm = y
            scale = 1.0
        if acceleration:
            style_embedding = self.mapping_network(acceleration)
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir, style_embedding)
        else:
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir)
        loss_constraint_L = torch.square(torch.mean(loss_layers_adj_L[0])) / self.backbone_net.LayerNo
        loss_constraint_S = torch.square(torch.mean(loss_layers_adj_S[0])) / self.backbone_net.LayerNo
        for k in range(self.backbone_net.LayerNo - 1):
            loss_constraint_S += torch.square(torch.mean(loss_layers_adj_S[k + 1])) / self.backbone_net.LayerNo
            loss_constraint_L += torch.square(torch.mean(loss_layers_adj_L[k + 1])) / self.backbone_net.LayerNo
        recon = (L + S) * scale                 # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat, loss_constraint_L + loss_constraint_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step

=== mc.py ===
from typing import Union
import torch
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from radial import to_torch_complex
class MCLoss(Loss):
    r"""
    Measurement consistency loss
    This loss enforces that the reconstructions are measurement-consistent, i.e., :math:`y=\forw{\inverse{y}}`.
    The measurement consistency loss is defined as
    .. math::
        \|y-\forw{\inverse{y}}\|^2
    where :math:`\inverse{y}` is the reconstructed signal and :math:`A` is a forward operator.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param Metric, torch.nn.Module metric: metric used for computing data consistency, which is set as the mean squared error by default.
    """
    def __init__(self, model_type, metric: Union[Metric, torch.nn.Module] = torch.nn.MSELoss()):
        super(MCLoss, self).__init__()
        self.name = "mc"
        self.metric = metric
        self.device = torch.device("cuda")
        self.model_type = model_type
    def forward(self, y, x_net, physics, csmap, **kwargs):
        r"""
        Computes the measurement splitting loss
        :param torch.Tensor y: measurements.
        :param torch.Tensor x_net: reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: forward operator associated with the measurements.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.model_type == "CRNN":
            return self.metric(physics.A(x_net, csmap), y)
        elif self.model_type == "LSFPNet":
            x_net = to_torch_complex(x_net)
            y_hat = physics(inv=False, data=x_net, smaps=csmap).to(self.device)
            y_hat = torch.stack([y_hat.real, y_hat.imag], dim=-1)
            y = torch.stack([y.real, y.imag], dim=-1)
            return self.metric(y_hat, y)

=== ei.py ===
from typing import Union
import torch
import torch.nn as nn
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from einops import rearrange
from radial import to_torch_complex
class EILoss(Loss):
    r"""
    Equivariant imaging self-supervised loss.
    Assumes that the set of signals is invariant to a group of transformations (rotations, translations, etc.)
    in order to learn from incomplete measurement data alone https://https://arxiv.org/pdf/2103.14756.pdf.
    The EI loss is defined as
    .. math::
        \| T_g \hat{x} - \inverse{\forw{T_g \hat{x}}}\|^2
    where :math:`\hat{x}=\inverse{y}` is a reconstructed signal and
    :math:`T_g` is a transformation sampled at random from a group :math:`g\sim\group`.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param deepinv.transform.Transform transform: Transform to generate the virtually augmented measurement.
        It can be any torch-differentiable function (e.g., a :class:`torch.nn.Module`)
        including `torchvision transforms <https://pytorch.org/vision/stable/transforms.html>`_.
    :param Metric, torch.nn.Module metric: Metric used to compute the error between the reconstructed augmented measurement and the reference
        image.
    :param bool apply_noise: if ``True``, the augmented measurement is computed with the full sensing model
        :math:`\sensor{\noise{\forw{\hat{x}}}}` (i.e., noise and sensor model),
        otherwise is generated as :math:`\forw{\hat{x}}`.
    :param float weight: Weight of the loss.
    :param bool no_grad: if ``True``, the gradient does not propagate through :math:`T_g`. Default: ``False``.
        This option is useful for super-resolution problems, see https://arxiv.org/abs/2312.11232.
    """
    def __init__(
        self,
        transform: Transform,
        model_type: str,
        metric: Union[Metric, nn.Module] = torch.nn.MSELoss(),
        apply_noise=True,
        weight=1.0,
        no_grad=False,
        *args,
        **kwargs,
    ):
        super(EILoss, self).__init__(*args, **kwargs)
        self.name = "ei"
        self.metric = metric
        self.weight = weight
        self.T = transform
        self.noise = apply_noise
        self.no_grad = no_grad
        self.model_type = model_type
    def forward(self, x_net, physics, model, csmap, acceleration, **kwargs):
        r"""
        Computes the EI loss
        :param torch.Tensor x_net: Reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: Forward operator associated with the measurements.
        :param torch.nn.Module model: Reconstruction function.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.no_grad:
            with torch.no_grad():
                x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
                x2_rearranged = self.T(x_net_rearranged)
                x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
                x2 = x2.detach()
        else:
            x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
            x2_rearranged = self.T(x_net_rearranged)
            x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
        x2_complex = to_torch_complex(x2)
        y = physics(inv=False, data=x2_complex, smaps=csmap).to(csmap.device)
        x3, *_ = model(y, physics, csmap, acceleration, epoch=None)
        loss_ei = self.weight * self.metric(x3, x2)
        return loss_ei, x2

=== transform.py ===
import deepinv as dinv
import torch
from deepinv.transform import Transform
from einops import rearrange
import torch.nn.functional as F
from torchvision.transforms.functional import rotate
from typing import Union
class VideoRotate(Transform):
    r"""
    CORRECTED 2D Rotation for Videos (Handles deepinv composition).
    This class correctly applies a single, consistent random rotation to all frames of a video.
    It samples angles uniformly from a continuous range and is robust to being called
    from a deepinv composition operator that pre-flattens the video tensor.
    :param tuple[float, float] or float degrees: Range of degrees to select from.
        If degrees is a number instead of sequence like (min, max), the range of degrees
        will be (-degrees, +degrees).
    :param str interpolation_mode: "bilinear" or "nearest".
    :param bool constant_shape: if True, output has the same shape as the input.
    """
    def __init__(
        self,
        *args,
        degrees: Union[float, tuple[float, float]] = 180.0,
        interpolation_mode: str = "bilinear",
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        if isinstance(degrees, (int, float)):
            if degrees < 0:
                raise ValueError("If degrees is a single number, it must be non-negative.")
            self.degrees = (-degrees, degrees)
        else:
            if len(degrees) != 2:
                raise ValueError("If degrees is a sequence, it must be of length 2.")
            self.degrees = degrees
        self.interpolation_mode = interpolation_mode
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Uniformly samples `n_trans` random angles from the specified continuous range.
        """
        angles = [
            torch.empty(1).uniform_(self.degrees[0], self.degrees[1]).item()
            for _ in range(self.n_trans)
        ]
        return {"theta": angles}
    def _transform(
        self,
        x: torch.Tensor,
        theta: Union[torch.Tensor, list] = [],
        **kwargs,
    ) -> torch.Tensor:
        """
        Applies the rotation transformations. This method now explicitly handles 5D video tensors.
        """
        if not self._check_x_5D(x):
             raise ValueError("VideoRotate is designed for 5D video tensors (B, C, T, H, W).")
        B, C, T, H, W = x.shape
        if not theta:
            params = self._get_params(x)
            theta = params["theta"]
        if not theta:
            raise ValueError("Rotation angle 'theta' not provided.")
        angle_for_video = theta[0]
        angle_rad = -torch.tensor(angle_for_video) * (torch.pi / 180.0)
        cos_a, sin_a = torch.cos(angle_rad), torch.sin(angle_rad)
        self.last_angle = angle_for_video
        matrix = torch.tensor(
            [[cos_a, -sin_a, 0], [sin_a, cos_a, 0]], 
            dtype=torch.float32, device=x.device
        ).unsqueeze(0)
        matrix = matrix.repeat(B, 1, 1)
        grid_single = F.affine_grid(matrix, (B, C, H, W), align_corners=False)
        grid_expanded = grid_single.repeat_interleave(T, dim=0)
        x_flat = dinv.physics.TimeMixin.flatten(x)
        transformed_flat = F.grid_sample(x_flat, grid_expanded, mode=self.interpolation_mode, padding_mode='zeros', align_corners=False)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class VideoDiffeo(dinv.transform.CPABDiffeomorphism):
    """A Diffeomorphism transform that correctly handles 5D video tensors."""
    def _transform(self, x: torch.Tensor, **params) -> torch.Tensor:
        if not self._check_x_5D(x):
            return super()._transform(x, **params)
        B = x.shape[0]
        x_flat = dinv.physics.TimeMixin.flatten(x)
        flat_params = self.get_params(x_flat)
        transformed_flat = super()._transform(x_flat, **flat_params)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class SubsampleTime(Transform):
    r"""
    Augments a video by taking a random contiguous temporal sub-sequence of a
    RANDOM length, and then interpolating it back to the original length.
    :param int n_trans: Number of transformed versions to generate per input image.
    :param tuple[float, float] subsample_ratio_range: The min and max ratio of the
                                                     total time frames to keep (e.g., (0.7, 0.95)).
    :param torch.Generator rng: Random number generator.
    """
    def __init__(self, *args, subsample_ratio_range: tuple[float, float] = (0.7, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False  # Operate on 5D tensor
        min_r, max_r = subsample_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "subsample_ratio_range must be a valid range (min, max) between 0 and 1."
        self.subsample_ratio_range = subsample_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a random ratio and a random start index for each transform.
        """
        total_time_frames = x.shape[2]  # Shape is (B, C, T, H, W)
        min_r, max_r = self.subsample_ratio_range
        ratios = min_r + (max_r - min_r) * torch.rand(self.n_trans, generator=self.rng)
        start_indices = []
        for ratio in ratios:
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                start_indices.append(0)
                continue
            max_start_index = total_time_frames - subsample_length
            start_idx = torch.randint(
                low=0, high=max_start_index + 1, size=(1,), generator=self.rng
            ).item()
            start_indices.append(start_idx)
        return {"ratios": ratios, "start_indices": torch.tensor(start_indices, dtype=torch.long)}
    def _transform(
        self, x: torch.Tensor, ratios: torch.Tensor, start_indices: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """Performs the temporal subsampling and resizing for each requested transform."""
        B, C, total_time_frames, H, W = x.shape
        assert B == 1, "This transform implementation assumes a batch size of 1 for simplicity."
        output_list = []
        for i in range(self.n_trans):
            ratio = ratios[i]
            start_idx = start_indices[i]
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                output_list.append(x.clone()) # Use clone to avoid issues
                continue
            sub_sequence = x[:, :, start_idx : start_idx + subsample_length, :, :]
            flat_for_interp = rearrange(sub_sequence, "b c t h w -> b (c h w) t")
            resized_flat = torch.nn.functional.interpolate(
                flat_for_interp,
                size=total_time_frames,
                mode="linear",
                align_corners=False,
            )
            resized_sequence = rearrange(
                resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W
            )
            output_list.append(resized_sequence)
        return torch.cat(output_list, dim=0)
class PeakAwareBiPhasicWarp(Transform):
    r"""
    An enhancement-peak-aware temporal augmentation that warps BOTH the wash-in
    and wash-out phases independently with different random ratios.
    It finds the time of peak enhancement, splits the video, and then
    time-warps (compresses/stretches) both phases before reassembling them.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
                                                 compressing a phase. e.g., (0.6, 0.95).
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.6, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "warp_ratio_range must be valid."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generate two independent warp ratios for each requested transform:
        one for wash-in and one for wash-out.
        """
        params_list = []
        min_r, max_r = self.warp_ratio_range
        for _ in range(self.n_trans):
            washin_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            washout_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            params_list.append({
                "washin_ratio": washin_ratio,
                "washout_ratio": washout_ratio
            })
        return {"params_list": params_list}
    def _transform(self, x: torch.Tensor, params_list: list[dict], **kwargs) -> torch.Tensor:
        """Applies the independent, bi-phasic warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        mean_signal_curve = torch.mean(x, dim=(0, 1, 3, 4)) # Avg over B, C, H, W
        peak_idx = torch.argmax(mean_signal_curve)
        if peak_idx <= 0 or peak_idx >= x.shape[2] - 1:
            return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for params in params_list:
            wash_in_phase = x[:, :, :peak_idx, :, :]
            peak_frame = x[:, :, peak_idx:peak_idx+1, :, :]
            wash_out_phase = x[:, :, peak_idx+1:, :, :]
            warped_wash_in = self._warp_phase(wash_in_phase, params["washin_ratio"])
            warped_wash_out = self._warp_phase(wash_out_phase, params["washout_ratio"])
            new_x = torch.cat([warped_wash_in, peak_frame, warped_wash_out], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to subsample and interpolate a video phase."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase <= 1: # Cannot warp a single frame or empty tensor
            return phase_tensor
        subsample_len = max(1, int(T_phase * ratio)) # Ensure at least 1 frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        subsampled_flat = F.interpolate(flat_for_interp, size=subsample_len, mode='linear', align_corners=False)
        resized_flat = F.interpolate(subsampled_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class MonophasicTimeWarp(Transform):
    r"""
    A temporal augmentation specifically designed for monophasic enhancement curves
    (e.g., persistent or plateau types) where there is no wash-out phase.
    This transform keeps the first (pre-contrast) frame fixed and applies a
    single, smooth time-warp to the entire subsequent enhancement phase.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
        compressing/stretching the enhancement phase. e.g., (0.7, 1.3).
        Values < 1 compress time, values > 1 stretch time.
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.7, 1.3), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r, "warp_ratio_range must be a valid positive range."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single random warp ratio for the entire enhancement phase.
        """
        min_r, max_r = self.warp_ratio_range
        ratios = [min_r + (max_r - min_r) * torch.rand(1, generator=self.rng) for _ in range(self.n_trans)]
        return {"ratios": ratios}
    def _transform(self, x: torch.Tensor, ratios: list[float], **kwargs) -> torch.Tensor:
        """Applies the monophasic time warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        if x.shape[2] <= 1: # Cannot warp if there's only one frame
             return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for ratio in ratios:
            pre_contrast_frame = x[:, :, :1, :, :]
            enhancement_phase = x[:, :, 1:, :, :]
            warped_enhancement_phase = self._warp_phase(enhancement_phase, ratio)
            new_x = torch.cat([pre_contrast_frame, warped_enhancement_phase], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to interpolate a video phase to a new length."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase == 0:
            return phase_tensor
        ratio = ratio.item()
        new_length = int(round(T_phase * ratio))
        if new_length == 0: new_length = 1 # Ensure at least one frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        resized_flat = F.interpolate(flat_for_interp, size=new_length, mode='linear', align_corners=False)
        if new_length != T_phase:
            resized_flat = F.interpolate(resized_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class TemporalNoise(Transform):
    """ 
    Adds low-frequency random noise to the temporal signal of a video.
    This simulates smooth, slowly varying noise sources over time.
    """
    def __init__(self, *args, noise_strength: float = 0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        self.noise_strength = noise_strength
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single low-frequency noise vector for the transformation.
        """
        B, C, T, H, W = x.shape
        low_res_T = max(1, T // 4) 
        noise_low_res = torch.randn(B, 1, low_res_T, device=x.device)
        noise_high_res = F.interpolate(noise_low_res, size=T, mode='linear', align_corners=False)
        noise_norm = (noise_high_res - noise_high_res.mean(dim=-1, keepdim=True)) / (noise_high_res.std(dim=-1, keepdim=True) + 1e-8)
        final_noise = noise_norm * self.noise_strength
        return {'noise': final_noise}
    def _transform(self, x: torch.Tensor, noise: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the pre-generated noise to the image tensor.
        """
        B, C, T, H, W = x.shape
        x_noisy = x + noise.view(B, 1, T, 1, 1)
        return x_noisy
class TimeReverse(Transform):
    r"""
    Reverses the temporal order of frames in a video tensor.
    This transform flips the video along the time axis, effectively playing it
    backwards. This is a deterministic transformation.
    :param int n_trans: Number of transformed versions to generate per input image.
                        Since this is deterministic, it will just repeat the same
                        output if n_trans > 1.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        No random parameters are needed for time reversal as it's a
        deterministic operation.
        """
        return {}
    def _transform(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the time reversal transformation using torch.flip().
        """
        if len(x.shape) != 5:
            raise ValueError(f"TimeReverse expects a 5D tensor (B, C, T, H, W), but got shape {x.shape}.")
        return torch.flip(x, dims=[2])

=== eval.py ===
import os
import matplotlib.pyplot as plt
import torch
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import torchmetrics
import time
from dataloader import SimulatedDataset
from lsfpnet import to_torch_complex, from_torch_complex
import numpy as np
from scipy.optimize import curve_fit
from scipy.interpolate import PchipInterpolator
from tqdm import tqdm # A library for a nice progress bar
from scipy.stats import mannwhitneyu
from skimage.metrics import structural_similarity as ssim_map_func
import matplotlib.gridspec as gridspec
from skimage.measure import find_contours
from typing import List, Dict
from scipy.stats import pearsonr
def normalize_for_lpips(image, data_range):
    """Normalizes an image tensor to the [-1, 1] range for LPIPS."""
    min_val, max_val = data_range
    image_0_1 = (image - min_val) / (max_val - min_val)
    image_minus1_1 = 2 * image_0_1 - 1
    return image_minus1_1
def calc_image_metrics(input, reference, data_range, device, filename):
    """
    Calculates image metrics for a given input and reference image.
    """
    ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range).to(device)
    psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range).to(device)
    mse = torchmetrics.MeanSquaredError().to(device)
    lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=False).to(device)
    ssim = ssim(input, reference)
    psnr = psnr(input, reference)
    mse = mse(input, reference)
    if input.dim() == 5:
        num_slices = input.shape[2]
        lpips_scores = []
        for i in range(num_slices):
            input_slice = input[:, :, i, :, :]
            reference_slice = reference[:, :, i, :, :]
            input_lpips = normalize_for_lpips(input_slice.clone(), data_range)
            reference_lpips = normalize_for_lpips(reference_slice.clone(), data_range)
            if input_lpips.shape[1] == 1:
                input_lpips = input_lpips.repeat(1, 3, 1, 1)
                reference_lpips = reference_lpips.repeat(1, 3, 1, 1)
            input_lpips = input_lpips.to(reference_lpips.dtype)
            lpips_scores.append(lpips_metric(input_lpips, reference_lpips).item())
        final_lpips = sum(lpips_scores) / len(lpips_scores)
    return ssim.item(), psnr.item(), mse.item(), final_lpips
def calc_dc(input, reference, device):
    """
    Calculates data consistency MSE for a given input and reference k-space tensor.
    """
    mse = torchmetrics.MeanSquaredError().to(device)
    mae = torchmetrics.MeanAbsoluteError().to(device)
    input = from_torch_complex(input).to(device)
    reference = from_torch_complex(reference).to(device)
    mse = mse(input, reference)
    mae = mae(input, reference)
    return mse.item(), mae.item()
def evaluate_reconstruction_fidelity(
    ground_truth_params: np.ndarray,
    estimated_params: np.ndarray,
    masks: dict,
    param_names: list = None,
    regions_to_evaluate: list = None,
    display_plots: bool = True,
    filename: str = 'pk_param_maps.png'
) -> dict:
    """
    Evaluates the fidelity of reconstructed pharmacokinetic (PK) parameters against ground truth.
    This function performs a quantitative and visual comparison, mimicking the evaluation
    methods described in the research paper (e.g., Figures 6 and 8).
    Args:
        ground_truth_params (np.ndarray): The ground truth PK parameter map, typically a
                                          (H, W, 4) array from the `gen_dro` output.
        estimated_params (np.ndarray): The PK parameter map estimated from your reconstructed
                                       images, with the same shape as ground_truth_params.
        masks (dict): A dictionary of boolean masks for different tissue regions, typically
                      from the `gen_dro` output (e.g., dro_results['mask']).
        param_names (list, optional): A list of names for the 4 parameters.
                                      Defaults to ['ve', 'vp', 'Fp', 'PS'].
        regions_to_evaluate (list, optional): A list of region names (keys in the `masks`
                                            dict) to analyze. Defaults to all available masks.
        display_plots (bool): If True, generates and shows summary plots.
    Returns:
        dict: A nested dictionary containing the evaluation results (median error and p-value)
              for each region and each parameter.
    """
    if param_names is None:
        param_names = ['ve', 'vp', 'Fp (F_p)', 'PS'] # As ordered in gen_dro
    if regions_to_evaluate is None:
        regions_to_evaluate = [name for name, mask in masks.items() if mask.any()]
    if ground_truth_params.shape != estimated_params.shape:
        raise ValueError("Ground truth and estimated parameter maps must have the same shape.")
    evaluation_results = {}
    print("--- Reconstruction Fidelity Evaluation ---")
    print("-" * 40)
    for region in regions_to_evaluate:
        if region not in masks or not masks[region].any():
            continue
        print(f"Region: {region.capitalize()}")
        evaluation_results[region] = {}
        mask = masks[region]
        for i, p_name in enumerate(param_names):
            print("ground_truth_params: ", type(ground_truth_params))
            gt_values = ground_truth_params[:, :, i][mask]
            est_values = estimated_params[:, :, i][mask]
            print("gt_values: ", type(gt_values))
            gt_values_safe = gt_values.copy()
            gt_values_safe[gt_values_safe == 0] = 1e-9 # Add a small epsilon
            relative_error = (est_values - gt_values) / gt_values_safe
            median_err = np.median(relative_error)
            try:
                stat, p_value = mannwhitneyu(gt_values, est_values, alternative='two-sided')
            except ValueError: # Happens if all values are identical
                stat, p_value = 0, 1.0
            evaluation_results[region][p_name] = {
                'median_relative_error': median_err,
                'p_value': p_value
            }
            print(f"  - {p_name:<10}: Median Error = {median_err:+.2%}, p-value = {p_value:.4f}")
    if not display_plots:
        return evaluation_results
    num_params = ground_truth_params.shape[2]
    fig, axes = plt.subplots(num_params, 3, figsize=(15, 4 * num_params), sharex=True, sharey=True)
    fig.suptitle("Visual Comparison of PK Parameter Maps", fontsize=16)
    for i in range(num_params):
        p_name = param_names[i]
        gt_map = ground_truth_params[:, :, i]
        est_map = estimated_params[:, :, i]
        error_map = est_map - gt_map
        vmax = np.percentile(gt_map[gt_map > 0], 99) if (gt_map > 0).any() else 1.0
        vmin = 0
        im_gt = axes[i, 0].imshow(gt_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 0].set_title(f"Ground Truth: {p_name}")
        axes[i, 0].axis('off')
        fig.colorbar(im_gt, ax=axes[i, 0])
        im_est = axes[i, 1].imshow(est_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 1].set_title(f"Your Estimation: {p_name}")
        axes[i, 1].axis('off')
        fig.colorbar(im_est, ax=axes[i, 1])
        err_vmax = np.percentile(np.abs(error_map), 99)
        im_err = axes[i, 2].imshow(error_map, vmin=-err_vmax, vmax=err_vmax, cmap='coolwarm')
        axes[i, 2].set_title(f"Error Map (Est - GT)")
        axes[i, 2].axis('off')
        fig.colorbar(im_err, ax=axes[i, 2])
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return evaluation_results
def plot_spatial_quality(
    recon_img: np.ndarray,
    gt_img: np.ndarray,
    grasp_img: np.ndarray,
    time_frame_index: int,
    filename: str,
    grasp_comparison_filename: str,
    data_range: float, 
    acceleration: float,
):
    """
    Generates a comparison plot for a single time frame in a 2x4 grid.
    Each row includes: Ground Truth, Reconstruction, Error Map, and SSIM Map.
    Args:
        recon_img (np.ndarray): Your model's reconstructed image for this frame.
        gt_img (np.ndarray): The ground truth image for this frame.
        grasp_img (np.ndarray): The GRASP reconstruction image for this frame.
        time_frame_index (int): The index of the time frame for titling.
        filename (str): The path to save the output plot.
    """
    error_map_dl = recon_img - gt_img
    error_map_grasp = grasp_img - gt_img
    _, ssim_map_dl = ssim_map_func(gt_img, recon_img, data_range=data_range, full=True)
    _, ssim_map_grasp = ssim_map_func(gt_img, grasp_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(2, 4, figsize=(24, 12))
    fig.suptitle(f"Spatial Quality Comparison at Time Frame {time_frame_index} with AF {acceleration}", fontsize=20)
    axes[0, 0].imshow(gt_img, cmap='gray')
    axes[0, 0].set_title("Ground Truth")
    axes[0, 1].imshow(recon_img, cmap='gray')
    axes[0, 1].set_title("DL Reconstruction")
    im_err_dl = axes[0, 2].imshow(error_map_dl, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[0, 2].set_title("DL Error Map (Recon - GT)")
    fig.colorbar(im_err_dl, ax=axes[0, 2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[0, 3].imshow(ssim_map_dl, cmap='viridis', vmin=0, vmax=1)
    axes[0, 3].set_title("DL SSIM Map")
    fig.colorbar(im_ssim_dl, ax=axes[0, 3], fraction=0.046, pad=0.04)
    axes[1, 0].imshow(gt_img, cmap='gray')
    axes[1, 0].set_title("Ground Truth")
    axes[1, 1].imshow(grasp_img, cmap='gray')
    axes[1, 1].set_title("GRASP Reconstruction")
    im_err_grasp = axes[1, 2].imshow(error_map_grasp, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[1, 2].set_title("GRASP Error Map (Recon - GT)")
    fig.colorbar(im_err_grasp, ax=axes[1, 2], fraction=0.046, pad=0.04)
    im_ssim_grasp = axes[1, 3].imshow(ssim_map_grasp, cmap='viridis', vmin=0, vmax=1)
    axes[1, 3].set_title("GRASP SSIM Map")
    fig.colorbar(im_ssim_grasp, ax=axes[1, 3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    plt.savefig(filename)
    plt.close()
    error_map = recon_img - grasp_img
    ssim, ssim_map = ssim_map_func(grasp_img, recon_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(1, 4, figsize=(24, 6))
    fig.suptitle(f"DL vs GRASP Comparison at Time Frame {time_frame_index} with AF {acceleration}", fontsize=20)
    axes[0].imshow(grasp_img, cmap='gray')
    axes[0].set_title("GRASP Reconstruction")
    axes[1].imshow(recon_img, cmap='gray')
    axes[1].set_title("DL Reconstruction")
    im_err_dl = axes[2].imshow(error_map, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[2].set_title("Error Map (DL Recon - GRASP)")
    fig.colorbar(im_err_dl, ax=axes[2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[3].imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)
    axes[3].set_title(f"SSIM Map (SSIM between DL and GRASP Recons: {round(ssim, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    print("SSIM between GRASP and DL Recon: ", ssim)
    plt.savefig(grasp_comparison_filename)
    plt.close()
def plot_temporal_curves(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    masks: dict,
    time_points: np.ndarray,
    filename: str, 
    acceleration: float,
):
    """
    Plots the mean signal intensity vs. time for different tissue regions.
    This is CRITICAL for debugging PK model fitting.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        masks (dict): Dictionary of boolean NumPy masks for different regions.
        time_points (np.ndarray): The time vector for the x-axis.
        filename (str): The path to save the output plot.
    """
    regions = [r for r in ['malignant', 'glandular', 'muscle'] if r in masks and masks[r].any()]
    if not regions:
        print("No relevant regions found in mask to plot temporal curves.")
        return
    fig, axes = plt.subplots(1, len(regions), figsize=(7 * len(regions), 5), sharey=True)
    if len(regions) == 1: axes = [axes] # Ensure axes is always a list
    fig.suptitle(f"Temporal Fidelity: Mean Signal vs. Time (AF = {acceleration})", fontsize=16)
    for i, region in enumerate(regions):
        mask = masks[region]
        gt_curve = [gt_img_stack[:, :, t][mask].mean() for t in range(gt_img_stack.shape[2])]
        recon_curve = [recon_img_stack[:, :, t][mask].mean() for t in range(recon_img_stack.shape[2])]
        grasp_curve = [grasp_img_stack[:, :, t][mask].mean() for t in range(grasp_img_stack.shape[2])]
        if region == 'malignant':
            recon_correlation, _ = pearsonr(recon_curve, gt_curve)
            grasp_correlation, _ = pearsonr(grasp_curve, gt_curve)
        axes[i].plot(time_points, gt_curve, 'k-', label='Ground Truth', linewidth=2, marker='o')
        axes[i].plot(time_points, recon_curve, 'r--', label='DL Recon', marker='o')
        axes[i].plot(time_points, grasp_curve, 'b:', label='GRASP Recon', marker='o')
        axes[i].set_title(f"Region: {region.capitalize()}")
        axes[i].set_xlabel("Time (s)")
        axes[i].grid(True)
        axes[i].legend()
    axes[0].set_ylabel("Mean Signal Intensity")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return recon_correlation, grasp_correlation
def plot_single_temporal_curve(
    img_stack: np.ndarray,
    masks: Dict[str, np.ndarray],
    time_points: np.ndarray,
    num_frames: int,
    filename: str,
    acceleration: float,
    frames_to_show: List[int] = None,
):
    """
    Generates a comprehensive analysis plot for a single sample, showing the
    Tumor Contrast Enhancement Curve (CEC) and corresponding image frames with
    the tumor Region of Interest (ROI) highlighted.
    This function is modified to produce a detailed analysis plot for the
    'malignant' tissue type, using the ground truth data.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        grasp_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        masks (dict): Dictionary of boolean NumPy masks. Expects a 'malignant' key.
        time_points (np.ndarray): The time vector for the x-axis (e.g., frame numbers).
        filename (str): The path to save the output plot.
        sample_name (str): The name of the sample for the main plot title.
        frames_to_show (List[int]): A list of 4 frame indices to display in the
                                    image grid and highlight on the curve.
                                    If None, defaults to [0, 6, 13, 20].
    """
    region_key = 'malignant'
    if region_key not in masks or not masks[region_key].any():
        print(f"'{region_key}' mask not found or is empty. Skipping plot generation.")
        return
    tumor_mask = masks[region_key]
    if frames_to_show is None:
        interval = round(num_frames / 4)
        frames_to_show = [0, interval, 2*interval, num_frames-1]
    if len(frames_to_show) != 4:
        raise ValueError(f"This function is designed to show exactly 4 frames, but {len(frames_to_show)} were provided.")
    fig = plt.figure(figsize=(20, 8.5))
    fig.suptitle(f"Tumor Enhancement Over Time (AF = {acceleration})")
    gs = gridspec.GridSpec(2, 4, figure=fig, hspace=0.1, wspace=0.1)
    ax_curve = fig.add_subplot(gs[:, 0:2])
    ax_imgs = [
        fig.add_subplot(gs[0, 2]), fig.add_subplot(gs[0, 3]),
        fig.add_subplot(gs[1, 2]), fig.add_subplot(gs[1, 3])
    ]
    mean_curve = [img_stack[:, :, t][tumor_mask].mean() for t in range(img_stack.shape[2])]
    ax_curve.plot(time_points, mean_curve, 'o-', label='Mean Tumor Signal', linewidth=2, markersize=6)
    highlight_times = [time_points[i] for i in frames_to_show]
    highlight_vals = [mean_curve[i] for i in frames_to_show]
    ax_curve.plot(highlight_times, highlight_vals, 'r*', markersize=18, zorder=10) # zorder to ensure stars are on top
    ax_curve.set_title("Tumor Contrast Enhancement Curve (CEC)", fontsize=18, pad=10)
    ax_curve.set_xlabel("Time Frame", fontsize=16)
    ax_curve.set_ylabel("Mean Signal Intensity", fontsize=16)
    ax_curve.legend(fontsize=14)
    ax_curve.grid(True, linestyle='--')
    ax_curve.tick_params(axis='both', which='major', labelsize=14)
    contours = find_contours(tumor_mask, 0.5)
    for i, frame_idx in enumerate(frames_to_show):
        ax = ax_imgs[i]
        image = img_stack[:, :, frame_idx]
        ax.imshow(image, cmap='gray')#, vmin=vmin, vmax=vmax)
        for contour in contours:
            ax.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='red')
        ax.set_title(f"Frame {frame_idx}", fontsize=16)
        ax.axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect for suptitle
    plt.savefig(filename, bbox_inches='tight', dpi=150)
    plt.close(fig)
def plot_time_series(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    filename: str,
    acceleration: float,
):
    """
    Plots the middle 5 time points for Ground Truth, DL Recon, and GRASP.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        filename (str): The path to save the output plot.
    """
    num_frames = gt_img_stack.shape[2]
    indices = np.linspace(0, num_frames - 1, 5, dtype=int)
    fig, axes = plt.subplots(3, 5, figsize=(25, 15))
    fig.suptitle(f"Temporal Series Comparison (AF = {acceleration})", fontsize=20)
    for i, frame_idx in enumerate(indices):
        img = gt_img_stack[:, :, frame_idx]
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f"GT: Frame {frame_idx}")
        axes[0, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = recon_img_stack[:, :, frame_idx]
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].set_title(f"DL: Frame {frame_idx}")
        axes[1, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = grasp_img_stack[:, :, frame_idx]
        axes[2, i].imshow(img, cmap='gray')
        axes[2, i].set_title(f"GRASP: Frame {frame_idx}")
        axes[2, i].axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(filename)
    plt.close()
def tofts_model(t, Ktrans, ve, aif_t, aif_c):
    """Calculates the tissue concentration curve using the standard Tofts model."""
    ve = max(ve, 1e-6)
    interp_func = PchipInterpolator(aif_t, aif_c, extrapolate=True)
    aif_interp = interp_func(t)
    impulse_response = Ktrans * np.exp(-t * (Ktrans / ve))
    dt = t[1] - t[0] if len(t) > 1 else 1.0
    Ct = np.convolve(aif_interp, impulse_response, mode='full')[:len(t)] * dt
    return Ct
def signal_to_concentration(signal_curve, S0_pixel, T10_pixel, TR, r1, flip_angle_rad):
    """Converts an MRI signal curve S(t) to a concentration curve C(t)."""
    if S0_pixel < 1e-6:
        return np.zeros_like(signal_curve)
    norm_signal = signal_curve / S0_pixel
    sin_fa, cos_fa = np.sin(flip_angle_rad), np.cos(flip_angle_rad)
    denominator = sin_fa - norm_signal * cos_fa
    E1 = (sin_fa - norm_signal) / np.maximum(denominator, 1e-9)
    E1 = np.maximum(E1, 1e-9)
    R1_t = -np.log(E1) / TR
    R10 = 1.0 / T10_pixel
    concentration_curve = (R1_t - R10) / r1
    return np.maximum(0, concentration_curve)
def estimate_pk_parameters(
    reconstructed_images: np.ndarray,
    aif_t: np.ndarray,
    aif_c: np.ndarray,
    S0_map: np.ndarray,
    T10_map: np.ndarray,
    TR: float = 4.87e-3,
    r1: float = 4.3,
    flip_angle_deg: float = 10.0
) -> np.ndarray:
    """
    Estimates pharmacokinetic parameters (Ktrans, ve) from reconstructed DCE-MRI images.
    Args:
        reconstructed_images (np.ndarray): A (H, W, Time) array of dynamic images,
                                           THIS IS THE OUTPUT FROM YOUR DL MODEL.
        aif_t, aif_c (np.ndarray): The time points and concentrations for the AIF.
        S0_map, T10_map (np.ndarray): Baseline maps from the ground truth DRO.
        TR, r1, flip_angle_deg: Sequence parameters.
    Returns:
        np.ndarray: A (H, W, 4) array containing the estimated [ve, Ktrans, 0, 0] maps.
    """
    height, width, num_frames = reconstructed_images.shape
    flip_angle_rad = np.deg2rad(flip_angle_deg)
    time_points = aif_t
    ktrans_map = np.zeros((height, width))
    ve_map = np.zeros((height, width))
    fitting_func = lambda t, Ktrans, ve: tofts_model(t, Ktrans, ve, aif_t, aif_c)
    DEBUG_PIXEL_R, DEBUG_PIXEL_C = 150, 150
    print("Estimating PK parameters from the reconstructed images...")
    for r in tqdm(range(height), desc="Fitting PK Model"):
        for c in range(width):
            if S0_map[r, c] < np.mean(S0_map) * 0.1:
                continue
            signal_curve = np.abs(reconstructed_images[r, c, :])
            concentration_curve = signal_to_concentration(
                signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
            )
            try:
                initial_guess = [0.1 / 60, 0.2] # Ktrans in s^-1
                bounds = ([0, 0], [2.0 / 60, 1.0])
                params, _ = curve_fit(
                    fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                )
                ktrans_map[r, c] = params[0] * 60 # Convert from s^-1 to min^-1
                ve_map[r, c] = params[1]
            except RuntimeError:
                pass # Fit failed, leave as 0
            if r == DEBUG_PIXEL_R and c == DEBUG_PIXEL_C:
                print(f"\n--- DEBUGGING PIXEL ({r}, {c}) ---")
                signal_curve = np.abs(reconstructed_images[r, c, :])
                concentration_curve = signal_to_concentration(
                    signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
                )
                plt.figure(figsize=(10, 6))
                plt.plot(time_points, concentration_curve, 'bo', label='Measured Concentration (from DL Recon)')
                try:
                    params, _ = curve_fit(
                        fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                    )
                    ktrans_fit, ve_fit = params
                    fitted_curve = tofts_model(time_points, ktrans_fit, ve_fit, aif_t, aif_c)
                    plt.plot(time_points, fitted_curve, 'r-', label=f'Tofts Fit (Ktrans={ktrans_fit*60:.3f}, ve={ve_fit:.3f})')
                except RuntimeError:
                    plt.title(f"DEBUG: Curve fit FAILED for pixel ({r}, {c})")
                plt.xlabel("Time (s)")
                plt.ylabel("Concentration")
                plt.legend()
                plt.grid(True)
                plt.savefig(f"debug_pixel_fit_{r}_{c}.png")
                plt.close()
                print("--- DEBUG PLOT SAVED ---")
    zeros_map = np.zeros_like(ktrans_map)
    estimated_pk_map = np.stack([ve_map, ktrans_map, zeros_map, zeros_map], axis=-1)
    return estimated_pk_map
def eval_grasp(kspace, csmap, ground_truth, grasp_recon, physics, device, output_dir):
    grasp_recon_complex = rearrange(to_torch_complex(grasp_recon).squeeze(), 'h t w -> h w t')
    kspace = kspace.squeeze()
    grasp_kspace = physics(False, grasp_recon_complex.to(csmap.dtype), csmap)
    dc_mse_grasp, dc_mae_grasp = calc_dc(grasp_kspace, kspace, device)
    grasp_recon_np = grasp_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    c = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_recon = torch.tensor(c * grasp_recon_np, device=device)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    grasp_mag = torch.sqrt(grasp_recon[:, 0, ...]**2 + grasp_recon[:, 1, ...]**2)
    grasp_mag = rearrange(grasp_mag, 'c h t w -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp = calc_image_metrics(grasp_mag.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    return ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp
def eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, output_dir, label, device):
    acceleration = round(acceleration.item(), 1)
    x_recon_complex = to_torch_complex(x_recon).squeeze()
    kspace = kspace.squeeze()
    recon_kspace = physics(False, x_recon_complex, csmap)
    dc_mse, dc_mae = calc_dc(recon_kspace, kspace, device)
    x_recon_np = x_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    grasp_recon_np = grasp_img.cpu().numpy()
    c = np.dot(x_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(x_recon_np.flatten(), x_recon_np.flatten())
    recon_complex_scaled = torch.tensor(c * x_recon_np, device=device)
    c_grasp = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_img = torch.tensor(c_grasp * grasp_recon_np, device=device)
    recon_mag_scaled = torch.sqrt(recon_complex_scaled[:, 0, ...]**2 + recon_complex_scaled[:, 1, ...]**2)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    recon_mag_scaled = rearrange(recon_mag_scaled, 'c h w t -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim, psnr, mse, lpips = calc_image_metrics(recon_mag_scaled.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    grasp_recon_complex_np = rearrange(to_torch_complex(grasp_img).squeeze(), 'h t w -> h w t').cpu().numpy()
    grasp_mag_np = np.abs(grasp_recon_complex_np)
    x_recon_complex_np = to_torch_complex(recon_complex_scaled).squeeze().cpu().numpy()
    gt_squeezed = ground_truth.squeeze()  # Shape: (C, T, H, W) -> (2, 22, 320, 320)
    gt_rearranged = rearrange(gt_squeezed, 'c t h w -> t c h w') # Shape: (22, 320, 320, 2)
    gt_complex_tensor = to_torch_complex(gt_rearranged) # Shape: (22, 320, 320)
    gt_final_tensor = rearrange(gt_complex_tensor, 't h w -> h w t') # Shape: (320, 320, 22)
    gt_complex_np = gt_final_tensor.cpu().numpy()
    recon_mag_np = np.abs(x_recon_complex_np)
    gt_mag_np = np.abs(gt_complex_np)
    masks_np = {key: val.cpu().numpy().squeeze().astype(bool) for key, val in mask.items()}
    num_frames = recon_mag_np.shape[2]
    aif_time_points = np.linspace(0, 150, num_frames)
    print("\nGenerating diagnostic plots...")
    if mask['malignant'].any() and label is not None:
        peak_frame = num_frames // 3
        data_range = gt_mag_np[:, :, peak_frame].max() - gt_mag_np[:, :, peak_frame].min()
        plot_spatial_quality(
            recon_img=recon_mag_np[:, :, peak_frame],
            gt_img=gt_mag_np[:, :, peak_frame],
            grasp_img=grasp_mag_np[:, :, peak_frame],
            time_frame_index=peak_frame,
            filename=os.path.join(output_dir, f"spatial_quality_{label}.png"),
            grasp_comparison_filename=os.path.join(output_dir, f"grasp_comparison_{label}.png"),
            data_range=data_range,
            acceleration=acceleration,
        )
        recon_corr, grasp_corr = plot_temporal_curves(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            filename=os.path.join(output_dir, f"temporal_curves_{label}.png"),
            acceleration=acceleration,
        )
        plot_single_temporal_curve(
            img_stack=recon_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            num_frames=num_frames,
            filename=os.path.join(output_dir, f"recon_temporal_curve_{label}.png"),
            acceleration=acceleration,
        )
        plot_time_series(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            filename=os.path.join(output_dir, f"time_points_{label}.png"),
            acceleration=acceleration,
        )
        print("Diagnostic plots saved.")
    else:
        recon_corr, grasp_corr = None, None
    return ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
from typing import Union, List, Optional
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Splits each volume into Z separate examples (one per slice).
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx: Optional[Union[int, range]] = 41,
        N_time = 8,
        N_coils=16,
        spf_aug=False,
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset (e.g. "kspace").
            file_pattern (str): Glob pattern to match your HDF5 files (default "*.h5").
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.N_time = N_time
        self.N_coils = N_coils
        self.spf_aug = spf_aug
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
        self.slice_index_map = []
        for fp in self.file_list:
            with h5py.File(fp, "r") as f:
                if self.dataset_key not in f:
                    raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                ds = f[self.dataset_key]
                num_slices = ds.shape[0]
            slices_to_add = []
            if isinstance(self.slice_idx, int):
                if self.slice_idx < num_slices:
                    slices_to_add = [self.slice_idx]
                else:
                    print(f"Warning: slice_idx {self.slice_idx} is out of bounds for {fp} "
                        f"(size {num_slices}). Skipping this file for this slice.")
            elif isinstance(self.slice_idx, range):
                slices_to_add = [s for s in self.slice_idx if s < num_slices]
                if len(slices_to_add) < len(self.slice_idx):
                    print(f"Warning: Some requested slices were out of bounds for {fp}. "
                        f"Using only the valid slice indices from the provided list.")
            else:
                raise TypeError(f"slice_idx must be an int, range, or None, but got {type(self.slice_idx)}")
            for z in slices_to_add:
                self.slice_index_map.append((fp, z))
        print(f"Dataset initialized with {len(self.slice_index_map)} total slice examples.")
        self.spokes_range = [2, 4, 8, 12, 16, 24, 32, 36, 48]
        self.spf_weights = [1.0 / spf for spf in self.spokes_range]
    def load_dynamic_img(self, patient_id, slice):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{slice:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; "
                                f"expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data) 
    def load_csmaps(self, patient_id, slice):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{slice:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def __len__(self):
        return len(self.slice_index_map)
    def __getitem__(self, idx):
        """
        Returns a single slice of k-space as a torch.Tensor.
        The output shape will be the standard (C=2, T, S, I) where C is [real, imag].
        """
        file_path, current_slice_idx = self.slice_index_map[idx]
        current_slice_idx = int(current_slice_idx)
        patient_id = file_path.split('/')[-1].strip('.h5')
        grasp_img = self.load_dynamic_img(patient_id, current_slice_idx)
        csmap = self.load_csmaps(patient_id, current_slice_idx)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[current_slice_idx]
        if self.spf_aug:
            total_spokes = kspace_slice.shape[0] * kspace_slice.shape[2]
            N_samples = kspace_slice.shape[-1]
            kspace = rearrange(kspace_slice, 't c sp sam -> t sp c sam')
            kspace_flat = kspace.contiguous().view(total_spokes, self.N_coils, N_samples)
            spokes_per_frame = random.choices(self.spokes_range, self.spf_weights, k=1)[0]
            N_time = total_spokes // spokes_per_frame
            kspace_binned = kspace_flat.view(N_time, spokes_per_frame, self.N_coils, N_samples)
            kspace_slice = rearrange(kspace_binned, 't sp c sam -> t c sp sam')
        else:
            N_time = self.N_time
            N_samples = kspace_slice.shape[-1]
            spokes_per_frame = kspace_slice.shape[-2]
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        kspace_final = torch.flip(kspace_final, dims=[-1])
        csmap_tensor = torch.from_numpy(csmap)
        csmap_tensor = torch.rot90(csmap_tensor, k=2, dims=[-2, -1]) # Assuming dims 0,1 are H,W
        csmap = csmap_tensor.numpy()
        return kspace_final, csmap, grasp_img, N_samples, spokes_per_frame, N_time
class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.sample_paths = sorted(glob.glob(os.path.join(root_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {root_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {root_dir} for this dataset.")
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        kspace_complex = np.load(os.path.join(sample_dir, 'simulated_kspace.npy'))
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_recon = np.load(os.path.join(sample_dir, 'grasp_recon.npy'))
        ground_truth_complex = dro['ground_truth_images']
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        if self.model_type == "CRNN":
            kspace_real_imag = np.stack([kspace_complex.real, kspace_complex.imag]) # (2, C, Samples, T)
            kspace_torch = torch.from_numpy(kspace_real_imag)
            kspace_torch = kspace_torch.permute(2, 0, 1, 3).unsqueeze(0) # (B, C, 2, Samples, T)
        elif self.model_type == "LSFPNet":
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            raise ValueError(f"Unsupported model_type for SimulatedDataset: {self.model_type}")
        grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
        grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask#, parMap, aif, S0, T10, mask
class SimulatedSPFDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.sample_paths = sorted(glob.glob(os.path.join(root_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {root_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {root_dir} for this dataset.")
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
        self.spokes_per_frame = 16
        self.num_frames = 20
        self.window = slice(1, 21)
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        print(f"  Testing {self.spokes_per_frame} spokes/frame with {self.num_frames} frames.")
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            print("setting grasp img to zero")
            grasp_recon_torch = 0
        ground_truth_complex = dro['ground_truth_images']
        ground_truth_complex = ground_truth_complex[..., self.window]
        smap_torch = rearrange(torch.tensor(csmaps), 'h w c -> c h w').unsqueeze(0)
        simImg_torch = torch.tensor(ground_truth_complex).to(torch.cfloat)
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        return smap_torch, simImg_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask

