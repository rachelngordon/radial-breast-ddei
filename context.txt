=== train.py ===
import argparse
import json
import os
import subprocess
import deepinv as dinv
import matplotlib.pyplot as plt
import torch
import yaml
from crnn import CRNN, ArtifactRemovalCRNN
from dataloader import SliceDataset
from deepinv.loss import MCLoss#, EILoss
from deepinv.transform import Transform
from einops import rearrange
from radial import DynamicRadialPhysics, RadialDCLayer
from torch.utils.data import DataLoader
from torchvision.transforms import InterpolationMode
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
def get_cosine_ei_weight(
    current_epoch,
    warmup_epochs,
    schedule_duration,
    target_weight
):
    """
    Calculates the EI loss weight for the current epoch using a cosine schedule.
    This implements a curriculum learning strategy:
    1. For `warmup_epochs`, the weight is 0 (MC loss only).
    2. Over the next `schedule_duration` epochs, the weight smoothly ramps
       up from 0 to `target_weight` following a cosine curve.
    3. After the schedule is complete, the weight stays at `target_weight`.
    Args:
        current_epoch (int): The current training epoch (starting from 1).
        warmup_epochs (int): Number of epochs to train with only MC loss.
        schedule_duration (int): Number of epochs for the ramp-up.
        target_weight (float): The final EI loss weight to reach.
    Returns:
        float: The EI loss weight for the current epoch.
    """
    if current_epoch <= warmup_epochs:
        return 0.0
    schedule_progress_epoch = current_epoch - warmup_epochs
    if schedule_progress_epoch >= schedule_duration:
        return target_weight
    cosine_multiplier = 0.5 * (1 - np.cos(np.pi * schedule_progress_epoch / schedule_duration))
    return target_weight * cosine_multiplier
def plot_reconstruction_sample(x_recon, title, filename, output_dir, grasp_img=None, batch_idx=0, transform=False):
    """
    Plot reconstruction sample showing magnitude images across timeframes.
    Args:
        x_recon: Reconstructed image tensor of shape (B, C, T, H, W)
        title: Title for the plot
        filename: Filename for saving (without extension)
        output_dir: Directory to save the plot
        batch_idx: Which batch element to plot (default: 0)
    """
    x_recon_mag = torch.sqrt(x_recon[:, 0, ...] ** 2 + x_recon[:, 1, ...] ** 2)
    grasp_img_mag = torch.sqrt(grasp_img[:, 0, ...] ** 2 + grasp_img[:, 1, ...] ** 2)
    n_timeframes = x_recon_mag.shape[1]
    fig, axes = plt.subplots(
        nrows=2,
        ncols=n_timeframes,
        figsize=(n_timeframes * 3, 8),
        squeeze=False,
    )
    if transform:
        axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
    if transform:
        axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        os.makedirs(os.path.join(output_dir, "transforms"), exist_ok=True)
    else:
        axes[0, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
        axes[1, 0].set_ylabel("GRASP Benchmark", fontsize=14, labelpad=10)
    for t in range(n_timeframes):
        img = x_recon_mag[batch_idx, t, :, :].cpu().detach().numpy()
        grasp_img = grasp_img_mag[batch_idx, t, :, :].cpu().detach().numpy()
        ax1 = axes[0, t]
        ax1.imshow(np.rot90(img, 2), cmap="gray")
        ax1.set_title(f"t = {t}")
        ax1.set_xticks([])
        ax1.set_yticks([])
        ax2 = axes[1, t]
        ax2.imshow(grasp_img, cmap="gray")
        ax2.set_title(f"t = {t}")
        ax2.set_xticks([])
        ax2.set_yticks([])
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(output_dir, f"{filename}.png"))
    plt.close(fig)
def get_git_commit():
    try:
        commit_hash = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        return commit_hash
    except Exception as e:
        print(f"Error retrieving Git commit: {e}")
        return "unknown"
parser = argparse.ArgumentParser(description="Train ReconResNet model.")
parser.add_argument(
    "--config",
    type=str,
    required=False,
    default="config.yaml",
    help="Path to the configuration file",
)
parser.add_argument(
    "--exp_name", type=str, required=True, help="Name of the experiment"
)
parser.add_argument(
    "--from_checkpoint",
    type=bool,
    required=False,
    default=False,
    help="Whether to load from a checkpoint",
)
args = parser.parse_args()
commit_hash = get_git_commit()
print(f"Running experiment on Git commit: {commit_hash}")
exp_name = args.exp_name
print(f"Experiment: {exp_name}")
if args.from_checkpoint == True:
    with open(f"output/{exp_name}/config.yaml", "r") as file:
        config = yaml.safe_load(file)
else:
    with open(args.config, "r") as file:
        config = yaml.safe_load(file)
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
os.makedirs(output_dir, exist_ok=True)
if args.from_checkpoint == False:
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
        yaml.dump(config, file)
split_file = config["data"]["split_file"]
batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]
mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_weight = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]
epochs = config["training"]["epochs"]
save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]
device = torch.device(config["training"]["device"])
start_epoch = 1
with open(split_file, "r") as fp:
    splits = json.load(fp)
if max_subjects < 300:
    max_train = max_subjects * (1 - config["data"]["val_split_ratio"])
    max_val = max_subjects * config["data"]["val_split_ratio"]
    train_patient_ids = splits["train"][:max_train]
    val_patient_ids = splits["val"][:max_val]
else:
    train_patient_ids = splits["train"]
    val_patient_ids = splits["val"]
train_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=train_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
)
val_dataset = SliceDataset(
    root_dir=config["data"]["root_dir"],
    patient_ids=val_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    file_pattern="*.h5",
    slice_idx=config["dataloader"]["slice_idx"],
)
train_loader = DataLoader(
    train_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
val_loader = DataLoader(
    val_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=config["dataloader"]["shuffle"],
    num_workers=config["dataloader"]["num_workers"],
)
H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils = (
    config["data"]["timeframes"],
    config["data"]["spokes_per_frame"],
    config["data"]["coils"],
)
N_spokes = int(config["data"]["total_spokes"] / N_time)
physics = DynamicRadialPhysics(
    im_size=(H, W, N_time),
    N_spokes=N_spokes,
    N_samples=N_samples,
    N_time=N_time,
    N_coils=N_coils,
)
datalayer = RadialDCLayer(physics=physics)
backbone = CRNN(
    num_cascades=config["model"]["cascades"],
    chans=config["model"]["channels"],
    datalayer=datalayer,
).to(device)
model = ArtifactRemovalCRNN(backbone_net=backbone).to(device)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=config["model"]["optimizer"]["lr"],
    betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
    eps=config["model"]["optimizer"]["eps"],
    weight_decay=config["model"]["optimizer"]["weight_decay"],
)
mc_loss_fn = MCLoss()
if use_ei_loss:
    rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear")
    diffeo = VideoDiffeo(n_trans=1, device=device)
    subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
    monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
    temp_noise = TemporalNoise(n_trans=1)
    time_reverse = TimeReverse(n_trans=1)
    if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
        ei_loss_fn = EILoss(subsample | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "monophasic":
        ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
        ei_loss_fn = EILoss(temp_noise | (diffeo | rotate))
    elif config['model']['losses']['ei_loss']['temporal_transform'] == "reverse":
        ei_loss_fn = EILoss(time_reverse | (diffeo | rotate))
    else:
        raise(ValueError, "Unsupported Temporal Transform.")
print(
    "--- Generating and saving a Zero-Filled (ZF) reconstruction sample before training ---"
)
with torch.no_grad():
    val_kspace_sample, grasp_img = next(iter(val_loader))
    val_kspace_sample = val_kspace_sample.to(device)
    x_zf = physics.A_adjoint(val_kspace_sample)
    plot_reconstruction_sample(
        x_zf,
        "Zero-Filled (ZF) Reconstruction (Before Training)",
        "zf_reconstruction_baseline",
        output_dir,
        grasp_img
    )
print("--- ZF baseline image saved to output directory. Starting training. ---")
train_mc_losses = []
val_mc_losses = []
train_ei_losses = []
val_ei_losses = []
weighted_train_mc_losses = []
weighted_train_ei_losses = []
iteration_count = 0
if args.from_checkpoint == False:
    model.eval()
    initial_train_mc_loss = 0.0
    initial_val_mc_loss = 0.0
    initial_train_ei_loss = 0.0
    initial_val_ei_loss = 0.0
    with torch.no_grad():
        for measured_kspace, grasp_img in tqdm(train_loader, desc="Step 0 Training Evaluation"):
            x_recon = model(
                measured_kspace.to(device), physics
            )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics)
            initial_train_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model
                )
                initial_train_ei_loss += ei_loss.item()
        step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
        train_mc_losses.append(step0_train_mc_loss)
        step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
        train_ei_losses.append(step0_train_ei_loss)
        for measured_kspace, grasp_img in tqdm(val_loader, desc="Step 0 Validation Evaluation"):
            x_recon = model(
                measured_kspace.to(device), physics
            )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics)
            initial_val_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model
                )
                initial_val_ei_loss += ei_loss.item()
        step0_val_mc_loss = initial_val_mc_loss / len(val_loader)
        val_mc_losses.append(step0_val_mc_loss)
        step0_val_ei_loss = initial_val_ei_loss / len(val_loader)
        val_ei_losses.append(step0_val_ei_loss)
for epoch in range(start_epoch, epochs + 1):
    model.train()
    running_mc_loss = 0.0
    running_ei_loss = 0.0
    with torch.autograd.set_detect_anomaly(False):
        train_loader_tqdm = tqdm(
            train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
        )
        for measured_kspace, grasp_img in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
            iteration_count += 1
            optimizer.zero_grad()
            x_recon = model(
                measured_kspace.to(device), physics
            )  # model output shape: (B, C, T, H, W)
            mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics)
            running_mc_loss += mc_loss.item()
            if use_ei_loss:
                ei_loss, t_img = ei_loss_fn(
                    x_recon, physics, model
                )
                ei_loss_weight = get_cosine_ei_weight(
                    current_epoch=epoch,
                    warmup_epochs=warmup,
                    schedule_duration=duration,
                    target_weight=target_weight
                )
                print(f"Epoch {epoch:2d}: EI Weight = {ei_loss_weight:.8f}")
                running_ei_loss += ei_loss.item()
                total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight
                train_loader_tqdm.set_postfix(
                    mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                )
            else:
                total_loss = mc_loss
                train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
            if torch.isnan(total_loss):
                print(
                    "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                )
                raise RuntimeError("total_loss is NaN")
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            if epoch % save_interval == 0:
                plot_reconstruction_sample(
                    x_recon,
                    f"Training Sample - Epoch {epoch}",
                    f"train_sample_epoch_{epoch}",
                    output_dir,
                    grasp_img
                )
                plot_reconstruction_sample(
                    t_img,
                    f"Transformed Train Sample - Epoch {epoch}",
                    f"transforms/transform_train_sample_epoch_{epoch}",
                    output_dir,
                    x_recon,
                    transform=True
                )
        epoch_train_mc_loss = running_mc_loss / len(train_loader)
        train_mc_losses.append(epoch_train_mc_loss)
        weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
        if use_ei_loss:
            epoch_train_ei_loss = running_ei_loss / len(train_loader)
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
        else:
            train_ei_losses.append(0.0)
            weighted_train_ei_losses.append(0.0)
        model.eval()
        val_running_mc_loss = 0.0
        val_running_ei_loss = 0.0
        val_loader_tqdm = tqdm(
            val_loader,
            desc=f"Epoch {epoch}/{epochs}  Validation",
            unit="batch",
            leave=False,
        )
        with torch.no_grad():
            for val_kspace_batch, val_grasp_img in val_loader_tqdm:
                val_x_recon = model(val_kspace_batch.to(device), physics)
                val_y_meas = val_kspace_batch
                val_mc_loss = mc_loss_fn(val_y_meas.to(device), val_x_recon, physics)
                val_running_mc_loss += val_mc_loss.item()
                if use_ei_loss:
                    val_ei_loss, val_t_img = ei_loss_fn(
                        val_x_recon, physics, model
                    )
                    val_running_ei_loss += val_ei_loss.item()
                    val_loader_tqdm.set_postfix(
                        val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                    )
                else:
                    val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
            if epoch % save_interval == 0:
                plot_reconstruction_sample(
                    val_x_recon,
                    f"Validation Sample - Epoch {epoch}",
                    f"val_sample_epoch_{epoch}",
                    output_dir,
                    val_grasp_img
                )
                plot_reconstruction_sample(
                    val_t_img,
                    f"Transformed Validation Sample - Epoch {epoch}",
                    f"transforms/transform_val_sample_epoch_{epoch}",
                    output_dir,
                    val_x_recon,
                    transform=True
                )
                model_save_path = os.path.join(output_dir, f'{exp_name}_model_checkpoint_epoch{epoch}.pth')
                torch.save(model.state_dict(), model_save_path)
                print(f'Model saved to {model_save_path}')
        epoch_val_mc_loss = val_running_mc_loss / len(val_loader)
        val_mc_losses.append(epoch_val_mc_loss)
        if use_ei_loss:
            epoch_val_ei_loss = val_running_ei_loss / len(val_loader)
            val_ei_losses.append(epoch_val_ei_loss)
        else:
            val_ei_losses.append(0.0)
        if epoch % save_interval == 0:
            plt.figure()
            plt.plot(train_mc_losses, label="Training MC Loss")
            plt.plot(val_mc_losses, label="Validation MC Loss")
            plt.xlabel("Epoch")
            plt.ylabel("MC Loss")
            plt.title("Measurement Consistency Loss")
            plt.legend()
            plt.grid(True)
            plt.savefig(os.path.join(output_dir, "mc_losses.png"))
            plt.close()
            if use_ei_loss:
                plt.figure()
                plt.plot(train_ei_losses, label="Training EI Loss")
                plt.plot(val_ei_losses, label="Validation EI Loss")
                plt.xlabel("Epoch")
                plt.ylabel("EI Loss")
                plt.title("Equivariant Imaging Loss")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "ei_losses.png"))
                plt.close()
                plt.figure()
                plt.plot(weighted_train_mc_losses, label="MC Loss")
                plt.plot(weighted_train_ei_losses, label="EI Loss")
                plt.xlabel("Epoch")
                plt.ylabel("Loss")
                plt.title("Weighted Training Losses")
                plt.legend()
                plt.grid(True)
                plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
                plt.close()
        print(
            f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
        )
        if use_ei_loss:
            print(
                f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
            )
model_save_path = os.path.join(output_dir, f'{exp_name}_model_checkpoint.pth')
torch.save(model.state_dict(), model_save_path)
print(f'Model saved to {model_save_path}')

=== transform.py ===
import deepinv as dinv
import torch
from deepinv.transform import Transform
from einops import rearrange
import torch.nn.functional as F
from torchvision.transforms.functional import rotate
from typing import Union
class VideoRotate(Transform):
    r"""
    CORRECTED 2D Rotation for Videos (Handles deepinv composition).
    This class correctly applies a single, consistent random rotation to all frames of a video.
    It samples angles uniformly from a continuous range and is robust to being called
    from a deepinv composition operator that pre-flattens the video tensor.
    :param tuple[float, float] or float degrees: Range of degrees to select from.
        If degrees is a number instead of sequence like (min, max), the range of degrees
        will be (-degrees, +degrees).
    :param str interpolation_mode: "bilinear" or "nearest".
    :param bool constant_shape: if True, output has the same shape as the input.
    """
    def __init__(
        self,
        *args,
        degrees: Union[float, tuple[float, float]] = 180.0,
        interpolation_mode: str = "bilinear",
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        if isinstance(degrees, (int, float)):
            if degrees < 0:
                raise ValueError("If degrees is a single number, it must be non-negative.")
            self.degrees = (-degrees, degrees)
        else:
            if len(degrees) != 2:
                raise ValueError("If degrees is a sequence, it must be of length 2.")
            self.degrees = degrees
        self.interpolation_mode = interpolation_mode
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Uniformly samples `n_trans` random angles from the specified continuous range.
        """
        angles = [
            torch.empty(1).uniform_(self.degrees[0], self.degrees[1]).item()
            for _ in range(self.n_trans)
        ]
        return {"theta": angles}
    def _transform(
        self,
        x: torch.Tensor,
        theta: Union[torch.Tensor, list] = [],
        **kwargs,
    ) -> torch.Tensor:
        """
        Applies the rotation transformations. This method now explicitly handles 5D video tensors.
        """
        if not self._check_x_5D(x):
             raise ValueError("VideoRotate is designed for 5D video tensors (B, C, T, H, W).")
        B, C, T, H, W = x.shape
        if not theta:
            params = self._get_params(x)
            theta = params["theta"]
        if not theta:
            raise ValueError("Rotation angle 'theta' not provided.")
        angle_for_video = theta[0]
        angle_rad = -torch.tensor(angle_for_video) * (torch.pi / 180.0)
        cos_a, sin_a = torch.cos(angle_rad), torch.sin(angle_rad)
        matrix = torch.tensor(
            [[cos_a, -sin_a, 0], [sin_a, cos_a, 0]], 
            dtype=torch.float32, device=x.device
        ).unsqueeze(0)
        matrix = matrix.repeat(B, 1, 1)
        grid_single = F.affine_grid(matrix, (B, C, H, W), align_corners=False)
        grid_expanded = grid_single.repeat_interleave(T, dim=0)
        x_flat = dinv.physics.TimeMixin.flatten(x)
        transformed_flat = F.grid_sample(x_flat, grid_expanded, mode=self.interpolation_mode, padding_mode='zeros', align_corners=False)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class VideoDiffeo(dinv.transform.CPABDiffeomorphism):
    """A Diffeomorphism transform that correctly handles 5D video tensors."""
    def _transform(self, x: torch.Tensor, **params) -> torch.Tensor:
        if not self._check_x_5D(x):
            return super()._transform(x, **params)
        B = x.shape[0]
        x_flat = dinv.physics.TimeMixin.flatten(x)
        flat_params = self.get_params(x_flat)
        transformed_flat = super()._transform(x_flat, **flat_params)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class SubsampleTime(Transform):
    r"""
    Augments a video by taking a random contiguous temporal sub-sequence of a
    RANDOM length, and then interpolating it back to the original length.
    :param int n_trans: Number of transformed versions to generate per input image.
    :param tuple[float, float] subsample_ratio_range: The min and max ratio of the
                                                     total time frames to keep (e.g., (0.7, 0.95)).
    :param torch.Generator rng: Random number generator.
    """
    def __init__(self, *args, subsample_ratio_range: tuple[float, float] = (0.7, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False  # Operate on 5D tensor
        min_r, max_r = subsample_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "subsample_ratio_range must be a valid range (min, max) between 0 and 1."
        self.subsample_ratio_range = subsample_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a random ratio and a random start index for each transform.
        """
        total_time_frames = x.shape[2]  # Shape is (B, C, T, H, W)
        min_r, max_r = self.subsample_ratio_range
        ratios = min_r + (max_r - min_r) * torch.rand(self.n_trans, generator=self.rng)
        start_indices = []
        for ratio in ratios:
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                start_indices.append(0)
                continue
            max_start_index = total_time_frames - subsample_length
            start_idx = torch.randint(
                low=0, high=max_start_index + 1, size=(1,), generator=self.rng
            ).item()
            start_indices.append(start_idx)
        return {"ratios": ratios, "start_indices": torch.tensor(start_indices, dtype=torch.long)}
    def _transform(
        self, x: torch.Tensor, ratios: torch.Tensor, start_indices: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """Performs the temporal subsampling and resizing for each requested transform."""
        B, C, total_time_frames, H, W = x.shape
        assert B == 1, "This transform implementation assumes a batch size of 1 for simplicity."
        output_list = []
        for i in range(self.n_trans):
            ratio = ratios[i]
            start_idx = start_indices[i]
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                output_list.append(x.clone()) # Use clone to avoid issues
                continue
            sub_sequence = x[:, :, start_idx : start_idx + subsample_length, :, :]
            flat_for_interp = rearrange(sub_sequence, "b c t h w -> b (c h w) t")
            resized_flat = torch.nn.functional.interpolate(
                flat_for_interp,
                size=total_time_frames,
                mode="linear",
                align_corners=False,
            )
            resized_sequence = rearrange(
                resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W
            )
            output_list.append(resized_sequence)
        return torch.cat(output_list, dim=0)
class PeakAwareBiPhasicWarp(Transform):
    r"""
    An enhancement-peak-aware temporal augmentation that warps BOTH the wash-in
    and wash-out phases independently with different random ratios.
    It finds the time of peak enhancement, splits the video, and then
    time-warps (compresses/stretches) both phases before reassembling them.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
                                                 compressing a phase. e.g., (0.6, 0.95).
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.6, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "warp_ratio_range must be valid."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generate two independent warp ratios for each requested transform:
        one for wash-in and one for wash-out.
        """
        params_list = []
        min_r, max_r = self.warp_ratio_range
        for _ in range(self.n_trans):
            washin_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            washout_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            params_list.append({
                "washin_ratio": washin_ratio,
                "washout_ratio": washout_ratio
            })
        return {"params_list": params_list}
    def _transform(self, x: torch.Tensor, params_list: list[dict], **kwargs) -> torch.Tensor:
        """Applies the independent, bi-phasic warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        mean_signal_curve = torch.mean(x, dim=(0, 1, 3, 4)) # Avg over B, C, H, W
        peak_idx = torch.argmax(mean_signal_curve)
        if peak_idx <= 0 or peak_idx >= x.shape[2] - 1:
            return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for params in params_list:
            wash_in_phase = x[:, :, :peak_idx, :, :]
            peak_frame = x[:, :, peak_idx:peak_idx+1, :, :]
            wash_out_phase = x[:, :, peak_idx+1:, :, :]
            warped_wash_in = self._warp_phase(wash_in_phase, params["washin_ratio"])
            warped_wash_out = self._warp_phase(wash_out_phase, params["washout_ratio"])
            new_x = torch.cat([warped_wash_in, peak_frame, warped_wash_out], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to subsample and interpolate a video phase."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase <= 1: # Cannot warp a single frame or empty tensor
            return phase_tensor
        subsample_len = max(1, int(T_phase * ratio)) # Ensure at least 1 frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        subsampled_flat = F.interpolate(flat_for_interp, size=subsample_len, mode='linear', align_corners=False)
        resized_flat = F.interpolate(subsampled_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class MonophasicTimeWarp(Transform):
    r"""
    A temporal augmentation specifically designed for monophasic enhancement curves
    (e.g., persistent or plateau types) where there is no wash-out phase.
    This transform keeps the first (pre-contrast) frame fixed and applies a
    single, smooth time-warp to the entire subsequent enhancement phase.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
        compressing/stretching the enhancement phase. e.g., (0.7, 1.3).
        Values < 1 compress time, values > 1 stretch time.
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.7, 1.3), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r, "warp_ratio_range must be a valid positive range."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single random warp ratio for the entire enhancement phase.
        """
        min_r, max_r = self.warp_ratio_range
        ratios = [min_r + (max_r - min_r) * torch.rand(1, generator=self.rng) for _ in range(self.n_trans)]
        return {"ratios": ratios}
    def _transform(self, x: torch.Tensor, ratios: list[float], **kwargs) -> torch.Tensor:
        """Applies the monophasic time warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        if x.shape[2] <= 1: # Cannot warp if there's only one frame
             return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for ratio in ratios:
            pre_contrast_frame = x[:, :, :1, :, :]
            enhancement_phase = x[:, :, 1:, :, :]
            warped_enhancement_phase = self._warp_phase(enhancement_phase, ratio)
            new_x = torch.cat([pre_contrast_frame, warped_enhancement_phase], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to interpolate a video phase to a new length."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase == 0:
            return phase_tensor
        ratio = ratio.item()
        new_length = int(round(T_phase * ratio))
        if new_length == 0: new_length = 1 # Ensure at least one frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        resized_flat = F.interpolate(flat_for_interp, size=new_length, mode='linear', align_corners=False)
        if new_length != T_phase:
            resized_flat = F.interpolate(resized_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class TemporalNoise(Transform):
    """ 
    Adds low-frequency random noise to the temporal signal of a video.
    This simulates smooth, slowly varying noise sources over time.
    """
    def __init__(self, *args, noise_strength: float = 0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        self.noise_strength = noise_strength
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single low-frequency noise vector for the transformation.
        """
        B, C, T, H, W = x.shape
        low_res_T = max(1, T // 4) 
        noise_low_res = torch.randn(B, 1, low_res_T, device=x.device)
        noise_high_res = F.interpolate(noise_low_res, size=T, mode='linear', align_corners=False)
        noise_norm = (noise_high_res - noise_high_res.mean(dim=-1, keepdim=True)) / (noise_high_res.std(dim=-1, keepdim=True) + 1e-8)
        final_noise = noise_norm * self.noise_strength
        return {'noise': final_noise}
    def _transform(self, x: torch.Tensor, noise: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the pre-generated noise to the image tensor.
        """
        B, C, T, H, W = x.shape
        x_noisy = x + noise.view(B, 1, T, 1, 1)
        return x_noisy
class TimeReverse(Transform):
    r"""
    Reverses the temporal order of frames in a video tensor.
    This transform flips the video along the time axis, effectively playing it
    backwards. This is a deterministic transformation.
    :param int n_trans: Number of transformed versions to generate per input image.
                        Since this is deterministic, it will just repeat the same
                        output if n_trans > 1.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        No random parameters are needed for time reversal as it's a
        deterministic operation.
        """
        return {}
    def _transform(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the time reversal transformation using torch.flip().
        """
        if len(x.shape) != 5:
            raise ValueError(f"TimeReverse expects a 5D tensor (B, C, T, H, W), but got shape {x.shape}.")
        return torch.flip(x, dims=[2])

=== radial.py ===
import deepinv as dinv
import numpy as np
import torch
import torch.nn as nn
from deepinv.physics.time import TimeMixin
from einops import rearrange
from torchkbnufft import KbNufft, KbNufftAdjoint
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
class RadialPhysics(dinv.physics.Physics):
    def __init__(self, im_size, N_spokes, N_samples, **kwargs):
        super().__init__(**kwargs)
        self.im_size = im_size
        self.N_spokes = N_spokes
        self.N_samples = N_samples
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        grid_size = [int(s * 2.0) for s in im_size]
        self.NUFFT = KbNufft(im_size=im_size, grid_size=grid_size).to(self.device)
        self.AdjNUFFT = KbNufftAdjoint(im_size=im_size, grid_size=grid_size).to(
            self.device
        )
        self.traj, self.sqrt_dcf = self.get_traj_and_dcf()
        self.traj = self.traj.to(self.device)
        self.sqrt_dcf = self.sqrt_dcf.to(self.device)
    def get_traj_and_dcf(self, angle_offset_rad=0.0):
        base_res = self.im_size[0]
        gind = 1
        N_samples = base_res * 2
        if N_samples != self.N_samples:
            print(
                f"Warning: Vendor logic implies N_samples should be {N_samples}, but class has {self.N_samples}. Using class value."
            )
            N_samples = self.N_samples  # Trust the class value passed during init
        base_lin = np.arange(N_samples).reshape(1, -1) - (N_samples // 2)
        tau = 0.5 * (1 + 5**0.5)
        base_rad = np.pi / (gind + tau - 1)
        base_rad = np.pi / (gind + tau - 1)
        spoke_indices = np.arange(self.N_spokes)
        base_rot = (spoke_indices * base_rad + angle_offset_rad).reshape(-1, 1)
        traj_flat = np.zeros((self.N_spokes, N_samples, 2))
        traj_flat[..., 0] = np.cos(base_rot) @ base_lin
        traj_flat[..., 1] = np.sin(base_rot) @ base_lin
        max_radius = N_samples / 2.0
        traj_flat = (traj_flat / max_radius) * np.pi
        traj = torch.from_numpy(traj_flat).float()
        traj_nufft_ready = rearrange(traj, "s i xy -> 1 xy (s i)")
        dcf_vals = torch.sqrt(
            traj_nufft_ready[0, 0, :] ** 2 + traj_nufft_ready[0, 1, :] ** 2
        )
        sqrt_dcf_vals = torch.sqrt(dcf_vals)
        sqrt_dcf = rearrange(sqrt_dcf_vals, "(s i) -> 1 1 (s i)", s=self.N_spokes)
        return traj_nufft_ready, sqrt_dcf
    def A(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        x_complex = to_torch_complex(x).unsqueeze(1)
        k_complex_nufft = self.NUFFT(x_complex, self.traj)
        y_complex_weighted = k_complex_nufft * self.sqrt_dcf
        y = from_torch_complex(y_complex_weighted.squeeze(1))
        return rearrange(y, "b c (s i) -> b c s i", s=self.N_spokes)
    def A_adjoint(self, y: torch.Tensor, **kwargs) -> torch.Tensor:
        y_flat = rearrange(y, "b c s i -> b c (s i)")
        y_complex = to_torch_complex(y_flat).unsqueeze(1)
        y_dcf_complex = y_complex * self.sqrt_dcf
        if torch.isnan(y_dcf_complex).any():
            print("!!! ERROR: NaN detected in y_dcf_complex in A_adjoint !!!")
        x_complex = self.AdjNUFFT(y_dcf_complex, self.traj).squeeze(1)
        return from_torch_complex(x_complex)
class DynamicRadialPhysics(RadialPhysics, TimeMixin):
    def __init__(self, im_size, N_spokes, N_samples, N_time, N_coils=1, **kwargs):
        TimeMixin.__init__(self)
        dinv.physics.Physics.__init__(self, **kwargs)
        self.im_size = im_size[:2]  # Static image size
        self.N_spokes = N_spokes  # Spokes PER FRAME
        self.N_samples = N_samples
        self.N_time = N_time
        self.N_coils = N_coils
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        grid_size = [int(s * 2.0) for s in self.im_size]
        self.NUFFT = KbNufft(im_size=self.im_size, grid_size=grid_size).to(self.device)
        self.AdjNUFFT = KbNufftAdjoint(im_size=self.im_size, grid_size=grid_size).to(
            self.device
        )
        total_spokes_in_scan = self.N_spokes * self.N_time
        temp_physics = RadialPhysics(
            self.im_size, N_spokes=total_spokes_in_scan, N_samples=self.N_samples
        )
        full_traj, full_sqrt_dcf = temp_physics.get_traj_and_dcf()
        self.traj_per_frame = rearrange(
            full_traj, "b c (t s i) -> t b c (s i)", t=self.N_time, s=self.N_spokes
        )
        self.sqrt_dcf_per_frame = rearrange(
            full_sqrt_dcf, "b c (t s i) -> t b c (s i)", t=self.N_time, s=self.N_spokes
        )
        self.traj_per_frame = self.traj_per_frame.to(self.device)
        self.sqrt_dcf_per_frame = self.sqrt_dcf_per_frame.to(self.device)
        self.mask = torch.ones(1, 2, self.N_time, self.N_spokes, self.N_samples).to(
            self.device
        )
    def A(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        B, C, T, H, W = x.shape
        output_kspace_frames = []
        for t in range(T):
            x_frame = x[:, :, t, :, :].unsqueeze(2)  # -> (B, C, 1, H, W)
            x_flat = self.flatten(x_frame)  # -> (B, C, H, W)
            traj_t = self.traj_per_frame[t]
            sqrt_dcf_t = self.sqrt_dcf_per_frame[t]
            x_complex = to_torch_complex(x_flat).unsqueeze(1)
            k_complex_nufft = self.NUFFT(x_complex, traj_t)
            y_complex_weighted = k_complex_nufft * sqrt_dcf_t
            y_frame = from_torch_complex(y_complex_weighted.squeeze(1))
            y_frame_reshaped = rearrange(
                y_frame, "b c (s i) -> b c s i", s=self.N_spokes
            )
            output_kspace_frames.append(y_frame_reshaped)
        y = torch.stack(output_kspace_frames, dim=2)  # Stack along the time dimension
        return y * self.mask
    def A_adjoint(self, y: torch.Tensor, **kwargs) -> torch.Tensor:
        B, C, T, S, I = y.shape
        output_image_frames = []
        y_masked = y * self.mask
        for t in range(T):
            y_frame = y_masked[:, :, t, :, :]  # -> (B, C, S, I)
            traj_t = self.traj_per_frame[t]
            sqrt_dcf_t = self.sqrt_dcf_per_frame[t]
            y_flat = rearrange(y_frame, "b c s i -> b c (s i)")
            y_complex = to_torch_complex(y_flat).unsqueeze(1)
            y_dcf_complex = y_complex * sqrt_dcf_t
            x_complex_frame = self.AdjNUFFT(y_dcf_complex, traj_t).squeeze(1)
            x_frame = from_torch_complex(x_complex_frame)  # -> (B, C, H, W)
            output_image_frames.append(x_frame)
        x = torch.stack(output_image_frames, dim=2)  # -> (B, C, T, H, W)
        return x
class RadialDCLayer(nn.Module):
    """
    Final Data Consistency layer.
    It takes the network's current image estimate and the original measurements,
    and returns a new image estimate that is a weighted average in k-space.
    """
    def __init__(
        self,
        physics: nn.Module,  # The DC layer now requires the physics operator
        lambda_init=np.log(np.exp(1) - 1.0) / 1.0,
        learnable=True,
    ):
        super(RadialDCLayer, self).__init__()
        self.learnable = learnable
        self.lambda_ = nn.Parameter(
            torch.ones(1) * lambda_init, requires_grad=self.learnable
        )
        self.physics = physics
    def forward(self, x_img_permuted, y_kspace_meas, mask_kspace):
        x_img = rearrange(x_img_permuted, "b h w t c -> b c t h w")
        y = y_kspace_meas
        A_x = self.physics.A(x_img)
        lambda_ = torch.sigmoid(self.lambda_)
        k_dc = (1 - mask_kspace) * A_x + mask_kspace * (
            lambda_ * A_x + (1 - lambda_) * y
        )
        x_dc_img = self.physics.A_adjoint(k_dc)
        x_dc_permuted = rearrange(x_dc_img, "b c t h w -> b h w t c")
        return x_dc_permuted

=== ei.py ===
from typing import Union
import torch
import torch.nn as nn
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
class EILoss(Loss):
    r"""
    Equivariant imaging self-supervised loss.
    Assumes that the set of signals is invariant to a group of transformations (rotations, translations, etc.)
    in order to learn from incomplete measurement data alone https://https://arxiv.org/pdf/2103.14756.pdf.
    The EI loss is defined as
    .. math::
        \| T_g \hat{x} - \inverse{\forw{T_g \hat{x}}}\|^2
    where :math:`\hat{x}=\inverse{y}` is a reconstructed signal and
    :math:`T_g` is a transformation sampled at random from a group :math:`g\sim\group`.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param deepinv.transform.Transform transform: Transform to generate the virtually augmented measurement.
        It can be any torch-differentiable function (e.g., a :class:`torch.nn.Module`)
        including `torchvision transforms <https://pytorch.org/vision/stable/transforms.html>`_.
    :param Metric, torch.nn.Module metric: Metric used to compute the error between the reconstructed augmented measurement and the reference
        image.
    :param bool apply_noise: if ``True``, the augmented measurement is computed with the full sensing model
        :math:`\sensor{\noise{\forw{\hat{x}}}}` (i.e., noise and sensor model),
        otherwise is generated as :math:`\forw{\hat{x}}`.
    :param float weight: Weight of the loss.
    :param bool no_grad: if ``True``, the gradient does not propagate through :math:`T_g`. Default: ``False``.
        This option is useful for super-resolution problems, see https://arxiv.org/abs/2312.11232.
    """
    def __init__(
        self,
        transform: Transform,
        metric: Union[Metric, nn.Module] = torch.nn.MSELoss(),
        apply_noise=True,
        weight=1.0,
        no_grad=False,
        *args,
        **kwargs,
    ):
        super(EILoss, self).__init__(*args, **kwargs)
        self.name = "ei"
        self.metric = metric
        self.weight = weight
        self.T = transform
        self.noise = apply_noise
        self.no_grad = no_grad
    def forward(self, x_net, physics, model, **kwargs):
        r"""
        Computes the EI loss
        :param torch.Tensor x_net: Reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: Forward operator associated with the measurements.
        :param torch.nn.Module model: Reconstruction function.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.no_grad:
            with torch.no_grad():
                x2 = self.T(x_net)
                x2 = x2.detach()
        else:
            x2 = self.T(x_net)
        if self.noise:
            y = physics(x2)
        else:
            y = physics.A(x2)
        x3 = model(y, physics)
        loss_ei = self.weight * self.metric(x3, x2)
        return loss_ei, x2

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Splits each volume into Z separate examples (one per slice).
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx=41,
        N_time = 8
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset (e.g. "kspace").
            file_pattern (str): Glob pattern to match your HDF5 files (default "*.h5").
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.N_time = N_time
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
    def load_dynamic_img(self, patient_id):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{self.slice_idx:03d}_frame_{t:03d}.nii'
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; "
                                f"expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data) 
    def __len__(self):
        return len(self.file_list)
    def __getitem__(self, idx):
        """
        Returns a single slice of k-space as a torch.Tensor.
        The output shape will be the standard (C=2, T, S, I) where C is [real, imag].
        """
        file_path = self.file_list[idx]
        patient_id = file_path.split('/')[-1].strip('.h5')
        grasp_img = self.load_dynamic_img(patient_id)
        with h5py.File(file_path, "r") as f:
            ds = torch.tensor(f[self.dataset_key][:])
            kspace_slice = ds[self.slice_idx]
        kspace_single_coil = kspace_slice[:, 0, :, :]  # Shape: (T, S, I)
        real_part = kspace_single_coil.real
        imag_part = kspace_single_coil.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        return kspace_final, grasp_img
if __name__ == "__main__":
    root_dir = "/ess/scratch/scratch1/rachelgordon/dce-12tf/binned_kspace"
    dataset_key = "ktspace"  # change if your HDF5 group/dataset is named differently
    def to_complex(x_np: "np.ndarray") -> "np.ndarray":
        """
        If x_np.shape = (C, H, W, 2) or similar where the last dim is [real, imag],
        convert to complex64 with shape (C, H, W).
        Adjust slicing logic if your real/imag channels are elsewhere.
        """
        real = x_np[..., 0].astype("float32")
        imag = x_np[..., 1].astype("float32")
        return (real + 1j * imag).astype("complex64")
    dataset = SliceDataset(
        root_dir=root_dir, dataset_key=dataset_key, file_pattern="*.h5"
    )
    loader = DataLoader(
        dataset,
        batch_size=1,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    for batch_idx, kspace_batch in enumerate(loader):
        print(
            f"Batch {batch_idx}: k-space batch shape = {kspace_batch.shape}, dtype = {kspace_batch.dtype}"
        )
        break

=== crnn.py ===
import torch
import torch.nn as nn
from einops import rearrange
import wandb
def log(key, value, step):
    print(f"Step {step} - {key}: {value:.4f}")
def _normalize_batch(x):
    b, h, w, t, c = x.shape
    x_flat = x.reshape(b, -1)
    max_vals, _ = torch.max(torch.abs(x_flat), dim=1, keepdim=True)
    max_vals = max_vals + 1e-8
    max_vals_reshaped = max_vals.reshape(b, 1, 1, 1, 1)
    return x / max_vals_reshaped
def _renormalize_by_input(x_after_dc, x_before_dc):
    norm_before = torch.linalg.vector_norm(x_before_dc.flatten(1), dim=1, keepdim=True)
    norm_after = torch.linalg.vector_norm(x_after_dc.flatten(1), dim=1, keepdim=True)
    scaling_factor = norm_before / (norm_after + 1e-8)
    return x_after_dc * scaling_factor.view(-1, 1, 1, 1, 1)
def _normalize_batch(x):
    """
    Normalizes each item in the batch to the range [-1, 1] based on its absolute maximum value.
    This preserves the internal structure and relative enhancement curve shape.
    Shape of x: (B, H, W, T, C)
    """
    b, h, w, t, c = x.shape
    x_flat = x.reshape(b, -1)
    max_vals, _ = torch.max(torch.abs(x_flat), dim=1, keepdim=True)
    max_vals = max_vals + 1e-8
    max_vals_reshaped = max_vals.reshape(b, 1, 1, 1, 1)
    return x / max_vals_reshaped
def normalize_batch_percentile(x, percentile=99.5):
    """
    Normalizes each item in the batch based on a high percentile of its absolute values.
    This method is more robust to extreme outliers (e.g., from NUFFT artifacts)
    than normalizing by the absolute maximum. It ensures that the scaling factor
    is determined by the bulk of the image signal, not a single rogue pixel.
    Shape of x: (B, H, W, T, C) or any shape starting with Batch.
    """
    if not 0 < percentile <= 100:
        raise ValueError("Percentile must be between 0 and 100.")
    b = x.shape[0]
    x_flat = x.reshape(b, -1)
    q = percentile / 100.0
    percentile_vals = torch.quantile(torch.abs(x_flat), q, dim=1, keepdim=True)
    norm_factors = percentile_vals + 1e-8
    dims_to_unsqueeze = [1] * (x.dim() - 1)
    norm_factors_reshaped = norm_factors.view(b, *dims_to_unsqueeze)
    return x / norm_factors_reshaped
def normalize_batch_standardize(x, epsilon=1e-8):
    """
    Standardizes each item in the batch to have a mean of 0 and a standard deviation of 1.
    This is a very common and robust normalization technique in deep learning. It centers
    the data and scales it, which can significantly improve network stability and
    convergence, especially when dealing with inputs that have varying offsets or scales.
    Shape of x: (B, H, W, T, C) or any shape starting with the Batch dimension.
    """
    b = x.shape[0]
    x_flat = x.reshape(b, -1)
    mean = torch.mean(x_flat, dim=1, keepdim=True)
    std = torch.std(x_flat, dim=1, keepdim=True)
    safe_std = std + epsilon
    dims_to_unsqueeze = [1] * (x.dim() - 1)
    mean_reshaped = mean.view(b, *dims_to_unsqueeze)
    std_reshaped = safe_std.view(b, *dims_to_unsqueeze)
    return (x - mean_reshaped) / std_reshaped
class ConvRNNCell(nn.Module):
    """
    Convolutional RNN cell.
    - Removed spectral_norm wrapper from convolutional layers. This reduces regularization,
      allowing the network to potentially learn finer details of the contrast enhancement,
      at the cost of relying more on other stabilization techniques.
    """
    def __init__(self, in_chans, out_chans, bias=True):
        super(ConvRNNCell, self).__init__()
        self.in_chans = in_chans
        self.out_chans = out_chans
        self.i2h = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=bias)
        self.h2h = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=bias)
        self.relu = nn.ReLU(inplace=True)
    def forward(self, input, hidden_iteration, hidden):
        in_to_hid = self.i2h(input)
        hid_to_hid = self.h2h(hidden)
        ih_to_ih = self.i2h(hidden_iteration)
        hidden = self.relu(in_to_hid + hid_to_hid + ih_to_ih)
        return hidden
class BCRNN(nn.Module):
    """
    Bidirectional Convolutional RNN.
    Note: The original DDEI implementation calls this BCRNN but it is a standard
    unidirectional CRNN operating over the time dimension. We keep the naming for consistency.
    """
    def __init__(self, in_chans, chans, n_layers, bias=True, device="cuda"):
        super(BCRNN, self).__init__()
        self.in_chans = in_chans
        self.chans = chans
        self.n_layers = n_layers
        self.device = device
        self.CRNN_model = ConvRNNCell(in_chans, chans, bias=bias)
    def forward(self, input, hidden_iteration):
        B, H, W, T, C = input.shape
        input = input.permute(3, 0, 4, 1, 2).contiguous()
        hidden_iteration = hidden_iteration.permute(3, 0, 4, 1, 2).contiguous()
        hidden = torch.zeros(B, self.chans, H, W, device=self.device)
        hidden_list = []
        for i in range(T):
            hidden = self.CRNN_model(input[i], hidden_iteration[i], hidden)
            hidden_list.append(hidden)
        out = torch.stack(hidden_list, dim=0).permute(1, 3, 4, 0, 2).contiguous()
        return out
class CRNN(nn.Module):
    """
    Main Unrolled CRNN architecture.
    1.  **Inter-Cascade Normalization**: The output of the data consistency (DC) layer from each
        cascade is normalized before being fed into the next cascade. This is the primary fix
        for the exploding activation problem. It breaks the amplification feedback loop.
    2.  **Residual Block Fix**: The code block for processing the BCRNN output and adding the
        residual connection was reshaped incorrectly, which was the likely cause of the
        `size mismatch` error. This has been completely rewritten to be correct.
    3.  **Cleaned up Logic**: The variable naming and flow within the forward pass are clarified
        to distinguish between different states of the image estimate (e.g., `x_pre_dc`, `x_post_dc`).
    """
    def __init__(self, num_cascades, chans, in_chans=2, datalayer=None, **kwargs):
        super(CRNN, self).__init__()
        self.num_cascades = num_cascades
        self.chans = chans
        self.in_chans = in_chans
        self.datalayer = datalayer
        self.bcrnn = BCRNN(in_chans, chans, n_layers=1, **kwargs)
        self.res_conv = nn.Conv2d(chans, in_chans, kernel_size=1, stride=1, padding=0)
    def forward(self, x_init_permuted, y, mask):  # 'mask' is the unused dummy mask
        B, H, W, T, C = x_init_permuted.shape
        x_cascade_in = x_init_permuted
        for i in range(self.num_cascades):
            bcrnn_out = self.bcrnn(x_cascade_in, x_init_permuted)
            bcrnn_out_reshaped = (
                bcrnn_out.permute(0, 3, 4, 1, 2)
                .contiguous()
                .reshape(B * T, self.chans, H, W)
            )
            res_out_reshaped = self.res_conv(bcrnn_out_reshaped)
            res_out = (
                res_out_reshaped.reshape(B, T, self.in_chans, H, W)
                .permute(0, 3, 4, 1, 2)
                .contiguous()
            )
            x_pre_dc = x_cascade_in + res_out
            x_post_dc = self.datalayer(x_pre_dc, y, self.datalayer.physics.mask)
            global_step = 0
            if i < self.num_cascades - 1:
                mean = torch.mean(x_post_dc, dim=(-3, -2, -1), keepdim=True)
                std = torch.std(x_post_dc, dim=(-3, -2, -1), keepdim=True)
                epsilon = 1e-8
                x_cascade_in = (x_post_dc - mean) / (std + epsilon)
            else:
                x_cascade_in = x_post_dc
        return x_cascade_in
class ArtifactRemovalCRNN(nn.Module):
    def __init__(self, backbone_net, **kwargs):
        super(ArtifactRemovalCRNN, self).__init__()
        self.backbone_net = backbone_net
    def forward(self, y, physics, **kwargs):
        x_init = physics.A_adjoint(y)
        norm_of_zf_recon = torch.linalg.vector_norm(x_init)
        x_init_permuted = rearrange(x_init, "b c t h w -> b h w t c")
        x_init_norm_permuted = normalize_batch_standardize(x_init_permuted)
        mask = physics.mask #torch.ones_like(y)
        x_hat_permuted_raw = self.backbone_net(x_init_norm_permuted, y, mask)
        x_hat_raw = rearrange(x_hat_permuted_raw, "b h w t c -> b c t h w")
        norm_of_raw_output = torch.linalg.vector_norm(x_hat_raw)
        scaling_factor = norm_of_zf_recon / (norm_of_raw_output + 1e-8)
        x_hat_final = x_hat_raw * scaling_factor
        return x_hat_final

