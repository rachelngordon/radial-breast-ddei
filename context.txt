=== train_distributed.py ===
import argparse
import json
import os
import matplotlib.pyplot as plt
import torch
import yaml
from dataloader import SliceDataset, SimulatedDataset, SimulatedSPFDataset
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet_encoding import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex, GRASPRecon, sliding_window_inference, set_seed
from eval import eval_grasp, eval_sample, calc_dc
import csv
import math
import random
import time 
import seaborn as sns
from loss_metrics import LPIPSVideoMetric, SSIMVideoMetric
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import h5py
from torch.utils.tensorboard import SummaryWriter
def setup():
    """Initializes the distributed process group."""
    dist.init_process_group("nccl")
def cleanup():
    dist.destroy_process_group()
def main():
    set_seed(12)
    parser = argparse.ArgumentParser(description="Train ReconResNet model.")
    parser.add_argument(
        "--config",
        type=str,
        required=False,
        default="config.yaml",
        help="Path to the configuration file",
    )
    parser.add_argument(
        "--exp_name", type=str, required=True, help="Name of the experiment"
    )
    parser.add_argument(
        "--from_checkpoint",
        type=bool,
        required=False,
        default=False,
        help="Whether to load from a checkpoint",
    )
    args = parser.parse_args()
    exp_name = args.exp_name
    if args.from_checkpoint == True:
        with open(f"output/{exp_name}/config.yaml", "r") as file:
            config = yaml.safe_load(file)
        with open(args.config, "r") as file:
            new_config = yaml.safe_load(file)
        epochs = new_config['training']["epochs"]
    else:
        with open(args.config, "r") as file:
            config = yaml.safe_load(file)
        epochs = config['training']["epochs"]
    if config['training']['multigpu']:
        setup()
        global_rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        device = local_rank
        if global_rank == 0:
            print(f"Starting distributed training with {world_size} GPUs.")
        print(f"  - [Rank {global_rank}] -> Using device cuda:{device}")
    else:
        global_rank = 0
        device = torch.device(config["training"]["device"])
    if global_rank == 0 or not config['training']['multigpu']:
        commit_hash = get_git_commit()
        print(f"Running experiment on Git commit: {commit_hash}")
        print(f"Experiment: {exp_name}")
    output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
    eval_dir = os.path.join(output_dir, "eval_results")
    block_dir = os.path.join(output_dir, "block_outputs")
    ec_dir = os.path.join(output_dir, 'enhancement_curves')
    if global_rank == 0 or not config['training']['multigpu']:
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(eval_dir, exist_ok=True)
        os.makedirs(block_dir, exist_ok=True)
        os.makedirs(ec_dir, exist_ok=True)
        log_dir = os.path.join(output_dir, 'logs')
        writer = SummaryWriter(log_dir)
        if args.from_checkpoint == False:
            with open(os.path.join(output_dir, 'config.yaml'), 'w') as file:
                yaml.dump(config, file)
    split_file = config["data"]["split_file"]
    batch_size = config["dataloader"]["batch_size"]
    max_subjects = config["dataloader"]["max_subjects"]
    initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                    'lambda_S': config['model']['lambda_S'], 
                    'lambda_spatial_L': config['model']['lambda_spatial_L'],
                    'lambda_spatial_S': config['model']['lambda_spatial_S'],
                    'gamma': config['model']['gamma'],
                    'lambda_step': config['model']['lambda_step']}
    mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
    adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]
    use_ei_loss = config["model"]["losses"]["use_ei_loss"]
    target_weight = config["model"]["losses"]["ei_loss"]["weight"]
    warmup = config["model"]["losses"]["ei_loss"]["warmup"]
    duration = config["model"]["losses"]["ei_loss"]["duration"]
    kspace_interpolation = config['training']['kspace_interpolation']
    save_interval = config["training"]["save_interval"]
    plot_interval = config["training"]["plot_interval"]
    model_type = config["model"]["name"]
    H, W = config["data"]["height"], config["data"]["width"]
    N_time, N_samples, N_coils = (
        config["data"]["timeframes"],
        config["data"]["samples"],
        config["data"]["coils"]
    )
    Ng = config["data"]["fpg"] 
    total_spokes = config["data"]["total_spokes"]
    N_spokes = int(total_spokes / N_time)
    N_full = config['data']['height'] * math.pi / 2
    eval_chunk_size = config["evaluation"]["chunk_size"]
    eval_chunk_overlap = config["evaluation"]["chunk_overlap"]
    cluster = config["experiment"].get("cluster", "Randi")
    if config["data"]["train_spokes_per_frame"] != "None":
        train_spokes_per_frame = config["data"]["train_spokes_per_frame"]
    else:
        train_spokes_per_frame = None
    curriculum_enabled = config['training']['curriculum_learning']['enabled']
    curriculum_phases = config['training']['curriculum_learning']['phases']
    initial_train_spokes_range = [8, 16, 24, 36]
    if curriculum_enabled:
        if not curriculum_phases:
            raise ValueError("Curriculum learning enabled but no phases defined in config.yaml")
        initial_train_spokes_range = curriculum_phases[0]['train_spokes_range']
        print(f"Curriculum Learning Enabled. Initial training with spokes range: {initial_train_spokes_range}")
    with open(split_file, "r") as fp:
        splits = json.load(fp)
    if max_subjects < 300:
        max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
        train_patient_ids = splits["train"][:max_train]
    else:
        train_patient_ids = splits["train"]
    val_patient_ids = splits["val"]
    val_dro_patient_ids = splits["val_dro"]
    for val_id in val_patient_ids:
        if val_id in train_patient_ids:
            raise ValueError(f"Data Leakage encountered! Duplicate sample in train and val patient IDs: {val_id}")
    if config['dataloader']['slice_range_start'] == "None" or config['dataloader']['slice_range_end'] == "None":
        train_dataset = SliceDataset(
            root_dir=config["data"]["root_dir"],
            patient_ids=train_patient_ids,
            dataset_key=config["data"]["dataset_key"],
            file_pattern="*.h5",
            slice_idx=config["dataloader"]["slice_idx"],
            num_random_slices=config["dataloader"].get("num_random_slices", None),
            N_time=N_time,
            N_coils=N_coils,
            spf_aug=config['data']['spf_aug'],
            spokes_per_frame=train_spokes_per_frame,
            weight_accelerations=config['data']['weight_accelerations'],
            initial_spokes_range=initial_train_spokes_range,
            interpolate_kspace=kspace_interpolation,
            cluster=cluster
        )
    else:
        train_dataset = SliceDataset(
            root_dir=config["data"]["root_dir"],
            patient_ids=train_patient_ids,
            dataset_key=config["data"]["dataset_key"],
            file_pattern="*.h5",
            slice_idx=range(config['dataloader']['slice_range_start'], config['dataloader']['slice_range_end']),
            num_random_slices=config["dataloader"].get("num_random_slices", None),
            N_time=N_time,
            N_coils=N_coils,
            spf_aug=config['data']['spf_aug'],
            spokes_per_frame=train_spokes_per_frame,
            weight_accelerations=config['data']['weight_accelerations'],
            initial_spokes_range=initial_train_spokes_range,
            interpolate_kspace=kspace_interpolation,
            cluster=cluster
        )
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)
    g = torch.Generator()
    g.manual_seed(12)
    if config['training']['multigpu']:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=global_rank)
        train_loader = DataLoader(
            train_dataset,
            batch_size=config["dataloader"]["batch_size"],
            sampler=train_sampler,
            num_workers=config["dataloader"]["num_workers"],
            pin_memory=True,
            worker_init_fn=seed_worker,
            generator=g,
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=config["dataloader"]["batch_size"],
            shuffle=config["dataloader"]["shuffle"],
            num_workers=config["dataloader"]["num_workers"],
            pin_memory=True,
            worker_init_fn=seed_worker,
            generator=g,
        )
    if curriculum_enabled:
        N_spokes_eval = curriculum_phases[0]['eval_spokes_per_frame']
        N_time_eval = curriculum_phases[0]['eval_num_frames']
    else:
        N_time_eval, N_spokes_eval = config["data"]["eval_timeframes"], config["data"]["eval_spokes"]
    eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
    eval_ktraj = eval_ktraj.to(device)
    eval_dcomp = eval_dcomp.to(device)
    eval_nufft_ob = eval_nufft_ob.to(device)
    eval_adjnufft_ob = eval_adjnufft_ob.to(device)
    eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
    val_dro_dataset = SimulatedDataset(
        root_dir=config["evaluation"]["simulated_dataset_path"], 
        model_type=model_type, 
        patient_ids=val_dro_patient_ids,
        spokes_per_frame=N_spokes_eval,
        num_frames=N_time_eval)
    val_dro_loader = DataLoader(
        val_dro_dataset,
        batch_size=config["dataloader"]["batch_size"],
        shuffle=False,
        num_workers=config["dataloader"]["num_workers"],
        pin_memory=True,
    )
    lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], 
                            lambdas=initial_lambdas, 
                            channels=config['model']['channels'],
                            style_dim=config['model']['style_dim'],
                            svd_mode=config['model']['svd_mode'],
                            use_lowk_dc=config['model']['use_lowk_dc'],
                            lowk_frac=config['model']['lowk_frac'],
                            lowk_alpha=config['model']['lowk_alpha'],
                            film_bounded=config['model']['film_bounded'],
                            film_gain=config['model']['film_gain'],
                            film_identity_init=config['model']['film_identity_init'],
                            svd_noise_std=config['model']['svd_noise_std'],
                            film_L=config['model']['film_L'],
                            )
    if config['model']['encode_acceleration'] and config['model']['encode_time_index']:
        model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir, channels=2).to(device)
    else:
        model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir, channels=1).to(device)
    if config['training']['multigpu']:
        model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["model"]["optimizer"]["lr"],
        betas=(config["model"]["optimizer"]["b1"], config["model"]["optimizer"]["b2"]),
        eps=config["model"]["optimizer"]["eps"],
        weight_decay=config["model"]["optimizer"]["weight_decay"],
    )
    if args.from_checkpoint == True:
        checkpoint_file = f'output/{exp_name}/{exp_name}_model.pth'
        model, optimizer, start_epoch, target_w_ei, step0_train_ei_loss, epoch_train_mc_loss, train_curves, val_curves, eval_curves = load_checkpoint(model, optimizer, checkpoint_file)
    else:
        start_epoch = 1
        target_w_ei = 0.0
    if config['model']['losses']['mc_loss']['metric'] == "MSE":
        mc_loss_fn = MCLoss(model_type=model_type)
    elif config['model']['losses']['mc_loss']['metric'] == "MAE":
        mc_loss_fn = MCLoss(model_type=model_type, metric=torch.nn.L1Loss())
    else:
        raise(ValueError, "Unsupported MC Loss Metric.")
    if config['model']['losses']['ei_loss']['metric'] == "LPIPS":
        ei_loss_metric = LPIPSVideoMetric(net_type='alex') 
    elif config['model']['losses']['ei_loss']['metric'] == "SSIM":
        ei_loss_metric = SSIMVideoMetric()
    else:
        ei_loss_metric = torch.nn.MSELoss()
    if use_ei_loss:
        rotate = VideoRotate(n_trans=1, interpolation_mode="bilinear", degrees=config['model']['losses']['ei_loss'].get("rotate_range", 180))
        diffeo = VideoDiffeo(n_trans=1, device=device)
        subsample = SubsampleTime(n_trans=1, subsample_ratio_range=(config['model']['losses']['ei_loss']['subsample_ratio_min'], config['model']['losses']['ei_loss']['subsample_ratio_max']))
        monophasic_warp = MonophasicTimeWarp(n_trans=1, warp_ratio_range=(config['model']['losses']['ei_loss']['warp_ratio_min'], config['model']['losses']['ei_loss']['warp_ratio_max']))
        temp_noise = TemporalNoise(n_trans=1, noise_strength=config['model']['losses']['ei_loss'].get("noise_strength", 0.5))
        time_reverse = TimeReverse(n_trans=1)
        if config['model']['losses']['ei_loss']['temporal_transform'] == "subsample":
            if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
                ei_loss_fn = EILoss(subsample, metric=ei_loss_metric, model_type=model_type)
            else:
                ei_loss_fn = EILoss(subsample | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp":
            if config['model']['losses']['ei_loss']['spatial_transform'] == "none":
                ei_loss_fn = EILoss(monophasic_warp, metric=ei_loss_metric, model_type=model_type)
            else:
                ei_loss_fn = EILoss(monophasic_warp | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['temporal_transform'] == "noise":
            ei_loss_fn = EILoss(temp_noise, metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['temporal_transform'] == "warp_subsample":
            ei_loss_fn = EILoss((subsample | monophasic_warp) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['temporal_transform'] == "none":
            if config['model']['losses']['ei_loss']['spatial_transform'] == "rotate":
                ei_loss_fn = EILoss(rotate, metric=ei_loss_metric, model_type=model_type)
            elif config['model']['losses']['ei_loss']['spatial_transform'] == "diffeo":
                ei_loss_fn = EILoss(diffeo, metric=ei_loss_metric, model_type=model_type)
            else:
                ei_loss_fn = EILoss(rotate | diffeo, metric=ei_loss_metric, model_type=model_type)
        elif config['model']['losses']['ei_loss']['spatial_transform'] == "all":
            if config['model']['losses']['ei_loss']['temporal_transform'] == "all":
                ei_loss_fn = EILoss((subsample | monophasic_warp | temp_noise) | (diffeo | rotate), metric=ei_loss_metric, model_type=model_type)
        else:
            raise(ValueError, "Unsupported Temporal Transform.")
    if args.from_checkpoint:
        train_mc_losses = train_curves["train_mc_losses"]
        val_mc_losses = val_curves["val_mc_losses"]
        train_ei_losses = train_curves["train_ei_losses"]
        val_ei_losses = val_curves["val_ei_losses"]
        train_adj_losses = train_curves["train_adj_losses"]
        val_adj_losses = val_curves["val_adj_losses"]
        weighted_train_mc_losses = train_curves["weighted_train_mc_losses"]
        weighted_train_ei_losses = train_curves["weighted_train_ei_losses"]
        weighted_train_adj_losses = train_curves["weighted_train_adj_losses"]
        eval_ssims = eval_curves["eval_ssims"]
        eval_psnrs = eval_curves["eval_psnrs"]
        eval_mses = eval_curves["eval_mses"]
        eval_lpipses = eval_curves["eval_lpipses"]
        eval_dc_mses = eval_curves["eval_dc_mses"]
        eval_dc_maes = eval_curves["eval_dc_maes"]
        eval_curve_corrs = eval_curves["eval_curve_corrs"]
    else:
        train_mc_losses = []
        val_mc_losses = []
        train_ei_losses = []
        val_ei_losses = []
        train_adj_losses = []
        val_adj_losses = []
        weighted_train_mc_losses = []
        weighted_train_ei_losses = []
        weighted_train_adj_losses = []
        eval_ssims = []
        eval_lpipses = []
        eval_psnrs = []
        eval_mses = []
        eval_dc_mses = []
        eval_dc_maes = []
        eval_curve_corrs = []
    grasp_ssims = []
    grasp_psnrs = []
    grasp_mses = []
    grasp_lpipses = []
    grasp_dc_mses = []
    grasp_dc_maes = []
    grasp_curve_corrs = []
    lambda_Ls = []
    lambda_Ss = []
    lambda_spatial_Ls = []
    lambda_spatial_Ss = []
    gammas = []
    lambda_steps = []
    iteration_count = 0
    if args.from_checkpoint == False and config['debugging']['calc_step_0'] == True:
        model.eval()
        initial_train_mc_loss = 0.0
        initial_val_mc_loss = 0.0
        initial_train_ei_loss = 0.0
        initial_val_ei_loss = 0.0
        initial_train_adj_loss = 0.0
        initial_val_adj_loss = 0.0
        initial_eval_ssims = []
        initial_eval_psnrs = []
        initial_eval_mses = []
        initial_eval_lpipses = []
        initial_eval_dc_mses = []
        initial_eval_dc_maes = []
        initial_eval_curve_corrs = []
        with torch.no_grad():
            for measured_kspace, csmap, N_samples, N_spokes, N_time in tqdm(train_loader, desc="Step 0 Training Evaluation"):
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                if N_time > Ng:
                    max_idx = N_time - Ng
                    random_index = random.randint(0, max_idx - 1) 
                    measured_kspace = measured_kspace[..., random_index:random_index + Ng]
                    ktraj_chunk = ktraj[..., random_index:random_index + Ng]
                    dcomp_chunk = dcomp[..., random_index:random_index + Ng]
                    physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
                    start_timepoint_index = torch.tensor([random_index], dtype=torch.float, device=device)
                else:
                    physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                    start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                csmap = csmap.to(device).to(measured_kspace.dtype)
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                if config['model']['encode_time_index'] == False:
                    start_timepoint_index = None
                x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                    measured_kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch="train0", norm=config['model']['norm']
                )
                initial_train_adj_loss += adj_loss.item()
                mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
                initial_train_mc_loss += mc_loss.item()
                if use_ei_loss:
                    ei_loss, t_img = ei_loss_fn(
                        x_recon, physics, model, csmap, acceleration_encoding, start_timepoint_index
                    )
                    initial_train_ei_loss += ei_loss.item()
            step0_train_mc_loss = initial_train_mc_loss / len(train_loader)
            train_mc_losses.append(step0_train_mc_loss)
            step0_train_ei_loss = initial_train_ei_loss / len(train_loader)
            train_ei_losses.append(step0_train_ei_loss)
            step0_train_adj_loss = initial_train_adj_loss / len(train_loader)
            train_adj_losses.append(step0_train_adj_loss)
            if global_rank == 0 or not config['training']['multigpu']:
                writer.add_scalar('Loss/Train_MC', step0_train_mc_loss, 0)
                writer.add_scalar('Loss/Train_EI', step0_train_ei_loss, 0)
                writer.add_scalar('Loss/Train_Weighted_EI', 0, 0)
                writer.add_scalar('Loss/Train_Adj', step0_train_ei_loss, 0)
            lambda_Ls.append(lambda_L.item())
            lambda_Ss.append(lambda_S.item())
            lambda_spatial_Ls.append(lambda_spatial_L.item())
            lambda_spatial_Ss.append(lambda_spatial_S.item())
            gammas.append(gamma.item())
            lambda_steps.append(lambda_step.item())
            for measured_kspace, csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):
                csmap = csmap.squeeze(0).to(device)   # Remove batch dim
                ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
                if type(measured_kspace) is list:
                    ground_truth_for_physics = rearrange(to_torch_complex(ground_truth), 'b t h w -> b h w t')
                    kspace_path = measured_kspace[0]
                    measured_kspace = eval_physics(False, ground_truth_for_physics, csmap)
                    np.save(kspace_path, measured_kspace.cpu().numpy())
                measured_kspace = measured_kspace.squeeze(0).to(device) # Remove batch dim
                if type(grasp_img) is int or len(grasp_img.shape) == 1:
                    print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                    grasp_img = GRASPRecon(csmap, measured_kspace, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                    grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                    grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                    grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                    grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                N_spokes = eval_ktraj.shape[1] / config['data']['samples']
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                if config['model']['encode_time_index'] == False:
                    start_timepoint_index = None
                else:
                    start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                if N_time_eval > eval_chunk_size:
                    print("Performing sliding window eval...")
                    print("measured kspace input shape: ", measured_kspace.shape)
                    x_recon, adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, measured_kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch="val0", device=device)  
                else:
                    print("measured kspace input shape: ", measured_kspace.shape)
                    x_recon, adj_loss, *_ = model(
                    measured_kspace.to(device), eval_physics, csmap, acceleration_encoding, start_timepoint_index, epoch="val0", norm=config['model']['norm']
                    )
                initial_val_adj_loss += adj_loss.item()
                mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, eval_physics, csmap)
                initial_val_mc_loss += mc_loss.item()
                if use_ei_loss:
                    ei_loss, t_img = ei_loss_fn(
                        x_recon, eval_physics, model, csmap, acceleration_encoding, start_timepoint_index
                    )
                    initial_val_ei_loss += ei_loss.item()
                ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
                grasp_recon = grasp_img.to(device) # Shape: (1, 2, H, T, W)
                if global_rank == 0 or not config['training']['multigpu']:
                    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(measured_kspace, csmap, ground_truth, grasp_recon, eval_physics, device, eval_dir)
                    grasp_ssims.append(ssim_grasp)
                    grasp_psnrs.append(psnr_grasp)
                    grasp_mses.append(mse_grasp)
                    grasp_lpipses.append(lpips_grasp)
                    grasp_dc_mses.append(dc_mse_grasp)
                    grasp_dc_maes.append(dc_mae_grasp)
                    ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(measured_kspace, csmap, ground_truth, x_recon, eval_physics, mask, grasp_recon, acceleration, int(N_spokes), eval_dir, label='val0', device=device)
                    initial_eval_ssims.append(ssim)
                    initial_eval_psnrs.append(psnr)
                    initial_eval_mses.append(mse)
                    initial_eval_lpipses.append(lpips)
                    initial_eval_dc_mses.append(dc_mse)
                    initial_eval_dc_maes.append(dc_mae)
                    if recon_corr is not None:
                        initial_eval_curve_corrs.append(recon_corr)
                        grasp_curve_corrs.append(grasp_corr)
            step0_val_mc_loss = initial_val_mc_loss / len(val_dro_loader)
            val_mc_losses.append(step0_val_mc_loss)
            step0_val_ei_loss = initial_val_ei_loss / len(val_dro_loader)
            val_ei_losses.append(step0_val_ei_loss)
            step0_val_adj_loss = initial_val_adj_loss / len(val_dro_loader)
            val_adj_losses.append(step0_val_adj_loss)
            if global_rank == 0 or not config['training']['multigpu']:
                writer.add_scalar('Loss/Val_MC', step0_val_mc_loss, 0)
                writer.add_scalar('Loss/Val_EI', step0_val_ei_loss, 0)
                writer.add_scalar('Loss/Val_Adj', step0_val_adj_loss, 0)
            initial_eval_ssim = np.mean(initial_eval_ssims)
            initial_eval_psnr = np.mean(initial_eval_psnrs)
            initial_eval_mse = np.mean(initial_eval_mses)
            initial_eval_lpips = np.mean(initial_eval_lpipses)
            initial_eval_dc_mse = np.mean(initial_eval_dc_mses)
            initial_eval_dc_mae = np.mean(initial_eval_dc_maes)
            initial_eval_curve_corr = np.mean(initial_eval_curve_corrs)
            eval_ssims.append(initial_eval_ssim)
            eval_psnrs.append(initial_eval_psnr)
            eval_mses.append(initial_eval_mse)
            eval_lpipses.append(initial_eval_lpips)
            eval_dc_mses.append(initial_eval_dc_mse) 
            eval_dc_maes.append(initial_eval_dc_mae) 
            eval_curve_corrs.append(initial_eval_curve_corr)
            if global_rank == 0 or not config['training']['multigpu']:
                writer.add_scalar('Metric/SSIM', initial_eval_ssim, 0)
                writer.add_scalar('Metric/PSNR', initial_eval_psnr, 0)
                writer.add_scalar('Metric/MSE', initial_eval_mse, 0)
                writer.add_scalar('Metric/LPIPS', initial_eval_lpips, 0)
                writer.add_scalar('Metric/DC_MSE', initial_eval_dc_mse, 0)
                writer.add_scalar('Metric/DC_MAE', initial_eval_dc_mae, 0)
                writer.add_scalar('Metric/EC_Corr', initial_eval_curve_corr, 0)
        print(f"Step 0 Train Losses: MC: {step0_train_mc_loss}, EI: {step0_train_ei_loss}, Adj: {step0_train_adj_loss}")
        print(f"Step 0 Val Losses: MC: {step0_val_mc_loss}, EI: {step0_val_ei_loss}, Adj: {step0_val_adj_loss}")
    svd_fail_count = 0
    if (epochs + 1) == start_epoch:
        raise(ValueError("Full training epochs already complete."))
    else: 
        current_curriculum_phase_idx = -1
        for epoch in range(start_epoch, epochs + 1):
            model.train()
            running_mc_loss = 0.0
            running_ei_loss = 0.0
            running_adj_loss = 0.0
            epoch_eval_ssims = []
            epoch_eval_psnrs = []
            epoch_eval_mses = []
            epoch_eval_lpipses = []
            epoch_eval_dc_mses = []
            epoch_eval_dc_maes = []
            epoch_eval_curve_corrs = []
            train_loader_tqdm = tqdm(
                train_loader, desc=f"Epoch {epoch}/{epochs}  Training", unit="batch"
            )
            if hasattr(train_dataset, 'resample_slices'):
                print(f"Epoch {epoch}: Resampling training slices...")
                train_dataset.resample_slices()
            if curriculum_enabled:
                for i, phase in enumerate(curriculum_phases):
                    if epoch >= phase['start_epoch']:
                        if i > current_curriculum_phase_idx: # Transition to a new phase
                            print(f"\n--- Entering Curriculum Phase: {phase['name']} at Epoch {epoch} ---")
                            train_dataset.spokes_range = phase['train_spokes_range']
                            train_dataset.update_spokes_weights()
                            current_curriculum_phase_idx = i
                            N_spokes_eval = phase['eval_spokes_per_frame']
                            N_time_eval = phase['eval_num_frames']
                            eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
                            eval_ktraj = eval_ktraj.to(device)
                            eval_dcomp = eval_dcomp.to(device)
                            eval_nufft_ob = eval_nufft_ob.to(device)
                            eval_adjnufft_ob = eval_adjnufft_ob.to(device)
                            eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)
                            val_dro_dataset.spokes_per_frame = N_spokes_eval
                            val_dro_dataset.num_frames = N_time_eval
                            val_dro_dataset._update_sample_paths()
                            val_dro_loader = DataLoader(
                                val_dro_dataset,
                                batch_size=config["dataloader"]["batch_size"],
                                shuffle=False,
                                num_workers=config["dataloader"]["num_workers"],
                                pin_memory=True,
                            )
            if use_ei_loss:
                if epoch < warmup + 1:
                    target_w_ei = 0.0
                elif epoch == warmup + 1:
                    mc_loss_at_transition = epoch_train_mc_loss
                    print(f"Transitioning at Epoch {epoch}. MC Loss: {mc_loss_at_transition:.4e}")
                    if step0_train_ei_loss > 0:
                        target_w_ei = mc_loss_at_transition / step0_train_ei_loss
                    else:
                        target_w_ei = 0.0 # Prevent division by zero
                    print(f"Dynamically calculated target EI weight: {target_w_ei:.4f}")
            if config['training']['multigpu']:
                train_loader.sampler.set_epoch(epoch)
            for measured_kspace, csmap, N_samples, N_spokes, N_time in train_loader_tqdm:  # measured_kspace shape: (B, C, I, S, T)
                print("spokes per frame: ", N_spokes)
                start = time.time()
                measured_kspace = to_torch_complex(measured_kspace).squeeze()
                measured_kspace = rearrange(measured_kspace, 't co sp sam -> co (sp sam) t')
                ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, N_spokes, N_time)
                ktraj = ktraj.to(device)
                dcomp = dcomp.to(device)
                nufft_ob = nufft_ob.to(device)
                adjnufft_ob = adjnufft_ob.to(device)
                if N_time > Ng:
                    max_idx = N_time - Ng
                    random_index = random.randint(0, max_idx - 1) 
                    measured_kspace = measured_kspace[..., random_index:random_index + Ng]
                    ktraj_chunk = ktraj[..., random_index:random_index + Ng]
                    dcomp_chunk = dcomp[..., random_index:random_index + Ng]
                    physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
                    start_timepoint_index = torch.tensor([random_index], dtype=torch.float, device=device)
                else:
                    physics = MCNUFFT(nufft_ob, adjnufft_ob, ktraj, dcomp)
                    start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                iteration_count += 1
                optimizer.zero_grad()
                csmap = csmap.to(device).to(measured_kspace.dtype)
                acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                if config['model']['encode_acceleration']:
                    acceleration_encoding = acceleration
                else: 
                    acceleration_encoding = None
                if config['model']['encode_time_index'] == False:
                    start_timepoint_index = None
                try:
                    x_recon, adj_loss, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step = model(
                        measured_kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=f"train{epoch}", norm=config['model']['norm']
                    )
                    running_adj_loss += adj_loss.item()
                    mc_loss = mc_loss_fn(measured_kspace.to(device), x_recon, physics, csmap)
                    running_mc_loss += mc_loss.item()
                    if use_ei_loss:
                        ei_loss, t_img = ei_loss_fn(
                            x_recon, physics, model, csmap, acceleration_encoding, start_timepoint_index
                        )
                        ei_loss_weight = get_cosine_ei_weight(
                            current_epoch=epoch,
                            warmup_epochs=warmup,
                            schedule_duration=duration,
                            target_weight=target_w_ei
                        )
                        running_ei_loss += ei_loss.item()
                        total_loss = mc_loss * mc_loss_weight + ei_loss * ei_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                        train_loader_tqdm.set_postfix(
                            mc_loss=mc_loss.item(), ei_loss=ei_loss.item()
                        )
                    else:
                        total_loss = mc_loss * mc_loss_weight + torch.mul(adj_loss_weight, adj_loss)
                        train_loader_tqdm.set_postfix(mc_loss=mc_loss.item())
                    if torch.isnan(total_loss):
                        print(
                            "!!! ERROR: total_loss is NaN before backward pass. Aborting. !!!"
                        )
                        raise RuntimeError("total_loss is NaN")
                    total_loss.backward()
                    if config["debugging"]["enable_gradient_monitoring"] == True and iteration_count % config["debugging"]["monitoring_interval"] == 0:
                        log_gradient_stats(
                            model=model,
                            epoch=epoch,
                            iteration=iteration_count,
                            output_dir=output_dir,
                            log_filename="gradient_stats.csv"
                        )
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    total = epochs; warm = 5
                    if epoch <= warm:
                        lr_scale = epoch / warm
                    else:
                        p = (epoch - warm) / max(1, total - warm)
                        lr_scale = 0.2 + 0.8 * 0.5 * (1 + math.cos(math.pi * p))
                    for pg in optimizer.param_groups:
                        pg['lr'] = config["model"]["optimizer"]["lr"] * lr_scale
                    end = time.time()
                    if global_rank == 0 or not config['training']['multigpu']:
                        print("time for one iteration: ", end-start)
                except RuntimeError as e:
                    if "svd" in str(e).lower():
                        svd_fail_count += 1
                        optimizer.zero_grad()
                        print(f"[Warning] Skipping batch {iteration_count} in epoch {epoch} due to SVD failure. "
                            f"Total failures so far: {svd_fail_count}")
                        continue  # skip this batch, go to next one
                    else:
                        raise  # re-raise other errors
            if epoch % save_interval == 0:
                if global_rank == 0 or not config['training']['multigpu']:
                    plot_reconstruction_sample(
                        x_recon,
                        f"Training Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                        f"train_sample_epoch_{epoch}",
                        output_dir,
                    )
                    x_recon_reshaped = rearrange(x_recon, 'b c h w t -> b c t h w')
                    plot_enhancement_curve(
                        x_recon_reshaped,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'train_sample_enhancement_curve_epoch_{epoch}.png'))
                    if use_ei_loss:
                        plot_reconstruction_sample(
                            t_img,
                            f"Transformed Train Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                            f"transforms/transform_train_sample_epoch_{epoch}",
                            output_dir,
                            x_recon,
                            transform=True
                        )
            epoch_train_mc_loss = running_mc_loss / len(train_loader)
            train_mc_losses.append(epoch_train_mc_loss)
            weighted_train_mc_losses.append(epoch_train_mc_loss*mc_loss_weight)
            if use_ei_loss:
                epoch_train_ei_loss = running_ei_loss / len(train_loader)
            else:
                epoch_train_ei_loss = 0.0
                ei_loss_weight = 0
            train_ei_losses.append(epoch_train_ei_loss)
            weighted_train_ei_losses.append(epoch_train_ei_loss*ei_loss_weight)
            epoch_train_adj_loss = running_adj_loss / len(train_loader)
            train_adj_losses.append(epoch_train_adj_loss)
            weighted_train_adj_losses.append(epoch_train_adj_loss*adj_loss_weight)
            if global_rank == 0 or not config['training']['multigpu']:
                writer.add_scalar('Loss/Train_MC', epoch_train_mc_loss, epoch)
                writer.add_scalar('Loss/Train_EI', epoch_train_ei_loss, epoch)
                writer.add_scalar('Loss/Train_Adj', epoch_train_adj_loss, epoch)
            lambda_Ls.append(lambda_L.item())
            lambda_Ss.append(lambda_S.item())
            lambda_spatial_Ls.append(lambda_spatial_L.item())
            lambda_spatial_Ss.append(lambda_spatial_S.item())
            gammas.append(gamma.item())
            lambda_steps.append(lambda_step.item())
            model.eval()
            val_running_mc_loss = 0.0
            val_running_ei_loss = 0.0
            val_running_adj_loss = 0.0
            val_loader_tqdm = tqdm(
                val_dro_loader,
                desc=f"Epoch {epoch}/{epochs}  Validation",
                unit="batch",
                leave=False,
            )
            with torch.no_grad():
                for val_kspace_batch, val_csmap, val_ground_truth, val_grasp_img, val_mask, grasp_path in tqdm(val_dro_loader):
                    val_csmap = val_csmap.squeeze(0).to(device)   # Remove batch dim
                    val_ground_truth = val_ground_truth.to(device) # Shape: (1, 2, T, H, W)
                    if type(val_kspace_batch) is list:
                        ground_truth_for_physics = rearrange(to_torch_complex(val_ground_truth), 'b t h w -> b h w t')
                        kspace_path = val_kspace_batch[0]
                        val_kspace_batch = eval_physics(False, ground_truth_for_physics, val_csmap)
                    val_kspace_batch = val_kspace_batch.squeeze(0).to(device) # Remove batch dim
                    if type(val_grasp_img) is int or len(val_grasp_img.shape) == 1:
                        print(f"No GRASP file found, performing reconstruction with {val_dro_dataset.spokes_per_frame} spokes/frame and {val_dro_dataset.num_frames} frames.")
                        val_grasp_img = GRASPRecon(val_csmap, val_kspace_batch, val_dro_dataset.spokes_per_frame, val_dro_dataset.num_frames, grasp_path[0])
                        val_grasp_img = torch.from_numpy(val_grasp_img).permute(2, 0, 1) # T, H, W
                        val_grasp_img = torch.stack([val_grasp_img.real, val_grasp_img.imag], dim=0)
                        val_grasp_img = torch.flip(val_grasp_img, dims=[-3])
                        val_grasp_img = torch.rot90(val_grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                    val_grasp_img_tensor = val_grasp_img.to(device)
                    N_spokes = eval_ktraj.shape[1] / config['data']['samples']
                    acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)
                    if config['model']['encode_acceleration']:
                        acceleration_encoding = acceleration
                    else: 
                        acceleration_encoding = None
                    if config['model']['encode_time_index'] == False:
                        start_timepoint_index = None
                    else:
                        start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                    try:
                        if N_time_eval > eval_chunk_size:
                            print("Performing sliding window eval...")
                            val_x_recon, val_adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, val_kspace_batch, val_csmap, acceleration_encoding, start_timepoint_index, model, epoch=f"val{epoch}", device=device)  
                        else:
                            val_x_recon, val_adj_loss, *_ = model(
                            val_kspace_batch.to(device), eval_physics, val_csmap, acceleration_encoding, start_timepoint_index, epoch=f"val{epoch}", norm=config['model']['norm']
                            )
                        val_running_adj_loss += val_adj_loss.item()
                        val_mc_loss = mc_loss_fn(val_kspace_batch.to(device), val_x_recon, eval_physics, val_csmap)
                        val_running_mc_loss += val_mc_loss.item()
                        if use_ei_loss:
                            val_ei_loss, val_t_img = ei_loss_fn(
                                val_x_recon, eval_physics, model, val_csmap, acceleration_encoding, start_timepoint_index
                            )
                            val_running_ei_loss += val_ei_loss.item()
                            val_loader_tqdm.set_postfix(
                                val_mc_loss=val_mc_loss.item(), val_ei_loss=val_ei_loss.item()
                            )
                        else:
                            val_loader_tqdm.set_postfix(val_mc_loss=val_mc_loss.item())
                        if global_rank == 0 or not config['training']['multigpu']:
                            ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, _ = eval_sample(val_kspace_batch, val_csmap, val_ground_truth, val_x_recon, eval_physics, val_mask, val_grasp_img_tensor, acceleration, int(N_spokes), eval_dir, f'epoch{epoch}', device)
                            epoch_eval_ssims.append(ssim)
                            epoch_eval_psnrs.append(psnr)
                            epoch_eval_mses.append(mse)
                            epoch_eval_lpipses.append(lpips)
                            epoch_eval_dc_mses.append(dc_mse)
                            epoch_eval_dc_maes.append(dc_mae)
                            if recon_corr is not None:
                                epoch_eval_curve_corrs.append(recon_corr)
                    except RuntimeError as e:
                        if "svd" in str(e).lower():
                            svd_fail_count += 1
                            optimizer.zero_grad()
                            print(f"[Warning] Skipping batch validation sample in epoch {epoch} due to SVD failure. "
                                f"Total failures so far: {svd_fail_count}")
                            continue  # skip this batch, go to next one
                        else:
                            raise  # re-raise other errors
            if global_rank == 0 or not config['training']['multigpu']:
                epoch_eval_ssim = np.mean(epoch_eval_ssims)
                epoch_eval_psnr = np.mean(epoch_eval_psnrs)
                epoch_eval_mse = np.mean(epoch_eval_mses)
                epoch_eval_lpips = np.mean(epoch_eval_lpipses)
                epoch_eval_dc_mse = np.mean(epoch_eval_dc_mses)
                epoch_eval_dc_mae = np.mean(epoch_eval_dc_maes)
                epoch_eval_curve_corr = np.mean(epoch_eval_curve_corrs)
                eval_ssims.append(epoch_eval_ssim)
                eval_psnrs.append(epoch_eval_psnr)
                eval_mses.append(epoch_eval_mse)
                eval_lpipses.append(epoch_eval_lpips)
                eval_dc_mses.append(epoch_eval_dc_mse) 
                eval_dc_maes.append(epoch_eval_dc_mae)    
                eval_curve_corrs.append(epoch_eval_curve_corr)  
                writer.add_scalar('Metric/SSIM', epoch_eval_ssim, epoch)
                writer.add_scalar('Metric/PSNR', epoch_eval_psnr, epoch)
                writer.add_scalar('Metric/MSE', epoch_eval_mse, epoch)
                writer.add_scalar('Metric/LPIPS', epoch_eval_lpips, epoch)
                writer.add_scalar('Metric/DC_MSE', epoch_eval_dc_mse, epoch)
                writer.add_scalar('Metric/DC_MAE', epoch_eval_dc_mae, epoch)
                writer.add_scalar('Metric/EC_Corr', epoch_eval_curve_corr, epoch)
                if epoch % save_interval == 0:
                    plot_reconstruction_sample(
                        val_x_recon,
                        f"Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                        f"val_sample_epoch_{epoch}",
                        output_dir,
                        val_grasp_img
                    )
                    val_x_recon_reshaped = rearrange(val_x_recon, 'b c h w t -> b c t h w')
                    plot_enhancement_curve(
                        val_x_recon_reshaped,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_sample_enhancement_curve_epoch_{epoch}.png'))
                    plot_enhancement_curve(
                        val_grasp_img,
                        output_filename = os.path.join(output_dir, 'enhancement_curves', f'val_grasp_sample_enhancement_curve_epoch_{epoch}.png'))
                    if use_ei_loss:
                        plot_reconstruction_sample(
                            val_t_img,
                            f"Transformed Validation Sample - Epoch {epoch} (AF = {round(acceleration.item(), 1)}, SPF = {int(N_spokes)})",
                            f"transforms/transform_val_sample_epoch_{epoch}",
                            output_dir,
                            val_x_recon,
                            transform=True
                        )
            epoch_val_mc_loss = val_running_mc_loss / len(val_dro_loader)
            val_mc_losses.append(epoch_val_mc_loss)
            if use_ei_loss:
                epoch_val_ei_loss = val_running_ei_loss / len(val_dro_loader)
            else:
                epoch_val_ei_loss = 0.0
            val_ei_losses.append(epoch_val_ei_loss)
            if model_type == "LSFPNet":
                epoch_val_adj_loss = val_running_adj_loss / len(val_dro_loader)
            else:
                epoch_val_adj_loss = 0.0
            val_adj_losses.append(epoch_val_adj_loss)
            if global_rank == 0 or not config['training']['multigpu']:
                writer.add_scalar('Loss/Val_MC', epoch_val_mc_loss, epoch)
                writer.add_scalar('Loss/Val_EI', epoch_val_ei_loss, epoch)
                writer.add_scalar('Loss/Val_Adj', epoch_val_adj_loss, epoch)
            if epoch % save_interval == 0:
                if global_rank == 0 or not config['training']['multigpu']:
                    train_curves = dict(
                        train_mc_losses=train_mc_losses,
                        train_ei_losses=train_ei_losses,
                        train_adj_losses=train_adj_losses,
                        weighted_train_mc_losses=weighted_train_mc_losses,
                        weighted_train_ei_losses=weighted_train_ei_losses,
                        weighted_train_adj_losses=weighted_train_adj_losses,
                    )
                    val_curves = dict(
                        val_mc_losses=val_mc_losses,
                        val_ei_losses=val_ei_losses,
                        val_adj_losses=val_adj_losses,
                    )
                    eval_curves = dict(
                        eval_ssims=eval_ssims,
                        eval_psnrs=eval_psnrs,
                        eval_mses=eval_mses,
                        eval_lpipses=eval_lpipses,
                        eval_dc_mses=eval_dc_mses,
                        eval_dc_maes=eval_dc_maes,
                        eval_curve_corrs=eval_curve_corrs
                    )
                    model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
                    save_checkpoint(model, optimizer, epoch + 1, train_curves, val_curves, eval_curves, target_w_ei, step0_train_ei_loss, epoch_train_mc_loss, model_save_path)
                    print(f'Model saved to {model_save_path}')
                    sns.set_style("whitegrid")
                    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
                    sns.lineplot(x=range(len(train_adj_losses)), y=train_adj_losses, ax=axes[0, 0])
                    axes[0, 0].set_title("Training Adjoint Loss")
                    axes[0, 0].set_xlabel("Epoch")
                    axes[0, 0].set_ylabel("Adjoint Loss")
                    sns.lineplot(x=range(len(train_mc_losses)), y=train_mc_losses, ax=axes[0, 1])
                    axes[0, 1].set_title("Training MC Loss")
                    axes[0, 1].set_xlabel("Epoch")
                    axes[0, 1].set_ylabel("MC Loss")
                    sns.lineplot(x=range(len(train_ei_losses)), y=train_ei_losses, ax=axes[0, 2])
                    axes[0, 2].set_title("Training EI Loss")
                    axes[0, 2].set_xlabel("Epoch")
                    axes[0, 2].set_ylabel("EI Loss")
                    sns.lineplot(x=range(len(val_adj_losses)), y=val_adj_losses, ax=axes[1, 0], color='orange')
                    axes[1, 0].set_title(f"Validation Adjoint Loss ({N_spokes_eval} spokes/frame)")
                    axes[1, 0].set_xlabel("Epoch")
                    axes[1, 0].set_ylabel("Adjoint Loss")
                    sns.lineplot(x=range(len(val_mc_losses)), y=val_mc_losses, ax=axes[1, 1], color='orange')
                    axes[1, 1].set_title(f"Validation MC Loss ({N_spokes_eval} spokes/frame)")
                    axes[1, 1].set_xlabel("Epoch")
                    axes[1, 1].set_ylabel("MC Loss")
                    sns.lineplot(x=range(len(val_ei_losses)), y=val_ei_losses, ax=axes[1, 2], color='orange')
                    axes[1, 2].set_title(f"Validation EI Loss ({N_spokes_eval} spokes/frame)")
                    axes[1, 2].set_xlabel("Epoch")
                    axes[1, 2].set_ylabel("EI Loss")
                    plt.tight_layout()
                    plt.savefig(os.path.join(output_dir, "losses.png"))
                    plt.close()
                    sns.set_style("whitegrid")
                    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
                    sns.lineplot(x=range(len(lambda_Ls)), y=lambda_Ls, ax=axes[0, 0])
                    axes[0, 0].set_title("Lambda_L Parameter Value")
                    axes[0, 0].set_xlabel("Epoch")
                    axes[0, 0].set_ylabel("Lambda_L")
                    sns.lineplot(x=range(len(lambda_Ss)), y=lambda_Ss, ax=axes[0, 1])
                    axes[0, 1].set_title("Lambda_S Parameter Value")
                    axes[0, 1].set_xlabel("Epoch")
                    axes[0, 1].set_ylabel("Lambda_S")
                    sns.lineplot(x=range(len(lambda_spatial_Ls)), y=lambda_spatial_Ls, ax=axes[0, 2])
                    axes[0, 2].set_title("Spatial Lambda_L Parameter Value")
                    axes[0, 2].set_xlabel("Epoch")
                    axes[0, 2].set_ylabel("Spatial Lambda_L")
                    sns.lineplot(x=range(len(lambda_spatial_Ss)), y=lambda_spatial_Ss, ax=axes[1, 0])
                    axes[1, 0].set_title("Spatial Lambda_S Parameter Value")
                    axes[1, 0].set_xlabel("Epoch")
                    axes[1, 0].set_ylabel("Spatial Lambda_S")
                    sns.lineplot(x=range(len(gammas)), y=gammas, ax=axes[1, 1])
                    axes[1, 1].set_title("Gamma Parameter Value")
                    axes[1, 1].set_xlabel("Epoch")
                    axes[1, 1].set_ylabel("Gamma")
                    sns.lineplot(x=range(len(lambda_steps)), y=lambda_steps, ax=axes[1, 2])
                    axes[1, 2].set_title("Lambda Step Parameter Value")
                    axes[1, 2].set_xlabel("Epoch")
                    axes[1, 2].set_ylabel("Lambda Step")
                    plt.tight_layout()
                    plt.savefig(os.path.join(output_dir, "parameters.png"))
                    plt.close()
                    plt.figure()
                    plt.plot(weighted_train_mc_losses, label="MC Loss")
                    plt.plot(weighted_train_ei_losses, label="EI Loss")
                    plt.plot(weighted_train_adj_losses, label="Adjoint Loss")
                    plt.xlabel("Epoch")
                    plt.ylabel("Loss")
                    plt.title("Weighted Training Losses")
                    plt.legend()
                    plt.grid(True)
                    plt.savefig(os.path.join(output_dir, "weighted_losses.png"))
                    plt.close()
                    sns.set_style("whitegrid")
                    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
                    fig.suptitle(f'Evaluation Metrics Over Epochs ({N_spokes_eval} spokes/frame)', fontsize=20)
                    sns.lineplot(x=range(len(eval_ssims)), y=eval_ssims, ax=axes[0, 0])
                    axes[0, 0].set_title("Evaluation SSIM")
                    axes[0, 0].set_xlabel("Epoch")
                    axes[0, 0].set_ylabel("SSIM")
                    sns.lineplot(x=range(len(eval_psnrs)), y=eval_psnrs, ax=axes[0, 1])
                    axes[0, 1].set_title("Evaluation PSNR")
                    axes[0, 1].set_xlabel("Epoch")
                    axes[0, 1].set_ylabel("PSNR")
                    sns.lineplot(x=range(len(eval_mses)), y=eval_mses, ax=axes[0, 2])
                    axes[0, 2].set_title("Evaluation Image MSE")
                    axes[0, 2].set_xlabel("Epoch")
                    axes[0, 2].set_ylabel("MSE")
                    sns.lineplot(x=range(len(eval_lpipses)), y=eval_lpipses, ax=axes[1, 0])
                    axes[1, 0].set_title("Evaluation LPIPS")
                    axes[1, 0].set_xlabel("Epoch")
                    axes[1, 0].set_ylabel("LPIPS")
                    sns.lineplot(x=range(len(eval_dc_maes)), y=eval_dc_maes, ax=axes[1, 1])
                    axes[1, 1].set_title("Evaluation k-space MAE")
                    axes[1, 1].set_xlabel("Epoch")
                    axes[1, 1].set_ylabel("MAE")
                    sns.lineplot(x=range(len(eval_curve_corrs)), y=eval_curve_corrs, ax=axes[1, 2])
                    axes[1, 2].set_title("Tumor Enhancement Curve Correlation")
                    axes[1, 2].set_xlabel("Epoch")
                    axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
                    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
                    plt.savefig(os.path.join(output_dir, "eval_metrics.png"))
                    plt.close()
                    plt.figure()
                    plt.plot(eval_dc_mses)
                    plt.xlabel("Epoch")
                    plt.ylabel("k-space MSE")
                    plt.title("Evaluation Data Consistency (MSE)")
                    plt.grid(True)
                    plt.savefig(os.path.join(eval_dir, "eval_dc_mses.png"))
                    plt.close()
            if global_rank == 0 or not config['training']['multigpu']:
                print(
                    f"Epoch {epoch}: Training MC Loss: {epoch_train_mc_loss:.6f}, Validation MC Loss: {epoch_val_mc_loss:.6f}"
                )
                if use_ei_loss:
                    print(
                        f"Epoch {epoch}: Training EI Loss: {epoch_train_ei_loss:.6f}, Validation EI Loss: {epoch_val_ei_loss:.6f}"
                    )
                if model_type == "LSFPNet":
                    print(
                        f"Epoch {epoch}: Training Adj Loss: {epoch_train_adj_loss:.6f}, Validation Adj Loss: {epoch_val_adj_loss:.6f}"
                    )
                print(f"--- Evaluation Metrics: Epoch {epoch} ---")
                print(f"Recon SSIM: {epoch_eval_ssim:.4f}  {np.std(epoch_eval_ssims):.4f}")
                print(f"Recon PSNR: {epoch_eval_psnr:.4f}  {np.std(epoch_eval_psnrs):.4f}")
                print(f"Recon MSE: {epoch_eval_mse:.4f}  {np.std(epoch_eval_mses):.4f}")
                print(f"Recon LPIPS: {epoch_eval_lpips:.4f}  {np.std(epoch_eval_lpipses):.4f}")
                print(f"Recon DC MSE: {epoch_eval_dc_mse:.4f}  {np.std(epoch_eval_dc_mses):.4f}")
                print(f"Recon DC MAE: {epoch_eval_dc_mae:.4f}  {np.std(epoch_eval_dc_maes):.4f}")
                print(f"Recon Enhancement Curve Correlation: {epoch_eval_curve_corr:.4f}  {np.std(epoch_eval_curve_corrs):.4f}")
                print(f"GRASP SSIM: {np.mean(grasp_ssims):.4f}  {np.std(grasp_ssims):.4f}")
                print(f"GRASP PSNR: {np.mean(grasp_psnrs):.4f}  {np.std(grasp_psnrs):.4f}")
                print(f"GRASP MSE: {np.mean(grasp_mses):.4f}  {np.std(grasp_mses):.4f}")
                print(f"GRASP LPIPS: {np.mean(grasp_lpipses):.4f}  {np.std(grasp_lpipses):.4f}")
                print(f"GRASP DC MSE: {np.mean(grasp_dc_mses):.6f}  {np.std(grasp_dc_mses):.4f}")
                print(f"GRASP DC MAE: {np.mean(grasp_dc_maes):.6f}  {np.std(grasp_dc_maes):.4f}")
                print(f"GRASP Enhancement Curve Correlation: {np.mean(grasp_curve_corrs):.6f}  {np.std(grasp_curve_corrs):.4f}")
    if global_rank == 0 or not config['training']['multigpu']:
        train_curves = dict(
            train_mc_losses=train_mc_losses,
            train_ei_losses=train_ei_losses,
            train_adj_losses=train_adj_losses,
            weighted_train_mc_losses=weighted_train_mc_losses,
            weighted_train_ei_losses=weighted_train_ei_losses,
            weighted_train_adj_losses=weighted_train_adj_losses,
        )
        val_curves = dict(
            val_mc_losses=val_mc_losses,
            val_ei_losses=val_ei_losses,
            val_adj_losses=val_adj_losses,
        )
        eval_curves = dict(
            eval_ssims=eval_ssims,
            eval_psnrs=eval_psnrs,
            eval_mses=eval_mses,
            eval_lpipses=eval_lpipses,
            eval_dc_mses=eval_dc_mses,
            eval_dc_maes=eval_dc_maes,
            eval_curve_corrs=eval_curve_corrs,
        )
        model_save_path = os.path.join(output_dir, f'{exp_name}_model.pth')
        save_checkpoint(model, optimizer, epochs + 1, train_curves, val_curves, eval_curves, target_w_ei, step0_train_ei_loss, epoch_train_mc_loss, model_save_path)
        print(f'Model saved to {model_save_path}')
        metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
        with open(metrics_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Recon', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
            writer.writerow(['DL', 
                            f'{epoch_eval_ssim:.4f}  {np.std(epoch_eval_ssims):.4f}', 
                            f'{epoch_eval_psnr:.4f}  {np.std(epoch_eval_psnrs):.4f}', 
                            f'{epoch_eval_mse:.4f}  {np.std(epoch_eval_mses):.4f}',
                            f'{epoch_eval_lpips:.4f}  {np.std(epoch_eval_lpipses):.4f}',  
                            f'{epoch_eval_dc_mse:.4f}  {np.std(epoch_eval_dc_mses):.4f}', 
                            f'{epoch_eval_dc_mae:.4f}  {np.std(epoch_eval_dc_maes):.4f}', 
                            f'{epoch_eval_curve_corr:.4f}  {np.std(epoch_eval_curve_corrs):.4f}'])
            writer.writerow(['GRASP', 
                            f'{np.mean(grasp_ssims):.4f}  {np.std(grasp_ssims):.4f}', 
                            f'{np.mean(grasp_psnrs):.4f}  {np.std(grasp_psnrs):.4f}', 
                            f'{np.mean(grasp_mses):.4f}  {np.std(grasp_mses):.4f}', 
                            f'{np.mean(grasp_lpipses):.4f}  {np.std(grasp_lpipses):.4f}', 
                            f'{np.mean(grasp_dc_mses):.4f}  {np.std(grasp_dc_mses):.4f}', 
                            f'{np.mean(grasp_dc_maes):.4f}  {np.std(grasp_dc_maes):.4f}', 
                            f'{np.mean(grasp_curve_corrs):.4f}  {np.std(grasp_curve_corrs):.4f}'])
        MAIN_EVALUATION_PLAN = [
            {
                "spokes_per_frame": 8,
                "num_frames": 36, # 8 * 36 = 288 total spokes
                "description": "High temporal resolution"
            },
            {
                "spokes_per_frame": 16,
                "num_frames": 18, # 16 * 18 = 288 total spokes
                "description": "High temporal resolution"
            },
            {
                "spokes_per_frame": 24,
                "num_frames": 12, # 24 * 12 = 288 total spokes
                "description": "Good temporal resolution"
            },
            {
                "spokes_per_frame": 32,
                "num_frames": 8, # 36 * 8 = 288 total spokes
                "description": "Standard temporal resolution"
            },
        ]
        STRESS_TEST_PLAN = [
            {
                "spokes_per_frame": 2,
                "num_frames": 144, # 2 * 144 = 288 total spokes
                "description": "Stress test: max temporal points, 2 spokes"
            },
            {
                "spokes_per_frame": 4,
                "num_frames": 72, # 4 * 72 = 288 total spokes
                "description": "Stress test: max temporal points, 4 spokes"
            },
        ]
        eval_spf_dataset = SimulatedSPFDataset(
            root_dir=config["evaluation"]["simulated_dataset_path"], 
            model_type=model_type, 
            patient_ids=val_dro_patient_ids,
            )
        eval_spf_loader = DataLoader(
            eval_spf_dataset,
            batch_size=config["dataloader"]["batch_size"],
            shuffle=False,
            num_workers=config["dataloader"]["num_workers"],
        )
        with torch.no_grad():
            spf_recon_ssim = {}
            spf_recon_psnr = {}
            spf_recon_mse = {}
            spf_recon_lpips = {}
            spf_recon_dc_mse = {}
            spf_recon_dc_mae = {}
            spf_recon_corr = {}
            spf_grasp_ssim = {}
            spf_grasp_psnr = {}
            spf_grasp_mse = {}
            spf_grasp_lpips = {}
            spf_grasp_dc_mse = {}
            spf_grasp_dc_mae = {}
            spf_grasp_corr = {}
            print("--- Running Stress Test Evaluation (Budget: 176 spokes) ---")
            for eval_config in STRESS_TEST_PLAN:
                stress_test_ssims = []
                stress_test_psnrs = []
                stress_test_mses = []
                stress_test_lpipses = []
                stress_test_dc_mses = []
                stress_test_dc_maes = []
                stress_test_corrs = []
                stress_test_grasp_ssims = []
                stress_test_grasp_psnrs = []
                stress_test_grasp_mses = []
                stress_test_grasp_lpipses = []
                stress_test_grasp_dc_mses = []
                stress_test_grasp_dc_maes = []
                stress_test_grasp_corrs = []
                spokes = eval_config["spokes_per_frame"]
                num_frames = eval_config["num_frames"]
                eval_spf_dataset.spokes_per_frame = spokes
                eval_spf_dataset.num_frames = num_frames
                eval_spf_dataset._update_sample_paths()
                for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
                    csmap = csmap.squeeze(0).to(device)   # Remove batch dim
                    ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
                    ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
                    physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
                    sim_kspace = physics(False, ground_truth, csmap)
                    kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
                    acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
                    if config['model']['encode_acceleration']:
                        acceleration_encoding = acceleration
                    else: 
                        acceleration_encoding = None
                    if config['model']['encode_time_index'] == False:
                        start_timepoint_index = None
                    else:
                        start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                    if type(grasp_img) is int or len(grasp_img.shape) == 1:
                        print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                        grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                        grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                        grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                        grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                    grasp_img = grasp_img.to(device)
                    if num_frames > eval_chunk_size:
                        print("Performing sliding window eval...")
                        x_recon, _ = sliding_window_inference(H, W, num_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch=None, device=device)  
                    else:
                        x_recon, *_ = model(
                            kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=None, norm=config['model']['norm']
                        )
                    ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
                    ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
                    ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, int(spokes), eval_dir, f"{spokes}spf", device)
                    stress_test_ssims.append(ssim)
                    stress_test_psnrs.append(psnr)
                    stress_test_mses.append(mse)
                    stress_test_lpipses.append(lpips)
                    stress_test_dc_mses.append(dc_mse)
                    stress_test_dc_maes.append(dc_mae)
                    if recon_corr is not None:
                        stress_test_corrs.append(recon_corr)
                        stress_test_grasp_corrs.append(grasp_corr)
                    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
                    stress_test_grasp_ssims.append(ssim_grasp)
                    stress_test_grasp_psnrs.append(psnr_grasp)
                    stress_test_grasp_mses.append(mse_grasp)
                    stress_test_grasp_lpipses.append(lpips_grasp)
                    stress_test_grasp_dc_mses.append(dc_mse_grasp)
                    stress_test_grasp_dc_maes.append(dc_mae_grasp)
                    spf_recon_ssim[spokes] = np.mean(stress_test_ssims)
                    spf_recon_psnr[spokes] = np.mean(stress_test_psnrs)
                    spf_recon_mse[spokes] = np.mean(stress_test_mses)
                    spf_recon_lpips[spokes] = np.mean(stress_test_lpipses)
                    spf_recon_dc_mse[spokes] = np.mean(stress_test_dc_mses)
                    spf_recon_dc_mae[spokes] = np.mean(stress_test_dc_maes)
                    spf_recon_corr[spokes] = np.mean(stress_test_corrs)
                    spf_grasp_ssim[spokes] = np.mean(stress_test_grasp_ssims)
                    spf_grasp_psnr[spokes] = np.mean(stress_test_grasp_psnrs)
                    spf_grasp_mse[spokes] = np.mean(stress_test_grasp_mses)
                    spf_grasp_lpips[spokes] = np.mean(stress_test_grasp_lpipses)
                    spf_grasp_dc_mse[spokes] = np.mean(stress_test_grasp_dc_mses)
                    spf_grasp_dc_mae[spokes] = np.mean(stress_test_grasp_dc_maes)
                    spf_grasp_corr[spokes] = np.mean(stress_test_grasp_corrs)
                for patient_id in val_patient_ids:
                    raw_kspace_path = os.path.join(config["data"]["root_dir"], f'{patient_id}_2.h5')
                    with h5py.File(raw_kspace_path, "r") as f:
                        raw_kspace = f[config["data"]["dataset_key"]][()] 
                    N_partitions = raw_kspace.shape[0]
                    raw_kspace = rearrange(raw_kspace, 'p t c sp sam -> t sp c sam p')
                    raw_kspace_flat = torch.tensor(raw_kspace).contiguous().view(total_spokes, N_coils, N_samples, N_partitions)
                    N_time = total_spokes // spokes
                    raw_kspace_binned = raw_kspace_flat.view(N_time, spokes, N_coils, N_samples, N_partitions)
                    raw_kspace_binned = rearrange(raw_kspace_binned, 't sp c sam p -> p c (sp sam) t')
                    slice_dc_mses = []
                    slice_dc_maes = []
                    grasp_slice_dc_mses = []
                    grasp_slice_dc_maes = []
                    for slice_idx, raw_kspace_slice in enumerate(raw_kspace_binned):
                        raw_kspace_slice = raw_kspace_slice.to(csmap.dtype)
                        grasp_save_path = os.path.join(os.path.dirname(os.path.dirname(config['data']['root_dir'])), f'raw_grasp/{patient_id}')
                        os.makedirs(grasp_save_path, exist_ok=True)
                        grasp_path = os.path.join(grasp_save_path, f'raw_grasp_spf{spokes}_frames{num_frames}_slice{slice_idx:03d}.npy')
                        if not os.path.exists(grasp_path):
                            print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                            grasp_img = GRASPRecon(csmap, raw_kspace_slice, spokes, num_frames, grasp_path)
                            grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                            grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                            grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                        grasp_img = grasp_img.to(device)
                        if num_frames > eval_chunk_size:
                            x_recon, _ = sliding_window_inference(H, W, num_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, eval_chunk_size, eval_chunk_overlap, raw_kspace_slice, csmap, acceleration_encoding, start_timepoint_index, model, epoch=None, device=device)  
                        else:
                            x_recon, *_ = model(
                                raw_kspace_slice.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=None, norm=config['model']['norm']
                            )
                        x_recon = to_torch_complex(x_recon)
                        sim_kspace = physics(False, x_recon, csmap)
                        print("grasp_img: ", grasp_img.shape)
                        if grasp_img.shape[1] == 2:
                            grasp_img = to_torch_complex(grasp_img)
                        if grasp_img.shape[-2] == num_frames: 
                            grasp_img = rearrange(grasp_img, 'b h t w -> b h w t')
                        print("x_recon: ", x_recon.dtype)
                        print("grasp_img: ", grasp_img.dtype)
                        print("csmap: ", csmap.dtype)
                        print("x_recon: ", x_recon.shape)
                        print("grasp_img: ", grasp_img.shape)
                        print("csmap: ", csmap.shape)
                        plot_path = f'/gpfs/data/karczmar-lab/workspaces/rachelgordon/breastMRI-recon/ddei/dl_grasp_orientation_comparison.png'
                        if not os.path.exists(plot_path):
                            timeframe = 18 
                            x_recon_timeframe = x_recon[0, :, :, timeframe]
                            grasp_img_timeframe = grasp_img[0, :, :, timeframe]
                            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
                            ax1.imshow(np.abs(x_recon_timeframe.cpu().numpy()), cmap='gray')
                            ax1.set_title(f'DL Recon - Timeframe: {timeframe}, Slice: {slice_idx}')
                            ax1.axis('off')
                            ax2.imshow(np.abs(grasp_img_timeframe.cpu().numpy()), cmap='gray')
                            ax2.set_title(f'GRASP Recon - Timeframe: {timeframe}, Slice: {slice_idx}')
                            ax2.axis('off')
                            plt.tight_layout()
                            plt.savefig(plot_path)
                            print(f"---- DL GRASP Orientation Comparsion Saved to: {plot_path} ----")
                        sim_kspace_grasp = physics(False, grasp_img.to(x_recon.dtype), csmap)
                        raw_dc_mse, raw_dc_mae = calc_dc(sim_kspace, raw_kspace_slice, device)
                        raw_grasp_dc_mse, raw_grasp_dc_mae = calc_dc(sim_kspace_grasp, raw_kspace_slice, device)
                        slice_dc_mses.append(raw_dc_mse)
                        slice_dc_maes.append(raw_dc_mae)
                        grasp_slice_dc_mses.append(raw_grasp_dc_mse)
                        grasp_slice_dc_maes.append(raw_grasp_dc_mae)
                    avg_dc_mse = np.mean(slice_dc_mses)
                    avg_dc_mae = np.mean(slice_dc_maes)
                    avg_grasp_dc_mse = np.mean(grasp_slice_dc_mses)
                    avg_grasp_dc_mae = np.mean(grasp_slice_dc_maes)
                    print(f"Avg DL DC MSE: {avg_dc_mse}")
                    print(f"Avg DL DC MAE: {avg_dc_mae}")
                    print(f"Avg GRASP DC MSE: {avg_grasp_dc_mse}")
                    print(f"Avg GRASP DC MAE: {avg_grasp_dc_mae}")
                spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
                with open(spf_metrics_path, 'a', newline='') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', "LPIPS", 'DC MSE', 'DC MAE', 'EC Correlation'])
                    writer.writerow(['DL', spokes, 
                    f'{np.mean(stress_test_ssims):.4f}  {np.std(stress_test_ssims):.4f}', 
                    f'{np.mean(stress_test_psnrs):.4f}  {np.std(stress_test_psnrs):.4f}', 
                    f'{np.mean(stress_test_mses):.4f}  {np.std(stress_test_mses):.4f}', 
                    f'{np.mean(stress_test_lpipses):.4f}  {np.std(stress_test_lpipses):.4f}', 
                    f'{np.mean(stress_test_dc_mses):.4f}  {np.std(stress_test_dc_mses):.4f}',
                    f'{np.mean(stress_test_dc_maes):.4f}  {np.std(stress_test_dc_maes):.4f}',
                    f'{np.mean(stress_test_corrs):.4f}  {np.std(stress_test_corrs):.4f}'
                    ])
                    writer.writerow(['GRASP', spokes, 
                    f'{np.mean(stress_test_grasp_ssims):.4f}  {np.std(stress_test_grasp_ssims):.4f}', 
                    f'{np.mean(stress_test_grasp_psnrs):.4f}  {np.std(stress_test_grasp_psnrs):.4f}', 
                    f'{np.mean(stress_test_grasp_mses):.4f}  {np.std(stress_test_grasp_mses):.4f}', 
                    f'{np.mean(stress_test_grasp_lpipses):.4f}  {np.std(stress_test_grasp_lpipses):.4f}', 
                    f'{np.mean(stress_test_grasp_dc_mses):.4f}  {np.std(stress_test_grasp_dc_mses):.4f}',
                    f'{np.mean(stress_test_grasp_dc_maes):.4f}  {np.std(stress_test_grasp_dc_maes):.4f}',
                    f'{np.mean(stress_test_grasp_corrs):.4f}  {np.std(stress_test_grasp_corrs):.4f}',
                    ])
            print("--- Running Main Evaluation (Budget: 320 spokes) ---")
            for eval_config in MAIN_EVALUATION_PLAN:
                spf_eval_ssims = []
                spf_eval_psnrs = []
                spf_eval_mses = []
                spf_eval_lpipses = []
                spf_eval_dc_mses = []
                spf_eval_dc_maes = []
                spf_eval_curve_corrs = []
                spf_grasp_ssims = []
                spf_grasp_psnrs = []
                spf_grasp_mses = []
                spf_grasp_lpipses = []
                spf_grasp_dc_mses = []
                spf_grasp_dc_maes = []
                spf_grasp_curve_corrs = []
                spokes = eval_config["spokes_per_frame"]
                num_frames = eval_config["num_frames"]
                eval_spf_dataset.spokes_per_frame = spokes
                eval_spf_dataset.num_frames = num_frames
                eval_spf_dataset._update_sample_paths()
                for csmap, ground_truth, grasp_img, mask, grasp_path in tqdm(eval_spf_loader, desc="Variable Spokes Per Frame Evaluation"):
                    csmap = csmap.squeeze(0).to(device)   # Remove batch dim
                    ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)
                    ktraj, dcomp, nufft_ob, adjnufft_ob = prep_nufft(N_samples, spokes, num_frames)
                    physics = MCNUFFT(nufft_ob.to(device), adjnufft_ob.to(device), ktraj.to(device), dcomp.to(device))
                    sim_kspace = physics(False, ground_truth, csmap)
                    kspace = sim_kspace.squeeze(0).to(device) # Remove batch dim
                    acceleration = torch.tensor([N_full / int(spokes)], dtype=torch.float, device=device)
                    if config['model']['encode_acceleration']:
                        acceleration_encoding = acceleration
                    else: 
                        acceleration_encoding = None
                    if config['model']['encode_time_index'] == False:
                        start_timepoint_index = None
                    else:
                        start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)
                    if type(grasp_img) is int or len(grasp_img.shape) == 1:
                        print(f"No GRASP file found, performing reconstruction with {spokes} spokes/frame and {num_frames} frames.")
                        grasp_img = GRASPRecon(csmap, sim_kspace, spokes, num_frames, grasp_path[0])
                        grasp_recon_torch = torch.from_numpy(grasp_img).permute(2, 0, 1) # T, H, W
                        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
                        grasp_img = torch.flip(grasp_recon_torch, dims=[-3])
                        grasp_img = torch.rot90(grasp_img, k=3, dims=[-3,-1]).unsqueeze(0)
                    grasp_img = grasp_img.to(device)
                    if num_frames > eval_chunk_size:
                        print("Performing sliding window eval...")
                        x_recon, _ = sliding_window_inference(H, W, num_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, eval_chunk_size, eval_chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch=None, device=device)  
                    else:
                        x_recon, *_ = model(
                        kspace.to(device), physics, csmap, acceleration_encoding, start_timepoint_index, epoch=None, norm=config['model']['norm']
                        )
                    ground_truth = torch.stack([ground_truth.real, ground_truth.imag], dim=1)
                    ground_truth = rearrange(ground_truth, 'b i h w t -> b i t h w')
                    ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, int(spokes), eval_dir, f'{spokes}spf', device)
                    spf_eval_ssims.append(ssim)
                    spf_eval_psnrs.append(psnr)
                    spf_eval_mses.append(mse)
                    spf_eval_lpipses.append(lpips)
                    spf_eval_dc_mses.append(dc_mse)
                    spf_eval_dc_maes.append(dc_mae)
                    if recon_corr is not None:
                        spf_eval_curve_corrs.append(recon_corr)
                        spf_grasp_curve_corrs.append(grasp_corr)
                    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(kspace, csmap, ground_truth, grasp_img, physics, device, eval_dir)
                    spf_grasp_ssims.append(ssim_grasp)
                    spf_grasp_psnrs.append(psnr_grasp)
                    spf_grasp_mses.append(mse_grasp)
                    spf_grasp_lpipses.append(lpips_grasp)
                    spf_grasp_dc_mses.append(dc_mse_grasp)
                    spf_grasp_dc_maes.append(dc_mae_grasp)
                spf_recon_ssim[spokes] = np.mean(spf_eval_ssims)
                spf_recon_psnr[spokes] = np.mean(spf_eval_psnrs)
                spf_recon_mse[spokes] = np.mean(spf_eval_mses)
                spf_recon_lpips[spokes] = np.mean(spf_eval_lpipses)
                spf_recon_dc_mse[spokes] = np.mean(spf_eval_dc_mses)
                spf_recon_dc_mae[spokes] = np.mean(spf_eval_dc_maes)
                spf_recon_corr[spokes] = np.mean(spf_eval_curve_corrs)
                spf_grasp_ssim[spokes] = np.mean(spf_grasp_ssims)
                spf_grasp_psnr[spokes] = np.mean(spf_grasp_psnrs)
                spf_grasp_mse[spokes] = np.mean(spf_grasp_mses)
                spf_grasp_lpips[spokes] = np.mean(spf_grasp_lpipses)
                spf_grasp_dc_mse[spokes] = np.mean(spf_grasp_dc_mses)
                spf_grasp_dc_mae[spokes] = np.mean(spf_grasp_dc_maes)
                spf_grasp_corr[spokes] = np.mean(spf_grasp_curve_corrs)
                spf_metrics_path = os.path.join(eval_dir, "eval_metrics.csv")
                with open(spf_metrics_path, 'a', newline='') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(['Recon', 'Spokes Per Frame', 'SSIM', 'PSNR', 'MSE', 'LPIPS', 'DC MSE', 'DC MAE', 'EC Correlation'])
                    writer.writerow(['DL', spokes, 
                    f'{np.mean(spf_eval_ssims):.4f}  {np.std(spf_eval_ssims):.4f}', 
                    f'{np.mean(spf_eval_psnrs):.4f}  {np.std(spf_eval_psnrs):.4f}', 
                    f'{np.mean(spf_eval_mses):.4f}  {np.std(spf_eval_mses):.4f}', 
                    f'{np.mean(spf_eval_lpipses):.4f}  {np.std(spf_eval_lpipses):.4f}', 
                    f'{np.mean(spf_eval_dc_mses):.4f}  {np.std(spf_eval_dc_mses):.4f}',
                    f'{np.mean(spf_eval_dc_maes):.4f}  {np.std(spf_eval_dc_maes):.4f}',
                    f'{np.mean(spf_eval_curve_corrs):.4f}  {np.std(spf_eval_curve_corrs):.4f}'
                    ])
                    writer.writerow(['GRASP', spokes, 
                    f'{np.mean(spf_grasp_ssims):.4f}  {np.std(spf_grasp_ssims):.4f}', 
                    f'{np.mean(spf_grasp_psnrs):.4f}  {np.std(spf_grasp_psnrs):.4f}', 
                    f'{np.mean(spf_grasp_mses):.4f}  {np.std(spf_grasp_mses):.4f}', 
                    f'{np.mean(spf_grasp_lpipses):.4f}  {np.std(spf_grasp_lpipses):.4f}', 
                    f'{np.mean(spf_grasp_dc_mses):.4f}  {np.std(spf_grasp_dc_mses):.4f}',
                    f'{np.mean(spf_grasp_dc_maes):.4f}  {np.std(spf_grasp_dc_maes):.4f}',
                    f'{np.mean(spf_grasp_curve_corrs):.4f}  {np.std(spf_grasp_curve_corrs):.4f}'
                    ])
        sns.set_style("whitegrid")
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        sns.lineplot(x=list(spf_recon_ssim.keys()), 
                    y=list(spf_recon_ssim.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[0, 0])
        sns.lineplot(x=list(spf_grasp_ssim.keys()), 
                    y=list(spf_grasp_ssim.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[0, 0])
        axes[0, 0].set_title("Evaluation SSIM vs Spokes/Frame")
        axes[0, 0].set_xlabel("Spokes per Frame")
        axes[0, 0].set_ylabel("SSIM")
        sns.lineplot(x=list(spf_recon_psnr.keys()), 
                    y=list(spf_recon_psnr.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[0, 1])
        sns.lineplot(x=list(spf_grasp_psnr.keys()), 
                    y=list(spf_grasp_psnr.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[0, 1])
        axes[0, 1].set_title("Evaluation PSNR vs Spokes/Frame")
        axes[0, 1].set_xlabel("Spokes per Frame")
        axes[0, 1].set_ylabel("PSNR")
        sns.lineplot(x=list(spf_recon_mse.keys()), 
                    y=list(spf_recon_mse.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[0, 2])
        sns.lineplot(x=list(spf_grasp_mse.keys()), 
                    y=list(spf_grasp_mse.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[0, 2])
        axes[0, 2].set_title("Evaluation Image MSE vs Spokes/Frame")
        axes[0, 2].set_xlabel("Spokes per Frame")
        axes[0, 2].set_ylabel("MSE")
        sns.lineplot(x=list(spf_recon_lpips.keys()), 
                    y=list(spf_recon_lpips.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[1, 0])
        sns.lineplot(x=list(spf_grasp_lpips.keys()), 
                    y=list(spf_grasp_lpips.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[1, 0])
        axes[1, 0].set_title("Evaluation LPIPS vs Spokes/Frame")
        axes[1, 0].set_xlabel("Spokes per Frame")
        axes[1, 0].set_ylabel("LPIPS")
        sns.lineplot(x=list(spf_recon_dc_mae.keys()), 
                    y=list(spf_recon_dc_mae.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[1, 1])
        sns.lineplot(x=list(spf_grasp_dc_mae.keys()), 
                    y=list(spf_grasp_dc_mae.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[1, 1])
        axes[1, 1].set_title("Evaluation k-space MAE vs Spokes/Frame")
        axes[1, 1].set_xlabel("Spokes per Frame")
        axes[1, 1].set_ylabel("MAE")
        sns.lineplot(x=list(spf_recon_corr.keys()), 
                    y=list(spf_recon_corr.values()), 
                    label="DL Recon", 
                    marker='o',
                    ax=axes[1, 2])
        sns.lineplot(x=list(spf_grasp_corr.keys()), 
                    y=list(spf_grasp_corr.values()), 
                    label="Standard Recon", 
                    marker='o',
                    ax=axes[1, 2])
        axes[1, 2].set_title("Tumor Enhancement Curve Correlation vs Spokes/Frame")
        axes[1, 2].set_xlabel("Spokes per Frame")
        axes[1, 2].set_ylabel("Pearson Correlation Coefficient")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "spf_eval_metrics.png"))
        plt.close()
    if global_rank == 0 or not config['training']['multigpu']:
        writer.close()
    cleanup()
if __name__ == '__main__':
    main()

=== lsfpnet_encoding.py ===
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from lsp import Project_inf, Wxs, Wtxs
from time import time
from einops import rearrange
import matplotlib.pyplot as plt
import numpy as np
dtype = torch.complex64
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def from_torch_complex(x: torch.Tensor):
    """(B, ...) complex -> (B, 2, ...) real"""
    return rearrange(torch.view_as_real(x), "b ... c -> b c ...").contiguous()
def _realify(a: torch.Tensor) -> torch.Tensor:
    """2x2 real block representation of complex matrix
    returns R(a) = [[Re a, -Im a], [Im a, Re a]] with shape (2m, 2n)
    """
    ar = torch.view_as_real(a)                              # (m, n, 2)
    x = ar[..., 0]                                          # Re
    y = ar[..., 1]                                          # Im
    top = torch.cat([x, -y], dim=-1)                        # (m, 2n)
    bot = torch.cat([y,  x], dim=-1)                        # (m, 2n)
    return torch.cat([top, bot], dim=-2)                    # (2m, 2n)
def _de_realify(r: torch.Tensor) -> torch.Tensor:
    """inverse of _realify; expects r with shape (2m, 2n) laid out as [[X, -Y], [Y, X]]"""
    m2, n2 = r.shape[-2], r.shape[-1]
    m, n = m2 // 2, n2 // 2
    x = r[..., :m, :n]
    y = r[..., m:, :n]
    return x + 1j * y
class MappingNetwork(nn.Module):
    """Maps a scalar input to a style vector using a simple MLP."""
    def __init__(self, style_dim, channels, num_layers=4):
        super().__init__()
        layers = [nn.Linear(channels, style_dim), nn.ReLU(True)]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(style_dim, style_dim), nn.ReLU(True)])
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        if x.dim() == 0:
            x = x.unsqueeze(0)
        if x.dim() == 1:
            x = x.unsqueeze(1)
        return self.net(x)
class BasicBlock(nn.Module):
    """
    one unrolled LS+S block with:
      - nuclear-norm prox via SVD (two modes: 'detached_uv' or 'mag')
      - positivity via softplus on all hyper-parameters
      - optional hard low-k projection inside the DC gradient
      - FiLM conditioning with identity init and bounded modulation
    Args:
        lambdas: dict with keys {'lambda_L','lambda_S','lambda_spatial_L','lambda_spatial_S','gamma','lambda_step'}
        channels: conv width
        style_dim: FiLM MLP latent dim
        svd_mode: 'detached_uv' (recommended) or 'mag' (real-SVD on |Z|)
        use_lowk_dc: if True, replace low-k residual with measured data inside gradient
        lowk_frac: fraction of radii treated as low-k (e.g., 0.100.15)
        lowk_alpha: blend for low-k (1.0 hard replace, 0.9 soft blend)
        film_bounded: if True, use tanh-bounded modulation; else raw scale+1, bias
        film_gain: magnitude of modulation when film_bounded=True
        film_identity_init: if True, FiLM heads are zero-initialized (identity)
        svd_mag_noise_std: optional noise std added to |Z| in 'mag' mode (0 means none)
    """
    def __init__(
        self,
        lambdas,
        channels=32,
        style_dim=128,
        svd_mode: str = "detached_uv",
        use_lowk_dc: bool = True,
        lowk_frac: float = 0.10,
        lowk_alpha: float = 0.7,
        film_bounded: bool = True,
        film_gain: float = 0.10,
        film_identity_init: bool = True,
        svd_noise_std: float = 0.0,
        film_L: bool = True,
    ):
        super().__init__()
        self.channels = channels
        self.style_dim = style_dim
        self.film_L = film_L
        self.lowk_alpha = lowk_alpha
        self.lowk_radius_frac = lowk_frac
        self.lambda_L        = nn.Parameter(torch.tensor([lambdas['lambda_L']]))
        self.lambda_S        = nn.Parameter(torch.tensor([lambdas['lambda_S']]))
        self.lambda_spatial_L= nn.Parameter(torch.tensor([lambdas['lambda_spatial_L']]))
        self.lambda_spatial_S= nn.Parameter(torch.tensor([lambdas['lambda_spatial_S']]))
        self.gamma           = nn.Parameter(torch.tensor([lambdas['gamma']]))
        self.lambda_step     = nn.Parameter(torch.tensor([lambdas['lambda_step']]))
        if self.film_L:
            self.style_injector_L = nn.Linear(self.style_dim, self.channels * 2)
        self.style_injector_S = nn.Linear(self.style_dim, self.channels * 2)
        if film_identity_init:
            if self.film_L:
                init.zeros_(self.style_injector_L.weight); init.zeros_(self.style_injector_L.bias)
            init.zeros_(self.style_injector_S.weight); init.zeros_(self.style_injector_S.bias)
        self.conv1_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_l = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.conv1_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, 1, 3, 3, 3)))
        self.conv2_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_forward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv1_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv2_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(self.channels, self.channels, 3, 3, 3)))
        self.conv3_backward_s = nn.Parameter(init.xavier_normal_(torch.Tensor(1, self.channels, 3, 3, 3)))
        self.svd_mode          = svd_mode
        self.use_lowk_dc       = use_lowk_dc
        self.lowk_frac         = lowk_frac
        self.lowk_alpha        = lowk_alpha
        self.film_bounded      = film_bounded
        self.film_gain         = film_gain
        self.svd_noise_std = svd_noise_std
    @staticmethod
    def _film(x, style_head, style_embedding, bounded: bool, gain: float):
        """apply FiLM modulation; identity at init; bounded if requested"""
        params = style_head(style_embedding)
        scale_raw, bias_raw = params.chunk(2, dim=-1)
        if bounded:
            scale = gain * torch.tanh(scale_raw)
            bias  = gain * torch.tanh(bias_raw)
            scale = scale.view(1, -1, 1, 1, 1)
            bias  = bias.view(1, -1, 1, 1, 1)
            return F.relu(x * (1.0 + scale) + bias)
        else:
            scale = scale_raw.view(1, -1, 1, 1, 1)
            bias  = bias_raw.view(1, -1, 1, 1, 1)
            return F.relu(x * (scale + 1.0) + bias)
    @staticmethod
    def _lowk_project(k_pred: torch.Tensor, y: torch.Tensor, ktraj: torch.Tensor, frac: float, alpha: float):
        """replace (or blend) low-k samples with measurements (vectorized, complex-safe)"""
        r = (ktraj[0]**2 + ktraj[1]**2).sqrt()
        thr = torch.quantile(r.reshape(-1), frac)
        M = (r <= thr)
        M = rearrange(M, 's t -> 1 s t')  # broadcast over coils
        return torch.where(M, alpha * y + (1.0 - alpha) * k_pred, k_pred)
    def forward(self, M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmaps, style_embedding=None):
        """
        runs one LS+S iteration
        inputs are complex in (nx, ny, nt) except p_*, pt_* which are packed as in your code
        returns same tuple as your original implementation
        """
        gamma          = F.softplus(self.gamma) + 1e-6
        lambda_step    = F.softplus(self.lambda_step) + 1e-6
        lambda_L_eff   = F.softplus(self.lambda_L) + 1e-8
        lambda_S_eff   = F.softplus(self.lambda_S) + 1e-8
        lam_sp_L_eff   = F.softplus(self.lambda_spatial_L) + 1e-8
        lam_sp_S_eff   = F.softplus(self.lambda_spatial_S) + 1e-8
        c = lambda_step / gamma
        nx, ny, nt = M0.size()
        x_sum  = torch.reshape(L + S, [nx, ny, nt])
        k_pred = param_E(inv=False, data=x_sum, smaps=csmaps)
        residual = k_pred - param_d  
        if self.use_lowk_dc:
            k = param_E.ktraj.to(param_d.device)                                         # (2, samples, t) or (2, samples)
            if k.dim() == 3:
                kr = (k[0]**2 + k[1]**2).sqrt()                                          # (samples, t)
                r0 = self.lowk_radius_frac * kr.max()
                mask = (kr <= r0).to(residual.real.dtype)                                 # 1 inside ball
                gate = 1.0 - self.lowk_alpha * rearrange(mask, 's t -> 1 s t')                # broadcast over coils
            else:                                                                         # singleframe edge case
                kr = (k[0]**2 + k[1]**2).sqrt()                                          # (samples,)
                r0 = self.lowk_radius_frac * kr.max()
                mask = (kr <= r0).to(residual.real.dtype)
                gate = 1.0 - self.lowk_alpha * rearrange(mask, 's -> 1 s 1')
            residual = residual * gate
        gradient = param_E(inv=True, data=residual, smaps=csmaps)  
        gradient = torch.reshape(gradient, [nx * ny, nt])
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = rearrange(pb_L.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)  # already flattened
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        y_L = L - gamma * gradient - gamma * pt_L - gamma * pb_L
        Z = c * y_L + pt_L  # (nx*ny, nt) complex
        if self.svd_mode == "detached_uv":
            U, Svals, Vh = torch.linalg.svd(Z, full_matrices=False)
            U_d, Vh_d = U.detach(), Vh.detach()
            S_shrunk = Project_inf(Svals, lambda_L_eff)
            pt_L = U_d @ torch.diag_embed(S_shrunk) @ Vh_d
        elif self.svd_mode == "mag":
            mag   = Z.abs() + 1e-8
            phase = Z / mag
            if self.svd_noise_std > 0.0:
                mag = mag + torch.randn_like(mag) * self.svd_noise_std
            U, Svals, Vh = torch.linalg.svd(mag, full_matrices=False)
            S_shrunk = Project_inf(Svals, lambda_L_eff, to_complex=False)
            pt_L_mag = U @ torch.diag_embed(S_shrunk) @ Vh
            pt_L = pt_L_mag * phase
        elif self.svd_mode == "real":
            R = _realify(Z)                                     # (2M, 2N) real
            print("noise std: ", self.svd_noise_std)
            if self.svd_noise_std > 0.0:
                print("adding noise...")
                R = R + self.svd_noise_std * torch.randn_like(R)
            Ur, Sr, VrT = torch.linalg.svd(R, full_matrices=False)
            Sr_shrunk = Project_inf(Sr, self.lambda_L, to_complex=False)        # same tau as complex case
            R_prox = Ur @ torch.diag_embed(Sr_shrunk) @ VrT
            pt_L = _de_realify(R_prox) 
        else:
            raise ValueError(f"unsupported svd_mode: {self.svd_mode}")
        tL_in = from_torch_complex(y_L)
        tL_in  = rearrange(tL_in, '(nx ny) two nt -> two 1 nx ny nt', nx=nx, ny=ny)
        tL     = F.conv3d(tL_in, self.conv1_forward_l, padding=1); tL = F.relu(tL)
        tL     = F.conv3d(tL,    self.conv2_forward_l, padding=1)
        if style_embedding is not None and self.film_L:
            tL = self._film(tL, self.style_injector_L, style_embedding, self.film_bounded, self.film_gain)
        else:
            tL = F.relu(tL)
        tL_out = F.conv3d(tL, self.conv3_forward_l, padding=1)
        tL_out_c = tL_out + p_L
        tL_out_c = tL_out_c[0, :, :, :, :] + 1j * tL_out_c[1, :, :, :, :]
        p_L = Project_inf(c * tL_out_c, lam_sp_L_eff)
        p_L = from_torch_complex(p_L)
        p_L = rearrange(p_L, 'ch two nx ny nt -> two ch nx ny nt')
        pb_L = F.conv3d(p_L, self.conv1_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L = F.conv3d(pb_L, self.conv2_backward_l, padding=1); pb_L = F.relu(pb_L)
        pb_L_out = F.conv3d(pb_L, self.conv3_backward_l, padding=1)
        pb_L = rearrange(pb_L_out.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_L = pb_L[0, :, :] + 1j * pb_L[1, :, :]
        L = L - gamma * gradient - gamma * pt_L - gamma * pb_L
        adjloss_L = tL_out * p_L - pb_L_out * tL_in
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = rearrange(pb_S.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        y_S  = S - gamma * gradient - gamma * Wtxs(pt_S) - gamma * pb_S
        pt_S = Project_inf(c * Wxs(y_S) + pt_S, lambda_S_eff)
        tS_in = from_torch_complex(y_S)
        tS_in  = rearrange(tS_in, '(nx ny) two nt -> two 1 nx ny nt', nx=nx, ny=ny)
        tS     = F.conv3d(tS_in, self.conv1_forward_s, padding=1); tS = F.relu(tS)
        tS     = F.conv3d(tS,    self.conv2_forward_s, padding=1)
        if style_embedding is not None:
            tS = self._film(tS, self.style_injector_S, style_embedding, self.film_bounded, self.film_gain)
        else:
            tS = F.relu(tS)
        tS_out = F.conv3d(tS, self.conv3_forward_s, padding=1)
        tS_out_c = tS_out + p_S
        tS_out_c = tS_out_c[0, :, :, :, :] + 1j * tS_out_c[1, :, :, :, :]
        p_S = Project_inf(c * tS_out_c, lam_sp_S_eff)
        p_S = from_torch_complex(p_S)
        p_S = rearrange(p_S, 'ch two nx ny nt -> two ch nx ny nt')
        pb_S = F.conv3d(p_S, self.conv1_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S = F.conv3d(pb_S, self.conv2_backward_s, padding=1); pb_S = F.relu(pb_S)
        pb_S_out = F.conv3d(pb_S, self.conv3_backward_s, padding=1)
        pb_S = rearrange(pb_S_out.squeeze(), 'two nx ny nt -> two (nx ny) nt', nx=nx, ny=ny)
        pb_S = pb_S[0, :, :] + 1j * pb_S[1, :, :]
        S = S - gamma * gradient - gamma * Wtxs(pt_S) - gamma * pb_S
        adjloss_S = tS_out * p_S - pb_S_out * tS_in
        return [
            L, S, adjloss_L, adjloss_S, pt_L, pt_S, p_L, p_S,
            lambda_L_eff, lambda_S_eff, lam_sp_L_eff, lam_sp_S_eff, gamma, lambda_step
        ]
class LSFPNet(nn.Module):
    def __init__(self, 
                 LayerNo: int, 
                 lambdas: dict, 
                 channels: int = 32, 
                 style_dim: int = 128,
                 svd_mode: str = "detached_uv",
                 use_lowk_dc: bool = True,
                 lowk_frac: float = 0.125,
                 lowk_alpha: float = 1.0,
                 film_bounded: bool = True,
                 film_gain: float = 0.10,
                 film_identity_init: bool = True,
                 svd_noise_std: float = 0.0,
                 film_L: bool = True,
        ):
        super(LSFPNet, self).__init__()
        onelayer = []
        self.LayerNo = LayerNo
        self.channels = channels
        self.style_dim = style_dim
        for ii in range(LayerNo):
            onelayer.append(BasicBlock(lambdas=lambdas, 
                                       channels=self.channels, 
                                       style_dim=style_dim,
                                       svd_mode=svd_mode,
                                       use_lowk_dc=use_lowk_dc,
                                       lowk_frac=lowk_frac,
                                       lowk_alpha=lowk_alpha,
                                       film_bounded=film_bounded,
                                       film_gain=film_gain,
                                       film_identity_init=film_identity_init,
                                       svd_noise_std=svd_noise_std,
                                       film_L=film_L,
                                       ))
        self.fcs = nn.ModuleList(onelayer)
    def plot_block_output(self, M0, L, S, iter, epoch, output_dir):
        time_frame_index = 3
        nx, ny, nt = M0.size()
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        output_image = L + S
        fig, axes = plt.subplots(1, 4, figsize=(24, 6))
        fig.suptitle(f"Basic Block Output at Time Frame {time_frame_index} and Iteration {iter}", fontsize=20)
        axes[0].imshow(np.abs(M0[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[0].set_title("Input Image")
        axes[0].axis("off")
        axes[1].imshow(np.abs(L[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[1].set_title("Background Component (L)")
        axes[1].axis("off")
        axes[2].imshow(np.abs(S[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[2].set_title("Dynamic Component (S)")
        axes[2].axis("off")
        axes[3].imshow(np.abs(output_image[..., time_frame_index].cpu().detach().numpy()), cmap='gray')
        axes[3].set_title("Combined Image (L + S)")
        axes[3].axis("off")
        filename = os.path.join(output_dir, f'basic_block_output_{epoch}_iter{iter}.png')
        plt.savefig(filename)
        plt.close()
    def forward(self, M0, param_E, param_d, csmap, epoch, output_dir, style_embedding=None):
        nx, ny, nt = M0.size()
        L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_L = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        pt_S = torch.zeros([nx * ny, nt], dtype=dtype).to(param_d.device)
        p_L = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        p_S = torch.zeros([2, self.channels, nx, ny, nt], dtype=torch.float32).to(param_d.device)
        layers_adj_L = []
        layers_adj_S = []
        for ii in range(self.LayerNo):
            [L, S, layer_adj_L, layer_adj_S, pt_L, pt_S, p_L, p_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step] = self.fcs[ii](M0, param_E, param_d, L, S, pt_L, pt_S, p_L, p_S, csmap, style_embedding)
            layers_adj_L.append(layer_adj_L)
            layers_adj_S.append(layer_adj_S)
            self.plot_block_output(M0, L, S, iter=ii, epoch=epoch, output_dir=output_dir)
        L = torch.reshape(L, [nx, ny, nt])
        S = torch.reshape(S, [nx, ny, nt])
        return [L, S, layers_adj_L, layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step]
class ArtifactRemovalLSFPNet(nn.Module):
    def __init__(self, backbone_net, output_dir, channels, **kwargs):
        super(ArtifactRemovalLSFPNet, self).__init__()
        self.backbone_net = backbone_net
        self.output_dir = output_dir
        self.style_dim = 128  # You can tune this hyperparameter
        self.mapping_network = MappingNetwork(style_dim=self.style_dim, channels=channels)
    @staticmethod
    def _normalise_both(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf.abs().max() + 1e-8                     # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_baseline(zf: torch.Tensor, data: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = zf[..., 0].abs().mean() + 1e-8                     # scalar, grads OK
        return zf / scale, data / scale, scale
    @staticmethod
    def _normalise_indep(x: torch.Tensor):
        """
        Per-dynamic-series max-magnitude scaling (paper default).
        Both `zf` (image) and `data` (k-space) share the SAME scalar.
        """
        scale = torch.quantile(x.abs(), 0.99) + 1e-6
        if scale < 1e-6: # Handle case where input is all zeros
             scale = 1.0
        return x / scale, scale
    def forward(self, y, E, csmap, acceleration=None, start_timepoint_index=None, epoch=None, norm="both", **kwargs):
        x_init = E(inv=True, data=y, smaps=csmap)
        if norm =="both":
            x_init_norm, y_norm, scale = self._normalise_both(x_init, y)
        elif norm == "independent":
            x_init_norm, scale = self._normalise_indep(x_init)
            y_norm, scale_y = self._normalise_indep(y)
        elif norm == "baseline":
            x_init_norm, y_norm, scale = self._normalise_baseline(x_init, y)
        elif norm == "none":
            x_init_norm = x_init
            y_norm = y
            scale = 1.0
        if acceleration or start_timepoint_index:                                          # already small
            if start_timepoint_index is not None:
                T = x_init_norm.shape[-1]
                start_frac = (start_timepoint_index / max(T - 1, 1)).view(-1, 1)
                if acceleration is not None:
                    H = x_init_norm.shape[-2]
                    N_full = H * np.pi / 2.0
                    inv_af = (1.0 / acceleration.clamp_min(1e-6)).view(-1, 1)          # smaller numbers are safer
                    spf_est = (N_full / acceleration.clamp_min(1e-6)).view(-1, 1)      # useful too
                    inv_af_feat = inv_af   
                    combined_input = torch.cat([inv_af_feat, start_frac], dim=1).to(x_init_norm.device)
                else:
                    combined_input = start_frac
            else:
                H = x_init_norm.shape[-2]
                N_full = H * np.pi / 2.0
                inv_af = (1.0 / acceleration.clamp_min(1e-6)).view(-1, 1)          # smaller numbers are safer
                spf_est = (N_full / acceleration.clamp_min(1e-6)).view(-1, 1)      # useful too
                inv_af_feat = inv_af   
                combined_input = inv_af_feat
            style_embedding = self.mapping_network(combined_input)
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir, style_embedding)
        else:
            L, S, loss_layers_adj_L, loss_layers_adj_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step  = self.backbone_net(x_init_norm, E, y_norm, csmap, epoch, self.output_dir)
        loss_constraint_L = torch.square(torch.mean(loss_layers_adj_L[0])) / self.backbone_net.LayerNo
        loss_constraint_S = torch.square(torch.mean(loss_layers_adj_S[0])) / self.backbone_net.LayerNo
        for k in range(self.backbone_net.LayerNo - 1):
            loss_constraint_S += torch.square(torch.mean(loss_layers_adj_S[k + 1])) / self.backbone_net.LayerNo
            loss_constraint_L += torch.square(torch.mean(loss_layers_adj_L[k + 1])) / self.backbone_net.LayerNo
        recon = (L + S) * scale                 # rescale to original units
        x_hat = torch.stack((recon.real, recon.imag), dim=0).unsqueeze(0)  # (B,2,H,W,T)
        return x_hat, loss_constraint_L + loss_constraint_S, lambda_L, lambda_S, lambda_spatial_L, lambda_spatial_S, gamma, lambda_step

=== radial_lsfp.py ===
import torch
import torch.nn as nn
import numpy as np
from time import time
dtype = torch.complex64
class MCNUFFT(nn.Module):
    def __init__(self, nufft_ob, adjnufft_ob, ktraj, dcomp):
        super(MCNUFFT, self).__init__()
        self.nufft_ob = nufft_ob
        self.adjnufft_ob = adjnufft_ob
        self.ktraj = torch.squeeze(ktraj)
        self.dcomp = torch.squeeze(dcomp)
    def forward(self, inv, data, smaps):
        data = torch.squeeze(data)
        Nx, Ny = smaps.shape[2], smaps.shape[3]
        if len(data.shape) > 2:  # multi-frame
            is_complex_data = torch.is_complex(data)
            if inv: # Adjoint NUFFT (k-space -> image)
                kd = data.permute(2, 0, 1) # -> [time, coils, samples]
                d = self.dcomp.permute(1, 0) # -> [time, samples]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                d = d.unsqueeze(1) 
                x_temp = self.adjnufft_ob(kd * d, k, smaps=smaps.to(dtype))
                x = x_temp.squeeze(1).permute(1, 2, 0) / np.sqrt(Nx * Ny)
            else: # Forward NUFFT (image -> k-space)
                image = data.permute(2, 0, 1).unsqueeze(1) # -> [time, 1, Nx, Ny]
                k = self.ktraj.permute(2, 0, 1) # -> [time, samples, 2]
                x_temp = self.nufft_ob(image, k, smaps=smaps)
                x = x_temp.permute(1, 2, 0) / np.sqrt(Nx * Ny)
        else:  # single frame (original logic is fine)
            if inv:
                kd = data.unsqueeze(0)
                d = self.dcomp.unsqueeze(0).unsqueeze(0)
                x = self.adjnufft_ob(kd * d, self.ktraj, smaps=smaps.to(dtype))
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
            else:
                image = data.unsqueeze(0).unsqueeze(0)
                x = self.nufft_ob(image, self.ktraj, smaps=smaps)
                x = torch.squeeze(x) / np.sqrt(Nx * Ny)
        return x

=== mc.py ===
from typing import Union
import torch
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from radial import to_torch_complex
class MCLoss(Loss):
    r"""
    Measurement consistency loss
    This loss enforces that the reconstructions are measurement-consistent, i.e., :math:`y=\forw{\inverse{y}}`.
    The measurement consistency loss is defined as
    .. math::
        \|y-\forw{\inverse{y}}\|^2
    where :math:`\inverse{y}` is the reconstructed signal and :math:`A` is a forward operator.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param Metric, torch.nn.Module metric: metric used for computing data consistency, which is set as the mean squared error by default.
    """
    def __init__(self, model_type, metric: Union[Metric, torch.nn.Module] = torch.nn.MSELoss()):
        super(MCLoss, self).__init__()
        self.name = "mc"
        self.metric = metric
        self.device = torch.device("cuda")
        self.model_type = model_type
    def forward(self, y, x_net, physics, csmap, **kwargs):
        r"""
        Computes the measurement splitting loss
        :param torch.Tensor y: measurements.
        :param torch.Tensor x_net: reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: forward operator associated with the measurements.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.model_type == "CRNN":
            return self.metric(physics.A(x_net, csmap), y)
        elif self.model_type == "LSFPNet":
            x_net = to_torch_complex(x_net)
            y_hat = physics(inv=False, data=x_net, smaps=csmap).to(self.device)
            y_hat = torch.stack([y_hat.real, y_hat.imag], dim=-1)
            y = torch.stack([y.real, y.imag], dim=-1)
            return self.metric(y_hat, y)

=== ei.py ===
from typing import Union
import torch
import torch.nn as nn
from deepinv.loss.loss import Loss
from deepinv.loss.metric.metric import Metric
from deepinv.transform.base import Transform
from einops import rearrange
from radial import to_torch_complex
class EILoss(Loss):
    r"""
    Equivariant imaging self-supervised loss.
    Assumes that the set of signals is invariant to a group of transformations (rotations, translations, etc.)
    in order to learn from incomplete measurement data alone https://https://arxiv.org/pdf/2103.14756.pdf.
    The EI loss is defined as
    .. math::
        \| T_g \hat{x} - \inverse{\forw{T_g \hat{x}}}\|^2
    where :math:`\hat{x}=\inverse{y}` is a reconstructed signal and
    :math:`T_g` is a transformation sampled at random from a group :math:`g\sim\group`.
    By default, the error is computed using the MSE metric, however any other metric (e.g., :math:`\ell_1`)
    can be used as well.
    :param deepinv.transform.Transform transform: Transform to generate the virtually augmented measurement.
        It can be any torch-differentiable function (e.g., a :class:`torch.nn.Module`)
        including `torchvision transforms <https://pytorch.org/vision/stable/transforms.html>`_.
    :param Metric, torch.nn.Module metric: Metric used to compute the error between the reconstructed augmented measurement and the reference
        image.
    :param bool apply_noise: if ``True``, the augmented measurement is computed with the full sensing model
        :math:`\sensor{\noise{\forw{\hat{x}}}}` (i.e., noise and sensor model),
        otherwise is generated as :math:`\forw{\hat{x}}`.
    :param float weight: Weight of the loss.
    :param bool no_grad: if ``True``, the gradient does not propagate through :math:`T_g`. Default: ``False``.
        This option is useful for super-resolution problems, see https://arxiv.org/abs/2312.11232.
    """
    def __init__(
        self,
        transform: Transform,
        model_type: str,
        metric: Union[Metric, nn.Module] = torch.nn.MSELoss(),
        apply_noise=True,
        weight=1.0,
        no_grad=False,
        *args,
        **kwargs,
    ):
        super(EILoss, self).__init__(*args, **kwargs)
        self.name = "ei"
        self.metric = metric
        self.weight = weight
        self.T = transform
        self.noise = apply_noise
        self.no_grad = no_grad
        self.model_type = model_type
    def forward(self, x_net, physics, model, csmap, acceleration, start_timepoint_index, **kwargs):
        r"""
        Computes the EI loss
        :param torch.Tensor x_net: Reconstructed image :math:`\inverse{y}`.
        :param deepinv.physics.Physics physics: Forward operator associated with the measurements.
        :param torch.nn.Module model: Reconstruction function.
        :return: (:class:`torch.Tensor`) loss.
        """
        if self.no_grad:
            with torch.no_grad():
                x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
                x2_rearranged = self.T(x_net_rearranged)
                x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
                x2 = x2.detach()
        else:
            x_net_rearranged = rearrange(x_net, 'b c h w t -> b c t h w')
            x2_rearranged = self.T(x_net_rearranged)
            x2 = rearrange(x2_rearranged, 'b c t h w -> b c h w t')
        x2_complex = to_torch_complex(x2)
        y = physics(inv=False, data=x2_complex, smaps=csmap).to(csmap.device)
        x3, *_ = model(y, physics, csmap, acceleration, start_timepoint_index, epoch=None)
        loss_ei = self.weight * self.metric(x3, x2)
        return loss_ei, x2

=== transform.py ===
import deepinv as dinv
import torch
from deepinv.transform import Transform
from einops import rearrange
import torch.nn.functional as F
from torchvision.transforms.functional import rotate
from typing import Union
class VideoRotate(Transform):
    r"""
    CORRECTED 2D Rotation for Videos (Handles deepinv composition).
    This class correctly applies a single, consistent random rotation to all frames of a video.
    It samples angles uniformly from a continuous range and is robust to being called
    from a deepinv composition operator that pre-flattens the video tensor.
    :param tuple[float, float] or float degrees: Range of degrees to select from.
        If degrees is a number instead of sequence like (min, max), the range of degrees
        will be (-degrees, +degrees).
    :param str interpolation_mode: "bilinear" or "nearest".
    :param bool constant_shape: if True, output has the same shape as the input.
    """
    def __init__(
        self,
        *args,
        degrees: Union[float, tuple[float, float]] = 180.0,
        interpolation_mode: str = "bilinear",
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        if isinstance(degrees, (int, float)):
            if degrees < 0:
                raise ValueError("If degrees is a single number, it must be non-negative.")
            self.degrees = (-degrees, degrees)
        else:
            if len(degrees) != 2:
                raise ValueError("If degrees is a sequence, it must be of length 2.")
            self.degrees = degrees
        self.interpolation_mode = interpolation_mode
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Uniformly samples `n_trans` random angles from the specified continuous range.
        """
        angles = [
            torch.empty(1).uniform_(self.degrees[0], self.degrees[1]).item()
            for _ in range(self.n_trans)
        ]
        return {"theta": angles}
    def _transform(
        self,
        x: torch.Tensor,
        theta: Union[torch.Tensor, list] = [],
        **kwargs,
    ) -> torch.Tensor:
        """
        Applies the rotation transformations. This method now explicitly handles 5D video tensors.
        """
        if not self._check_x_5D(x):
             raise ValueError("VideoRotate is designed for 5D video tensors (B, C, T, H, W).")
        B, C, T, H, W = x.shape
        if not theta:
            params = self._get_params(x)
            theta = params["theta"]
        if not theta:
            raise ValueError("Rotation angle 'theta' not provided.")
        angle_for_video = theta[0]
        angle_rad = -torch.tensor(angle_for_video) * (torch.pi / 180.0)
        cos_a, sin_a = torch.cos(angle_rad), torch.sin(angle_rad)
        self.last_angle = angle_for_video
        matrix = torch.tensor(
            [[cos_a, -sin_a, 0], [sin_a, cos_a, 0]], 
            dtype=torch.float32, device=x.device
        ).unsqueeze(0)
        matrix = matrix.repeat(B, 1, 1)
        grid_single = F.affine_grid(matrix, (B, C, H, W), align_corners=False)
        grid_expanded = grid_single.repeat_interleave(T, dim=0)
        x_flat = dinv.physics.TimeMixin.flatten(x)
        transformed_flat = F.grid_sample(x_flat, grid_expanded, mode=self.interpolation_mode, padding_mode='zeros', align_corners=False)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class VideoDiffeo(dinv.transform.CPABDiffeomorphism):
    """A Diffeomorphism transform that correctly handles 5D video tensors."""
    def _transform(self, x: torch.Tensor, **params) -> torch.Tensor:
        if not self._check_x_5D(x):
            return super()._transform(x, **params)
        B = x.shape[0]
        x_flat = dinv.physics.TimeMixin.flatten(x)
        flat_params = self.get_params(x_flat)
        transformed_flat = super()._transform(x_flat, **flat_params)
        return dinv.physics.TimeMixin.unflatten(transformed_flat, batch_size=B)
class SubsampleTime(Transform):
    r"""
    Augments a video by taking a random contiguous temporal sub-sequence of a
    RANDOM length, and then interpolating it back to the original length.
    :param int n_trans: Number of transformed versions to generate per input image.
    :param tuple[float, float] subsample_ratio_range: The min and max ratio of the
                                                     total time frames to keep (e.g., (0.7, 0.95)).
    :param torch.Generator rng: Random number generator.
    """
    def __init__(self, *args, subsample_ratio_range: tuple[float, float] = (0.7, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False  # Operate on 5D tensor
        min_r, max_r = subsample_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "subsample_ratio_range must be a valid range (min, max) between 0 and 1."
        self.subsample_ratio_range = subsample_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a random ratio and a random start index for each transform.
        """
        total_time_frames = x.shape[2]  # Shape is (B, C, T, H, W)
        min_r, max_r = self.subsample_ratio_range
        ratios = min_r + (max_r - min_r) * torch.rand(self.n_trans, generator=self.rng)
        start_indices = []
        for ratio in ratios:
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                start_indices.append(0)
                continue
            max_start_index = total_time_frames - subsample_length
            start_idx = torch.randint(
                low=0, high=max_start_index + 1, size=(1,), generator=self.rng
            ).item()
            start_indices.append(start_idx)
        return {"ratios": ratios, "start_indices": torch.tensor(start_indices, dtype=torch.long)}
    def _transform(
        self, x: torch.Tensor, ratios: torch.Tensor, start_indices: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """Performs the temporal subsampling and resizing for each requested transform."""
        B, C, total_time_frames, H, W = x.shape
        assert B == 1, "This transform implementation assumes a batch size of 1 for simplicity."
        output_list = []
        for i in range(self.n_trans):
            ratio = ratios[i]
            start_idx = start_indices[i]
            subsample_length = int(total_time_frames * ratio.item())
            if subsample_length >= total_time_frames:
                output_list.append(x.clone()) # Use clone to avoid issues
                continue
            sub_sequence = x[:, :, start_idx : start_idx + subsample_length, :, :]
            flat_for_interp = rearrange(sub_sequence, "b c t h w -> b (c h w) t")
            resized_flat = torch.nn.functional.interpolate(
                flat_for_interp,
                size=total_time_frames,
                mode="linear",
                align_corners=False,
            )
            resized_sequence = rearrange(
                resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W
            )
            output_list.append(resized_sequence)
        return torch.cat(output_list, dim=0)
class PeakAwareBiPhasicWarp(Transform):
    r"""
    An enhancement-peak-aware temporal augmentation that warps BOTH the wash-in
    and wash-out phases independently with different random ratios.
    It finds the time of peak enhancement, splits the video, and then
    time-warps (compresses/stretches) both phases before reassembling them.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
                                                 compressing a phase. e.g., (0.6, 0.95).
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.6, 0.95), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r <= 1.0, "warp_ratio_range must be valid."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generate two independent warp ratios for each requested transform:
        one for wash-in and one for wash-out.
        """
        params_list = []
        min_r, max_r = self.warp_ratio_range
        for _ in range(self.n_trans):
            washin_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            washout_ratio = min_r + (max_r - min_r) * torch.rand(1, generator=self.rng).item()
            params_list.append({
                "washin_ratio": washin_ratio,
                "washout_ratio": washout_ratio
            })
        return {"params_list": params_list}
    def _transform(self, x: torch.Tensor, params_list: list[dict], **kwargs) -> torch.Tensor:
        """Applies the independent, bi-phasic warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        mean_signal_curve = torch.mean(x, dim=(0, 1, 3, 4)) # Avg over B, C, H, W
        peak_idx = torch.argmax(mean_signal_curve)
        if peak_idx <= 0 or peak_idx >= x.shape[2] - 1:
            return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for params in params_list:
            wash_in_phase = x[:, :, :peak_idx, :, :]
            peak_frame = x[:, :, peak_idx:peak_idx+1, :, :]
            wash_out_phase = x[:, :, peak_idx+1:, :, :]
            warped_wash_in = self._warp_phase(wash_in_phase, params["washin_ratio"])
            warped_wash_out = self._warp_phase(wash_out_phase, params["washout_ratio"])
            new_x = torch.cat([warped_wash_in, peak_frame, warped_wash_out], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to subsample and interpolate a video phase."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase <= 1: # Cannot warp a single frame or empty tensor
            return phase_tensor
        subsample_len = max(1, int(T_phase * ratio)) # Ensure at least 1 frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        subsampled_flat = F.interpolate(flat_for_interp, size=subsample_len, mode='linear', align_corners=False)
        resized_flat = F.interpolate(subsampled_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class MonophasicTimeWarp(Transform):
    r"""
    A temporal augmentation specifically designed for monophasic enhancement curves
    (e.g., persistent or plateau types) where there is no wash-out phase.
    This transform keeps the first (pre-contrast) frame fixed and applies a
    single, smooth time-warp to the entire subsequent enhancement phase.
    :param tuple[float, float] warp_ratio_range: The min/max ratio for
        compressing/stretching the enhancement phase. e.g., (0.7, 1.3).
        Values < 1 compress time, values > 1 stretch time.
    """
    def __init__(self, *args, warp_ratio_range: tuple[float, float] = (0.7, 1.3), **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        min_r, max_r = warp_ratio_range
        assert 0.0 < min_r <= max_r, "warp_ratio_range must be a valid positive range."
        self.warp_ratio_range = warp_ratio_range
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single random warp ratio for the entire enhancement phase.
        """
        min_r, max_r = self.warp_ratio_range
        ratios = [min_r + (max_r - min_r) * torch.rand(1, generator=self.rng) for _ in range(self.n_trans)]
        return {"ratios": ratios}
    def _transform(self, x: torch.Tensor, ratios: list[float], **kwargs) -> torch.Tensor:
        """Applies the monophasic time warp."""
        assert x.shape[0] == 1, "This transform assumes a batch size of 1 for the input."
        if x.shape[2] <= 1: # Cannot warp if there's only one frame
             return x.repeat(self.n_trans, 1, 1, 1, 1)
        output_list = []
        for ratio in ratios:
            pre_contrast_frame = x[:, :, :1, :, :]
            enhancement_phase = x[:, :, 1:, :, :]
            warped_enhancement_phase = self._warp_phase(enhancement_phase, ratio)
            new_x = torch.cat([pre_contrast_frame, warped_enhancement_phase], dim=2)
            output_list.append(new_x)
        return torch.cat(output_list, dim=0)
    def _warp_phase(self, phase_tensor: torch.Tensor, ratio: float) -> torch.Tensor:
        """Helper function to interpolate a video phase to a new length."""
        B, C, T_phase, H, W = phase_tensor.shape
        if T_phase == 0:
            return phase_tensor
        ratio = ratio.item()
        new_length = int(round(T_phase * ratio))
        if new_length == 0: new_length = 1 # Ensure at least one frame
        flat_for_interp = rearrange(phase_tensor, "b c t h w -> b (c h w) t")
        resized_flat = F.interpolate(flat_for_interp, size=new_length, mode='linear', align_corners=False)
        if new_length != T_phase:
            resized_flat = F.interpolate(resized_flat, size=T_phase, mode='linear', align_corners=False)
        warped_phase = rearrange(resized_flat, "b (c h w) t -> b c t h w", c=C, h=H, w=W)
        return warped_phase
class TemporalNoise(Transform):
    """ 
    Adds low-frequency random noise to the temporal signal of a video.
    This simulates smooth, slowly varying noise sources over time.
    """
    def __init__(self, *args, noise_strength: float = 0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
        self.noise_strength = noise_strength
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        Generates a single low-frequency noise vector for the transformation.
        """
        B, C, T, H, W = x.shape
        low_res_T = max(1, T // 4) 
        noise_low_res = torch.randn(B, 1, low_res_T, device=x.device)
        noise_high_res = F.interpolate(noise_low_res, size=T, mode='linear', align_corners=False)
        noise_norm = (noise_high_res - noise_high_res.mean(dim=-1, keepdim=True)) / (noise_high_res.std(dim=-1, keepdim=True) + 1e-8)
        final_noise = noise_norm * self.noise_strength
        return {'noise': final_noise}
    def _transform(self, x: torch.Tensor, noise: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the pre-generated noise to the image tensor.
        """
        B, C, T, H, W = x.shape
        x_noisy = x + noise.view(B, 1, T, 1, 1)
        return x_noisy
class TimeReverse(Transform):
    r"""
    Reverses the temporal order of frames in a video tensor.
    This transform flips the video along the time axis, effectively playing it
    backwards. This is a deterministic transformation.
    :param int n_trans: Number of transformed versions to generate per input image.
                        Since this is deterministic, it will just repeat the same
                        output if n_trans > 1.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.flatten_video_input = False
    def _get_params(self, x: torch.Tensor) -> dict:
        """
        No random parameters are needed for time reversal as it's a
        deterministic operation.
        """
        return {}
    def _transform(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Applies the time reversal transformation using torch.flip().
        """
        if len(x.shape) != 5:
            raise ValueError(f"TimeReverse expects a 5D tensor (B, C, T, H, W), but got shape {x.shape}.")
        return torch.flip(x, dims=[2])

=== utils.py ===
import os
import subprocess
import matplotlib.pyplot as plt
import torch
import numpy as np
from einops import rearrange
import torchkbnufft as tkbn
import csv
import sigpy as sp
from sigpy.mri import app
from radial_lsfp import MCNUFFT
import random
from torch.nn.parallel import DistributedDataParallel as DDP
def log_gradient_stats(model, epoch, iteration, output_dir, log_filename="gradient_stats.csv"):
    """
    Computes, prints, and logs the L2 norm of gradients for each parameter and the total gradient norm.
    Args:
        model (torch.nn.Module): The model being trained.
        epoch (int): The current epoch.
        iteration (int): The current global iteration/step count.
        output_dir (str): The main experiment output directory.
        log_filename (str): The CSV filename for storing detailed logs.
    """
    total_norm = 0.0
    param_norms = []
    for name, p in model.named_parameters():
        if p.grad is not None and p.requires_grad:
            param_norm = p.grad.data.norm(2)
            if not torch.isfinite(param_norm):
                param_norm_item = float('inf')
            else:
                param_norm_item = param_norm.item()
            param_norms.append((name, param_norm_item))
            total_norm += param_norm_item ** 2
    total_norm = total_norm ** 0.5
    print(f"--- Gradient Stats (Epoch {epoch}, Iter {iteration}) ---")
    print(f"Total Gradient Norm: {total_norm:.4e}")
    param_norms.sort(key=lambda x: x[1], reverse=True)
    print("Top 5 layers with largest gradients:")
    for name, norm in param_norms[:5]:
        print(f"  - {name}: {norm:.4e}")
    print("Top 5 layers with smallest gradients:")
    non_zero_norms = [p for p in param_norms if p[1] > 0]
    for name, norm in non_zero_norms[-5:]:
        print(f"  - {name}: {norm:.4e}")
    print("-------------------------------------------------")
    log_path = os.path.join(output_dir, log_filename)
    file_exists = os.path.isfile(log_path)
    with open(log_path, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow(['epoch', 'iteration', 'total_norm', 'param_name', 'param_norm'])
        writer.writerow([epoch, iteration, total_norm, '---TOTAL---', total_norm])
        for name, norm in param_norms:
            writer.writerow([epoch, iteration, total_norm, name, norm])
def trajGR(Nkx, Nspokes):
    '''
    function for generating golden-angle radial sampling trajectory
    :param Nkx: spoke length
    :param Nspokes: number of spokes
    :return: ktraj: golden-angle radial sampling trajectory
    '''
    ga = np.pi * ((1 - np.sqrt(5)) / 2)
    kx = np.zeros(shape=(Nkx, Nspokes))
    ky = np.zeros(shape=(Nkx, Nspokes))
    ky[:, 0] = np.linspace(-np.pi, np.pi, Nkx)
    for i in range(1, Nspokes):
        kx[:, i] = np.cos(ga) * kx[:, i - 1] - np.sin(ga) * ky[:, i - 1]
        ky[:, i] = np.sin(ga) * kx[:, i - 1] + np.cos(ga) * ky[:, i - 1]
    ky = np.transpose(ky)
    kx = np.transpose(kx)
    ktraj = np.stack((ky.flatten(), kx.flatten()), axis=0)
    return ktraj
def prep_nufft(Nsample, Nspokes, Ng):
    overSmaple = 2
    im_size = (int(Nsample/overSmaple), int(Nsample/overSmaple))
    grid_size = (Nsample, Nsample)
    ktraj = trajGR(Nsample, Nspokes * Ng)
    ktraj = torch.tensor(ktraj, dtype=torch.float)
    dcomp = tkbn.calc_density_compensation_function(ktraj=ktraj, im_size=im_size)
    dcomp = dcomp.squeeze()
    ktraju = np.zeros([2, Nspokes * Nsample, Ng], dtype=float)
    dcompu = np.zeros([Nspokes * Nsample, Ng], dtype=complex)
    for ii in range(0, Ng):
        ktraju[:, :, ii] = ktraj[:, (ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
        dcompu[:, ii] = dcomp[(ii * Nspokes * Nsample):((ii + 1) * Nspokes * Nsample)]
    ktraju = torch.tensor(ktraju, dtype=torch.float)
    dcompu = torch.tensor(dcompu, dtype=torch.complex64)
    nufft_ob = tkbn.KbNufft(im_size=im_size, grid_size=grid_size)  # forward nufft
    adjnufft_ob = tkbn.KbNufftAdjoint(im_size=im_size, grid_size=grid_size)  # backward nufft
    return ktraju, dcompu, nufft_ob, adjnufft_ob
def _calculate_top_percentile_curve(dynamic_slice: torch.Tensor, percentile: float) -> list[float]:
    """Helper function to calculate the enhancement curve for a single dynamic slice."""
    if dynamic_slice.dim() != 5 or dynamic_slice.shape[0] != 1 or dynamic_slice.shape[1] != 2:
        raise ValueError(f"Expected input shape (1, 2, T, H, W), but got {dynamic_slice.shape}")
    magnitude_video = torch.sqrt(dynamic_slice[:, 0, ...] ** 2 + dynamic_slice[:, 1, ...] ** 2).squeeze(0)
    num_time_frames = magnitude_video.shape[0]
    top_percentile_means = []
    q = percentile / 100.0
    for t in range(num_time_frames):
        frame_t = magnitude_video[t, :, :]
        if frame_t.max() == 0:
            top_percentile_means.append(0)
            continue
        threshold = torch.quantile(frame_t.flatten(), q)
        bright_pixels = frame_t[frame_t > threshold]
        mean_val = torch.mean(bright_pixels) if bright_pixels.numel() > 0 else threshold
        top_percentile_means.append(mean_val.item())
    return top_percentile_means
def plot_enhancement_curve(
    model_output: torch.Tensor,
    percentile: float = 99.0,
    title: str = "Enhancement Curve Comparison",
    output_filename: str = None
):
    """
    Calculates and plots the enhancement curves for a model output and a benchmark
    image on the same graph for direct comparison.
    Args:
        model_output (torch.Tensor): The model's reconstructed dynamic slice.
                                     Shape (1, 2, T, H, W).
        benchmark_image (torch.Tensor): The ground truth or benchmark dynamic slice.
                                        Shape (1, 2, T, H, W).
        percentile (float, optional): The percentile for defining the brightest pixels.
                                      Defaults to 99.0.
        title (str, optional): The title for the plot. Defaults to "Enhancement Curve Comparison".
        output_filename (str, optional): If provided, saves the plot to this file path.
                                         Defaults to None (displays plot).
    """
    if not 0 < percentile < 100:
        raise ValueError("Percentile must be between 0 and 100.")
    model_curve = _calculate_top_percentile_curve(model_output.detach(), percentile)
    num_time_frames = model_output.shape[2]
    time_axis = np.arange(num_time_frames)
    plt.figure(figsize=(12, 7))
    plt.plot(time_axis, model_curve, label='Model Output', marker='o', linestyle='-', color='tab:blue')
    plt.title(title, fontsize=16)
    plt.xlabel("Time Frame", fontsize=12)
    plt.ylabel(f"Mean Signal of Top {100-percentile:.1f}% Pixels", fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    if output_filename:
        output_dir = os.path.dirname(output_filename)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        plt.savefig(output_filename)
    else:
        plt.show()
    plt.close()
def get_cosine_ei_weight(
    current_epoch,
    warmup_epochs,
    schedule_duration,
    target_weight
):
    """
    Calculates the EI loss weight for the current epoch using a cosine schedule.
    This implements a curriculum learning strategy:
    1. For `warmup_epochs`, the weight is 0 (MC loss only).
    2. Over the next `schedule_duration` epochs, the weight smoothly ramps
       up from 0 to `target_weight` following a cosine curve.
    3. After the schedule is complete, the weight stays at `target_weight`.
    Args:
        current_epoch (int): The current training epoch (starting from 1).
        warmup_epochs (int): Number of epochs to train with only MC loss.
        schedule_duration (int): Number of epochs for the ramp-up.
        target_weight (float): The final EI loss weight to reach.
    Returns:
        float: The EI loss weight for the current epoch.
    """
    if current_epoch <= warmup_epochs:
        return 0.0
    schedule_progress_epoch = current_epoch - warmup_epochs
    if schedule_progress_epoch >= schedule_duration:
        return target_weight
    cosine_multiplier = 0.5 * (1 - np.cos(np.pi * schedule_progress_epoch / schedule_duration))
    return target_weight * cosine_multiplier
def plot_reconstruction_sample(x_recon, title, filename, output_dir, grasp_img=None, batch_idx=0, transform=False):
    """
    Plot reconstruction sample showing magnitude images across timeframes.
    Args:
        x_recon: Reconstructed image tensor of shape (B, C, T, H, W)
        title: Title for the plot
        filename: Filename for saving (without extension)
        output_dir: Directory to save the plot
        batch_idx: Which batch element to plot (default: 0)
    """
    os.makedirs(output_dir, exist_ok=True)
    if x_recon.shape[1] == 2:
        x_recon_mag = torch.sqrt(x_recon[:, 0, ...] ** 2 + x_recon[:, 1, ...] ** 2)
    else:
        x_recon_mag = x_recon
    n_timeframes = x_recon_mag.shape[-1]
    if grasp_img is not None:
        grasp_img_mag = torch.sqrt(grasp_img[:, 0, ...] ** 2 + grasp_img[:, 1, ...] ** 2)
        fig, axes = plt.subplots(
            nrows=2,
            ncols=n_timeframes,
            figsize=(n_timeframes * 3, 8),
            squeeze=False,
        )
        if transform:
            axes[0, 0].set_ylabel("Transformed Image", fontsize=14, labelpad=10)
            axes[1, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
            os.makedirs(os.path.join(output_dir, "transforms"), exist_ok=True)
        else:
            axes[0, 0].set_ylabel("Model Output", fontsize=14, labelpad=10)
            axes[1, 0].set_ylabel("GRASP Benchmark", fontsize=14, labelpad=10)
    else:
        fig, axes = plt.subplots(
            nrows=1,
            ncols=n_timeframes,
            figsize=(n_timeframes * 3, 4),
            squeeze=True,
        )
    for t in range(n_timeframes):
        if x_recon_mag.shape[1] == n_timeframes:
            img = x_recon_mag[batch_idx, t, :, :].cpu().detach().numpy()
        else:
            img = x_recon_mag[batch_idx, ..., t].cpu().detach().numpy()
        if grasp_img is not None:
            if grasp_img_mag.shape[1] == n_timeframes:
                grasp_img = grasp_img_mag[batch_idx, t, :, :].cpu().detach().numpy()
            elif grasp_img_mag.shape[-1] == n_timeframes:
                grasp_img = grasp_img_mag[batch_idx, :, :, t].cpu().detach().numpy()
            else:
                grasp_img = grasp_img_mag[batch_idx, :, t, :].cpu().detach().numpy()
            ax1 = axes[0, t]
        else:
            ax1 = axes[t]
        ax1.imshow(img, cmap="gray")
        ax1.set_title(f"t = {t}")
        ax1.set_xticks([])
        ax1.set_yticks([])
        if grasp_img is not None:
            ax2 = axes[1, t]
            ax2.imshow(grasp_img, cmap="gray")
            ax2.set_title(f"t = {t}")
            ax2.set_xticks([])
            ax2.set_yticks([])
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(output_dir, f"{filename}.png"))
    plt.close(fig)
def get_git_commit():
    try:
        commit_hash = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        return commit_hash
    except Exception as e:
        print(f"Error retrieving Git commit: {e}")
        return "unknown"
def remove_module_prefix(state_dict):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('module.', '')  # Remove 'module.' prefix
        new_state_dict[new_key] = v
    return new_state_dict
def save_checkpoint(model, optimizer, epoch,
                    train_curves, val_curves, eval_curves, ei_weight, step0_train_ei_loss, epoch_train_mc_loss, filename):
    model_state = model.module.state_dict() if isinstance(model, DDP) else model.state_dict()
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "ei_weight": ei_weight,
        "step0_train_ei_loss": step0_train_ei_loss,
        "epoch_train_mc_loss": epoch_train_mc_loss,
        **train_curves,   # unpack the dicts
        **val_curves,
        **eval_curves,
    }
    torch.save(checkpoint, filename)
    print(f"Checkpoint saved at epoch {epoch} to {filename}")
def load_checkpoint(model, optimizer, filename):
    ckpt = torch.load(filename, map_location="cpu")
    model_to_load = model.module if isinstance(model, DDP) else model
    model_to_load.load_state_dict(remove_module_prefix(ckpt["model_state_dict"]))
    optimizer.load_state_dict(remove_module_prefix(ckpt["optimizer_state_dict"]))
    train_curves = {
        "train_mc_losses": ckpt.get("train_mc_losses", []),
        "train_ei_losses": ckpt.get("train_ei_losses", []),
        "train_adj_losses": ckpt.get("train_adj_losses", []),
        "weighted_train_mc_losses": ckpt.get("weighted_train_mc_losses", []),
        "weighted_train_ei_losses": ckpt.get("weighted_train_ei_losses", []),
        "weighted_train_adj_losses": ckpt.get("weighted_train_adj_losses", []),
    }
    val_curves = {
        "val_mc_losses": ckpt.get("val_mc_losses", []),
        "val_ei_losses": ckpt.get("val_ei_losses", []),
        "val_adj_losses": ckpt.get("val_adj_losses", []),
    }
    eval_curves = {
        "eval_ssims": ckpt.get("eval_ssims", []),
        "eval_psnrs": ckpt.get("eval_psnrs", []),
        "eval_mses": ckpt.get("eval_mses", []),
        "eval_lpipses": ckpt.get("eval_lpipses", []),
        "eval_dc_mses": ckpt.get("eval_dc_mses", []),
        "eval_dc_maes": ckpt.get("eval_dc_maes", []),
        "eval_curve_corrs": ckpt.get("eval_curve_corrs", []),
    }
    return model, optimizer, ckpt.get("epoch", 1), ckpt.get("ei_weight"), ckpt.get("step0_train_ei_loss"), ckpt.get("epoch_train_mc_loss"), train_curves, val_curves, eval_curves
def to_torch_complex(x: torch.Tensor):
    """(B, 2, ...) real -> (B, ...) complex"""
    assert x.shape[1] == 2, (
        f"Input tensor must have 2 channels (real, imag), but got shape {x.shape}"
    )
    return torch.view_as_complex(rearrange(x, "b c ... -> b ... c").contiguous())
def get_traj(N_spokes=13, N_time=1, base_res=320, gind=1):
    N_tot_spokes = N_spokes * N_time
    N_samples = base_res * 2
    base_lin = np.arange(N_samples).reshape(1, -1) - base_res
    tau = 0.5 * (1 + 5**0.5)
    base_rad = np.pi / (gind + tau - 1)
    base_rot = np.arange(N_tot_spokes).reshape(-1, 1) * base_rad
    traj = np.zeros((N_tot_spokes, N_samples, 2))
    traj[..., 0] = np.cos(base_rot) @ base_lin
    traj[..., 1] = np.sin(base_rot) @ base_lin
    traj = traj / 2
    traj = traj.reshape(N_time, N_spokes, N_samples, 2)
    return np.squeeze(traj)
def GRASPRecon(csmaps, kspace, spokes_per_frame, num_frames, grasp_path):
    traj = get_traj(N_spokes=spokes_per_frame, N_time=num_frames)
    device = sp.Device(0 if torch.cuda.is_available() else -1)
    kspace = rearrange(kspace, 'c (sp sam) t -> t c sp sam', sam=640).unsqueeze(1).unsqueeze(3).cpu().numpy()
    csmaps = rearrange(csmaps, 'b c h w -> c b h w').cpu().numpy()
    R1 = app.HighDimensionalRecon(kspace, csmaps,
                            combine_echo=False,
                            lamda=0.001,
                            coord=traj,
                            regu='TV', regu_axes=[0],
                            max_iter=10,
                            solver='ADMM', rho=0.1,
                            device=device,
                            show_pbar=False,
                            verbose=False).run()
    R1 = np.squeeze(R1.get())
    np.save(grasp_path, R1)
    print(f"GRASP Recon with {spokes_per_frame} spokes/frame and {num_frames} timeframes saved to {grasp_path}")
    return R1
def generate_sliding_window_indices(N_frames, chunk_size, overlap_size):
    """
    Generates start and end indices for a sliding window reconstruction.
    Args:
        N_frames (int): Total number of time frames in the dynamic MRI.
        chunk_size (int): The number of frames in each chunk.
        overlap_size (int): The number of frames that consecutive chunks will overlap.
    Returns:
        list of tuple: A list where each tuple contains (start_index, end_index)
                       for a chunk.
    """
    if chunk_size <= 0 or N_frames <= 0:
        raise ValueError("chunk_size and N_frames must be positive.")
    if overlap_size >= chunk_size:
        raise ValueError("overlap_size must be less than chunk_size.")
    if overlap_size < 0:
        raise ValueError("overlap_size cannot be negative.")
    chunks = []
    step_size = chunk_size - overlap_size
    current_start = 0
    while True:
        current_end = current_start + chunk_size
        if current_end > N_frames:
            if N_frames - chunk_size >= 0:
                current_start = N_frames - chunk_size
                current_end = N_frames
            else:
                current_start = 0
                current_end = N_frames
            chunks.append((current_start, current_end))
            break # We've covered the end of the sequence
        chunks.append((current_start, current_end))
        if current_end == N_frames:
            break
        current_start += step_size
    unique_chunks = []
    seen = set()
    for chunk in chunks:
        if chunk not in seen:
            unique_chunks.append(chunk)
            seen.add(chunk)
    return unique_chunks
def sliding_window_inference(H, W, N_frames, ktraj, dcomp, nufft_ob, adjnufft_ob, chunk_size, chunk_overlap, kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch, device):
    chunk_indices = generate_sliding_window_indices(N_frames, chunk_size, chunk_overlap)
    stitched_recon = torch.zeros(1, 2, H, W, N_frames).to(device)
    frame_contribution_count = torch.zeros(H, W, N_frames).to(device)
    csmap = csmap.to(device)
    for i, (start_idx, end_idx) in enumerate(chunk_indices):
        print(f"Processing chunk {i+1}: frames {start_idx}-{end_idx}")
        kspace_chunk = kspace[..., start_idx:end_idx].to(device)
        ktraj_chunk = ktraj[..., start_idx:end_idx].to(device)
        dcomp_chunk = dcomp[..., start_idx:end_idx].to(device)
        physics_chunk = MCNUFFT(nufft_ob, adjnufft_ob, ktraj_chunk, dcomp_chunk)
        if start_timepoint_index is not None:
            start_timepoint_index = torch.tensor([start_idx], dtype=torch.float, device=device)
        x_recon_chunk, adj_loss, *_ = model(
            kspace_chunk.to(device), physics_chunk, csmap, acceleration_encoding, start_timepoint_index, epoch=epoch, norm="both"
        )
        stitched_recon[..., start_idx:end_idx] += x_recon_chunk
        frame_contribution_count[..., start_idx:end_idx] += 1
    stitched_recon /= frame_contribution_count # This performs element-wise division
    return stitched_recon, adj_loss
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

=== eval.py ===
import os
import matplotlib.pyplot as plt
import torch
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import torchmetrics
import time
from dataloader import SimulatedDataset
from lsfpnet import to_torch_complex, from_torch_complex
import numpy as np
from scipy.optimize import curve_fit
from scipy.interpolate import PchipInterpolator
from tqdm import tqdm # A library for a nice progress bar
from scipy.stats import mannwhitneyu
from skimage.metrics import structural_similarity as ssim_map_func
import matplotlib.gridspec as gridspec
from skimage.measure import find_contours
from typing import List, Dict
from scipy.stats import pearsonr
def normalize_for_lpips(image, data_range):
    """Normalizes an image tensor to the [-1, 1] range for LPIPS."""
    min_val, max_val = data_range
    image_0_1 = (image - min_val) / (max_val - min_val)
    image_minus1_1 = 2 * image_0_1 - 1
    return image_minus1_1
def calc_image_metrics(input, reference, data_range, device, filename):
    """
    Calculates image metrics for a given input and reference image.
    """
    ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range).to(device)
    psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range).to(device)
    mse = torchmetrics.MeanSquaredError().to(device)
    lpips_metric = torchmetrics.image.LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=False).to(device)
    ssim = ssim(input, reference)
    psnr = psnr(input, reference)
    mse = mse(input, reference)
    if input.dim() == 5:
        num_slices = input.shape[2]
        lpips_scores = []
        for i in range(num_slices):
            input_slice = input[:, :, i, :, :]
            reference_slice = reference[:, :, i, :, :]
            input_lpips = normalize_for_lpips(input_slice.clone(), data_range)
            reference_lpips = normalize_for_lpips(reference_slice.clone(), data_range)
            if input_lpips.shape[1] == 1:
                input_lpips = input_lpips.repeat(1, 3, 1, 1)
                reference_lpips = reference_lpips.repeat(1, 3, 1, 1)
            input_lpips = input_lpips.to(reference_lpips.dtype)
            lpips_scores.append(lpips_metric(input_lpips, reference_lpips).item())
        final_lpips = sum(lpips_scores) / len(lpips_scores)
    return ssim.item(), psnr.item(), mse.item(), final_lpips
def calc_dc(input, reference, device):
    """
    Calculates data consistency MSE for a given input and reference k-space tensor.
    """
    mse = torchmetrics.MeanSquaredError().to(device)
    mae = torchmetrics.MeanAbsoluteError().to(device)
    input = from_torch_complex(input).to(device)
    reference = from_torch_complex(reference).to(device)
    mse = mse(input, reference)
    mae = mae(input, reference)
    return mse.item(), mae.item()
def evaluate_reconstruction_fidelity(
    ground_truth_params: np.ndarray,
    estimated_params: np.ndarray,
    masks: dict,
    param_names: list = None,
    regions_to_evaluate: list = None,
    display_plots: bool = True,
    filename: str = 'pk_param_maps.png'
) -> dict:
    """
    Evaluates the fidelity of reconstructed pharmacokinetic (PK) parameters against ground truth.
    This function performs a quantitative and visual comparison, mimicking the evaluation
    methods described in the research paper (e.g., Figures 6 and 8).
    Args:
        ground_truth_params (np.ndarray): The ground truth PK parameter map, typically a
                                          (H, W, 4) array from the `gen_dro` output.
        estimated_params (np.ndarray): The PK parameter map estimated from your reconstructed
                                       images, with the same shape as ground_truth_params.
        masks (dict): A dictionary of boolean masks for different tissue regions, typically
                      from the `gen_dro` output (e.g., dro_results['mask']).
        param_names (list, optional): A list of names for the 4 parameters.
                                      Defaults to ['ve', 'vp', 'Fp', 'PS'].
        regions_to_evaluate (list, optional): A list of region names (keys in the `masks`
                                            dict) to analyze. Defaults to all available masks.
        display_plots (bool): If True, generates and shows summary plots.
    Returns:
        dict: A nested dictionary containing the evaluation results (median error and p-value)
              for each region and each parameter.
    """
    if param_names is None:
        param_names = ['ve', 'vp', 'Fp (F_p)', 'PS'] # As ordered in gen_dro
    if regions_to_evaluate is None:
        regions_to_evaluate = [name for name, mask in masks.items() if mask.any()]
    if ground_truth_params.shape != estimated_params.shape:
        raise ValueError("Ground truth and estimated parameter maps must have the same shape.")
    evaluation_results = {}
    print("--- Reconstruction Fidelity Evaluation ---")
    print("-" * 40)
    for region in regions_to_evaluate:
        if region not in masks or not masks[region].any():
            continue
        print(f"Region: {region.capitalize()}")
        evaluation_results[region] = {}
        mask = masks[region]
        for i, p_name in enumerate(param_names):
            print("ground_truth_params: ", type(ground_truth_params))
            gt_values = ground_truth_params[:, :, i][mask]
            est_values = estimated_params[:, :, i][mask]
            print("gt_values: ", type(gt_values))
            gt_values_safe = gt_values.copy()
            gt_values_safe[gt_values_safe == 0] = 1e-9 # Add a small epsilon
            relative_error = (est_values - gt_values) / gt_values_safe
            median_err = np.median(relative_error)
            try:
                stat, p_value = mannwhitneyu(gt_values, est_values, alternative='two-sided')
            except ValueError: # Happens if all values are identical
                stat, p_value = 0, 1.0
            evaluation_results[region][p_name] = {
                'median_relative_error': median_err,
                'p_value': p_value
            }
            print(f"  - {p_name:<10}: Median Error = {median_err:+.2%}, p-value = {p_value:.4f}")
    if not display_plots:
        return evaluation_results
    num_params = ground_truth_params.shape[2]
    fig, axes = plt.subplots(num_params, 3, figsize=(15, 4 * num_params), sharex=True, sharey=True)
    fig.suptitle("Visual Comparison of PK Parameter Maps", fontsize=16)
    for i in range(num_params):
        p_name = param_names[i]
        gt_map = ground_truth_params[:, :, i]
        est_map = estimated_params[:, :, i]
        error_map = est_map - gt_map
        vmax = np.percentile(gt_map[gt_map > 0], 99) if (gt_map > 0).any() else 1.0
        vmin = 0
        im_gt = axes[i, 0].imshow(gt_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 0].set_title(f"Ground Truth: {p_name}")
        axes[i, 0].axis('off')
        fig.colorbar(im_gt, ax=axes[i, 0])
        im_est = axes[i, 1].imshow(est_map, vmin=vmin, vmax=vmax, cmap='viridis')
        axes[i, 1].set_title(f"Your Estimation: {p_name}")
        axes[i, 1].axis('off')
        fig.colorbar(im_est, ax=axes[i, 1])
        err_vmax = np.percentile(np.abs(error_map), 99)
        im_err = axes[i, 2].imshow(error_map, vmin=-err_vmax, vmax=err_vmax, cmap='coolwarm')
        axes[i, 2].set_title(f"Error Map (Est - GT)")
        axes[i, 2].axis('off')
        fig.colorbar(im_err, ax=axes[i, 2])
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return evaluation_results
def plot_spatial_quality(
    recon_img: np.ndarray,
    gt_img: np.ndarray,
    grasp_img: np.ndarray,
    time_frame_index: int,
    filename: str,
    grasp_comparison_filename: str,
    data_range: float, 
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Generates a comparison plot for a single time frame in a 2x4 grid.
    Each row includes: Ground Truth, Reconstruction, Error Map, and SSIM Map.
    Args:
        recon_img (np.ndarray): Your model's reconstructed image for this frame.
        gt_img (np.ndarray): The ground truth image for this frame.
        grasp_img (np.ndarray): The GRASP reconstruction image for this frame.
        time_frame_index (int): The index of the time frame for titling.
        filename (str): The path to save the output plot.
    """
    error_map_dl = recon_img - gt_img
    error_map_grasp = grasp_img - gt_img
    ssim_dl, ssim_map_dl = ssim_map_func(gt_img, recon_img, data_range=data_range, full=True)
    ssim_grasp, ssim_map_grasp = ssim_map_func(gt_img, grasp_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(2, 4, figsize=(24, 12))
    fig.suptitle(f"Spatial Quality Comparison at Time Frame {time_frame_index} with AF {acceleration} and SPF {spokes_per_frame}", fontsize=20)
    axes[0, 0].imshow(gt_img, cmap='gray')
    axes[0, 0].set_title("Ground Truth")
    axes[0, 1].imshow(recon_img, cmap='gray')
    axes[0, 1].set_title("DL Reconstruction")
    im_err_dl = axes[0, 2].imshow(error_map_dl, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[0, 2].set_title("DL Error Map (Recon - GT)")
    fig.colorbar(im_err_dl, ax=axes[0, 2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[0, 3].imshow(ssim_map_dl, cmap='viridis', vmin=0, vmax=1)
    axes[0, 3].set_title(f"DL SSIM Map (SSIM Recon vs GT: {round(ssim_dl, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[0, 3], fraction=0.046, pad=0.04)
    axes[1, 0].imshow(gt_img, cmap='gray')
    axes[1, 0].set_title("Ground Truth")
    axes[1, 1].imshow(grasp_img, cmap='gray')
    axes[1, 1].set_title("GRASP Reconstruction")
    im_err_grasp = axes[1, 2].imshow(error_map_grasp, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[1, 2].set_title("GRASP Error Map (Recon - GT)")
    fig.colorbar(im_err_grasp, ax=axes[1, 2], fraction=0.046, pad=0.04)
    im_ssim_grasp = axes[1, 3].imshow(ssim_map_grasp, cmap='viridis', vmin=0, vmax=1)
    axes[1, 3].set_title(f"GRASP SSIM Map (SSIM Recon vs GT: {round(ssim_grasp, 3)})")
    fig.colorbar(im_ssim_grasp, ax=axes[1, 3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    plt.savefig(filename)
    plt.close()
    error_map = recon_img - grasp_img
    ssim, ssim_map = ssim_map_func(grasp_img, recon_img, data_range=data_range, full=True)
    fig, axes = plt.subplots(1, 4, figsize=(24, 6))
    fig.suptitle(f"DL vs GRASP Comparison at Time Frame {time_frame_index} with AF {acceleration} and SPF {spokes_per_frame}", fontsize=20)
    axes[0].imshow(grasp_img, cmap='gray')
    axes[0].set_title("GRASP Reconstruction")
    axes[1].imshow(recon_img, cmap='gray')
    axes[1].set_title("DL Reconstruction")
    im_err_dl = axes[2].imshow(error_map, cmap='coolwarm', vmin=-0.5, vmax=0.5)
    axes[2].set_title("Error Map (DL Recon - GRASP)")
    fig.colorbar(im_err_dl, ax=axes[2], fraction=0.046, pad=0.04)
    im_ssim_dl = axes[3].imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)
    axes[3].set_title(f"SSIM Map (SSIM between DL and GRASP Recons: {round(ssim, 3)})")
    fig.colorbar(im_ssim_dl, ax=axes[3], fraction=0.046, pad=0.04)
    for ax in axes.flat:
        ax.axis('off')
    print("SSIM between GRASP and DL Recon: ", ssim)
    plt.savefig(grasp_comparison_filename)
    plt.close()
def plot_temporal_curves(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    masks: dict,
    time_points: np.ndarray,
    filename: str, 
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Plots the mean signal intensity vs. time for different tissue regions.
    This is CRITICAL for debugging PK model fitting.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        masks (dict): Dictionary of boolean NumPy masks for different regions.
        time_points (np.ndarray): The time vector for the x-axis.
        filename (str): The path to save the output plot.
    """
    regions = [r for r in ['malignant', 'glandular', 'muscle'] if r in masks and masks[r].any()]
    if not regions:
        print("No relevant regions found in mask to plot temporal curves.")
        return
    fig, axes = plt.subplots(1, len(regions), figsize=(7 * len(regions), 5), sharey=True)
    if len(regions) == 1: axes = [axes] # Ensure axes is always a list
    fig.suptitle(f"Temporal Fidelity: Mean Signal vs. Time (AF = {acceleration}, SPF = {spokes_per_frame})", fontsize=16)
    for i, region in enumerate(regions):
        mask = masks[region]
        gt_curve = [gt_img_stack[:, :, t][mask].mean() for t in range(gt_img_stack.shape[2])]
        recon_curve = [recon_img_stack[:, :, t][mask].mean() for t in range(recon_img_stack.shape[2])]
        grasp_curve = [grasp_img_stack[:, :, t][mask].mean() for t in range(grasp_img_stack.shape[2])]
        if region == 'malignant':
            recon_correlation, _ = pearsonr(recon_curve, gt_curve)
            grasp_correlation, _ = pearsonr(grasp_curve, gt_curve)
        axes[i].plot(time_points, gt_curve, 'k-', label='Ground Truth', linewidth=2, marker='o')
        axes[i].plot(time_points, recon_curve, 'r--', label='DL Recon', marker='o')
        axes[i].plot(time_points, grasp_curve, 'b:', label='GRASP Recon', marker='o')
        axes[i].set_title(f"Region: {region.capitalize()}")
        axes[i].set_xlabel("Time (s)")
        axes[i].grid(True)
        axes[i].legend()
    axes[0].set_ylabel("Mean Signal Intensity")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename)
    plt.close()
    return recon_correlation, grasp_correlation
def plot_single_temporal_curve(
    img_stack: np.ndarray,
    masks: Dict[str, np.ndarray],
    time_points: np.ndarray,
    num_frames: int,
    filename: str,
    acceleration: float,
    spokes_per_frame: int,
    frames_to_show: List[int] = None,
):
    """
    Generates a comprehensive analysis plot for a single sample, showing the
    Tumor Contrast Enhancement Curve (CEC) and corresponding image frames with
    the tumor Region of Interest (ROI) highlighted.
    This function is modified to produce a detailed analysis plot for the
    'malignant' tissue type, using the ground truth data.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        grasp_img_stack (np.ndarray): Unused in this plot, kept for signature compatibility.
        masks (dict): Dictionary of boolean NumPy masks. Expects a 'malignant' key.
        time_points (np.ndarray): The time vector for the x-axis (e.g., frame numbers).
        filename (str): The path to save the output plot.
        sample_name (str): The name of the sample for the main plot title.
        frames_to_show (List[int]): A list of 4 frame indices to display in the
                                    image grid and highlight on the curve.
                                    If None, defaults to [0, 6, 13, 20].
    """
    region_key = 'malignant'
    if region_key not in masks or not masks[region_key].any():
        print(f"'{region_key}' mask not found or is empty. Skipping plot generation.")
        return
    tumor_mask = masks[region_key]
    if frames_to_show is None:
        interval = round(num_frames / 4)
        frames_to_show = [0, interval, 2*interval, num_frames-1]
    if len(frames_to_show) != 4:
        raise ValueError(f"This function is designed to show exactly 4 frames, but {len(frames_to_show)} were provided.")
    fig = plt.figure(figsize=(20, 8.5))
    fig.suptitle(f"Tumor Enhancement Over Time (AF = {acceleration}, SPF = {spokes_per_frame})")
    gs = gridspec.GridSpec(2, 4, figure=fig, hspace=0.1, wspace=0.1)
    ax_curve = fig.add_subplot(gs[:, 0:2])
    ax_imgs = [
        fig.add_subplot(gs[0, 2]), fig.add_subplot(gs[0, 3]),
        fig.add_subplot(gs[1, 2]), fig.add_subplot(gs[1, 3])
    ]
    mean_curve = [img_stack[:, :, t][tumor_mask].mean() for t in range(img_stack.shape[2])]
    ax_curve.plot(time_points, mean_curve, 'o-', label='Mean Tumor Signal', linewidth=2, markersize=6)
    highlight_times = [time_points[i] for i in frames_to_show]
    highlight_vals = [mean_curve[i] for i in frames_to_show]
    ax_curve.plot(highlight_times, highlight_vals, 'r*', markersize=18, zorder=10) # zorder to ensure stars are on top
    ax_curve.set_title("Tumor Contrast Enhancement Curve (CEC)", fontsize=18, pad=10)
    ax_curve.set_xlabel("Time Frame", fontsize=16)
    ax_curve.set_ylabel("Mean Signal Intensity", fontsize=16)
    ax_curve.legend(fontsize=14)
    ax_curve.grid(True, linestyle='--')
    ax_curve.tick_params(axis='both', which='major', labelsize=14)
    contours = find_contours(tumor_mask, 0.5)
    for i, frame_idx in enumerate(frames_to_show):
        ax = ax_imgs[i]
        image = img_stack[:, :, frame_idx]
        ax.imshow(image, cmap='gray')#, vmin=vmin, vmax=vmax)
        for contour in contours:
            ax.plot(contour[:, 1], contour[:, 0], linewidth=1.5, color='red')
        ax.set_title(f"Frame {frame_idx}", fontsize=16)
        ax.axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect for suptitle
    plt.savefig(filename, bbox_inches='tight', dpi=150)
    plt.close(fig)
def plot_time_series(
    gt_img_stack: np.ndarray,
    recon_img_stack: np.ndarray,
    grasp_img_stack: np.ndarray,
    filename: str,
    acceleration: float,
    spokes_per_frame: int, 
):
    """
    Plots the middle 5 time points for Ground Truth, DL Recon, and GRASP.
    Args:
        gt_img_stack (np.ndarray): Time series of ground truth images (H, W, T).
        recon_img_stack (np.ndarray): Time series of your model's images (H, W, T).
        grasp_img_stack (np.ndarray): Time series of GRASP images (H, W, T).
        filename (str): The path to save the output plot.
    """
    num_frames = gt_img_stack.shape[2]
    indices = np.linspace(0, num_frames - 1, 5, dtype=int)
    fig, axes = plt.subplots(3, 5, figsize=(25, 15))
    fig.suptitle(f"Temporal Series Comparison (AF = {acceleration}, SPF = {spokes_per_frame})", fontsize=20)
    for i, frame_idx in enumerate(indices):
        img = gt_img_stack[:, :, frame_idx]
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f"GT: Frame {frame_idx}")
        axes[0, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = recon_img_stack[:, :, frame_idx]
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].set_title(f"DL: Frame {frame_idx}")
        axes[1, i].axis('off')
    for i, frame_idx in enumerate(indices):
        img = grasp_img_stack[:, :, frame_idx]
        axes[2, i].imshow(img, cmap='gray')
        axes[2, i].set_title(f"GRASP: Frame {frame_idx}")
        axes[2, i].axis('off')
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(filename)
    plt.close()
def tofts_model(t, Ktrans, ve, aif_t, aif_c):
    """Calculates the tissue concentration curve using the standard Tofts model."""
    ve = max(ve, 1e-6)
    interp_func = PchipInterpolator(aif_t, aif_c, extrapolate=True)
    aif_interp = interp_func(t)
    impulse_response = Ktrans * np.exp(-t * (Ktrans / ve))
    dt = t[1] - t[0] if len(t) > 1 else 1.0
    Ct = np.convolve(aif_interp, impulse_response, mode='full')[:len(t)] * dt
    return Ct
def signal_to_concentration(signal_curve, S0_pixel, T10_pixel, TR, r1, flip_angle_rad):
    """Converts an MRI signal curve S(t) to a concentration curve C(t)."""
    if S0_pixel < 1e-6:
        return np.zeros_like(signal_curve)
    norm_signal = signal_curve / S0_pixel
    sin_fa, cos_fa = np.sin(flip_angle_rad), np.cos(flip_angle_rad)
    denominator = sin_fa - norm_signal * cos_fa
    E1 = (sin_fa - norm_signal) / np.maximum(denominator, 1e-9)
    E1 = np.maximum(E1, 1e-9)
    R1_t = -np.log(E1) / TR
    R10 = 1.0 / T10_pixel
    concentration_curve = (R1_t - R10) / r1
    return np.maximum(0, concentration_curve)
def estimate_pk_parameters(
    reconstructed_images: np.ndarray,
    aif_t: np.ndarray,
    aif_c: np.ndarray,
    S0_map: np.ndarray,
    T10_map: np.ndarray,
    TR: float = 4.87e-3,
    r1: float = 4.3,
    flip_angle_deg: float = 10.0
) -> np.ndarray:
    """
    Estimates pharmacokinetic parameters (Ktrans, ve) from reconstructed DCE-MRI images.
    Args:
        reconstructed_images (np.ndarray): A (H, W, Time) array of dynamic images,
                                           THIS IS THE OUTPUT FROM YOUR DL MODEL.
        aif_t, aif_c (np.ndarray): The time points and concentrations for the AIF.
        S0_map, T10_map (np.ndarray): Baseline maps from the ground truth DRO.
        TR, r1, flip_angle_deg: Sequence parameters.
    Returns:
        np.ndarray: A (H, W, 4) array containing the estimated [ve, Ktrans, 0, 0] maps.
    """
    height, width, num_frames = reconstructed_images.shape
    flip_angle_rad = np.deg2rad(flip_angle_deg)
    time_points = aif_t
    ktrans_map = np.zeros((height, width))
    ve_map = np.zeros((height, width))
    fitting_func = lambda t, Ktrans, ve: tofts_model(t, Ktrans, ve, aif_t, aif_c)
    DEBUG_PIXEL_R, DEBUG_PIXEL_C = 150, 150
    print("Estimating PK parameters from the reconstructed images...")
    for r in tqdm(range(height), desc="Fitting PK Model"):
        for c in range(width):
            if S0_map[r, c] < np.mean(S0_map) * 0.1:
                continue
            signal_curve = np.abs(reconstructed_images[r, c, :])
            concentration_curve = signal_to_concentration(
                signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
            )
            try:
                initial_guess = [0.1 / 60, 0.2] # Ktrans in s^-1
                bounds = ([0, 0], [2.0 / 60, 1.0])
                params, _ = curve_fit(
                    fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                )
                ktrans_map[r, c] = params[0] * 60 # Convert from s^-1 to min^-1
                ve_map[r, c] = params[1]
            except RuntimeError:
                pass # Fit failed, leave as 0
            if r == DEBUG_PIXEL_R and c == DEBUG_PIXEL_C:
                print(f"\n--- DEBUGGING PIXEL ({r}, {c}) ---")
                signal_curve = np.abs(reconstructed_images[r, c, :])
                concentration_curve = signal_to_concentration(
                    signal_curve, S0_map[r, c], T10_map[r, c], TR, r1, flip_angle_rad
                )
                plt.figure(figsize=(10, 6))
                plt.plot(time_points, concentration_curve, 'bo', label='Measured Concentration (from DL Recon)')
                try:
                    params, _ = curve_fit(
                        fitting_func, time_points, concentration_curve, p0=initial_guess, bounds=bounds, method='trf'
                    )
                    ktrans_fit, ve_fit = params
                    fitted_curve = tofts_model(time_points, ktrans_fit, ve_fit, aif_t, aif_c)
                    plt.plot(time_points, fitted_curve, 'r-', label=f'Tofts Fit (Ktrans={ktrans_fit*60:.3f}, ve={ve_fit:.3f})')
                except RuntimeError:
                    plt.title(f"DEBUG: Curve fit FAILED for pixel ({r}, {c})")
                plt.xlabel("Time (s)")
                plt.ylabel("Concentration")
                plt.legend()
                plt.grid(True)
                plt.savefig(f"debug_pixel_fit_{r}_{c}.png")
                plt.close()
                print("--- DEBUG PLOT SAVED ---")
    zeros_map = np.zeros_like(ktrans_map)
    estimated_pk_map = np.stack([ve_map, ktrans_map, zeros_map, zeros_map], axis=-1)
    return estimated_pk_map
def eval_grasp(kspace, csmap, ground_truth, grasp_recon, physics, device, output_dir):
    grasp_recon_complex = rearrange(to_torch_complex(grasp_recon).squeeze(), 'h t w -> h w t')
    kspace = kspace.squeeze()
    grasp_kspace = physics(False, grasp_recon_complex.to(csmap.dtype), csmap)
    dc_mse_grasp, dc_mae_grasp = calc_dc(grasp_kspace, kspace, device)
    grasp_recon_np = grasp_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    c = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_recon = torch.tensor(c * grasp_recon_np, device=device)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    grasp_mag = torch.sqrt(grasp_recon[:, 0, ...]**2 + grasp_recon[:, 1, ...]**2)
    grasp_mag = rearrange(grasp_mag, 'c h t w -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp = calc_image_metrics(grasp_mag.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    return ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp
def eval_sample(kspace, csmap, ground_truth, x_recon, physics, mask, grasp_img, acceleration, spokes_per_frame, output_dir, label, device):
    acceleration = round(acceleration.item(), 1)
    x_recon_complex = to_torch_complex(x_recon).squeeze()
    kspace = kspace.squeeze()
    recon_kspace = physics(False, x_recon_complex, csmap)
    dc_mse, dc_mae = calc_dc(recon_kspace, kspace, device)
    x_recon_np = x_recon.cpu().numpy()
    ground_truth_np = ground_truth.cpu().numpy()
    grasp_recon_np = grasp_img.cpu().numpy()
    c = np.dot(x_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(x_recon_np.flatten(), x_recon_np.flatten())
    recon_complex_scaled = torch.tensor(c * x_recon_np, device=device)
    c_grasp = np.dot(grasp_recon_np.flatten(), ground_truth_np.flatten()) / np.dot(grasp_recon_np.flatten(), grasp_recon_np.flatten())
    grasp_img = torch.tensor(c_grasp * grasp_recon_np, device=device)
    recon_mag_scaled = torch.sqrt(recon_complex_scaled[:, 0, ...]**2 + recon_complex_scaled[:, 1, ...]**2)
    gt_mag = torch.sqrt(ground_truth[:, 0, ...]**2 + ground_truth[:, 1, ...]**2)
    recon_mag_scaled = rearrange(recon_mag_scaled, 'c h w t -> c t h w').unsqueeze(0)
    gt_mag = rearrange(gt_mag, 'c t h w -> c t h w').unsqueeze(0)
    min_val = torch.min(gt_mag).item()
    max_val = torch.max(gt_mag).item()
    data_range = (min_val, max_val)
    filename=os.path.join(output_dir, f"grasp_metric_inputs.png")
    ssim, psnr, mse, lpips = calc_image_metrics(recon_mag_scaled.contiguous(), gt_mag.contiguous(), data_range, device, filename)
    grasp_recon_complex_np = rearrange(to_torch_complex(grasp_img).squeeze(), 'h t w -> h w t').cpu().numpy()
    grasp_mag_np = np.abs(grasp_recon_complex_np)
    x_recon_complex_np = to_torch_complex(recon_complex_scaled).squeeze().cpu().numpy()
    gt_squeezed = ground_truth.squeeze()  # Shape: (C, T, H, W) -> (2, 22, 320, 320)
    gt_rearranged = rearrange(gt_squeezed, 'c t h w -> t c h w') # Shape: (22, 320, 320, 2)
    gt_complex_tensor = to_torch_complex(gt_rearranged) # Shape: (22, 320, 320)
    gt_final_tensor = rearrange(gt_complex_tensor, 't h w -> h w t') # Shape: (320, 320, 22)
    gt_complex_np = gt_final_tensor.cpu().numpy()
    recon_mag_np = np.abs(x_recon_complex_np)
    gt_mag_np = np.abs(gt_complex_np)
    masks_np = {key: val.cpu().numpy().squeeze().astype(bool) for key, val in mask.items()}
    num_frames = recon_mag_np.shape[2]
    aif_time_points = np.linspace(0, 150, num_frames)
    print("\nGenerating diagnostic plots...")
    if mask['malignant'].any() and label is not None:
        peak_frame = num_frames // 3
        data_range = gt_mag_np[:, :, peak_frame].max() - gt_mag_np[:, :, peak_frame].min()
        plot_spatial_quality(
            recon_img=recon_mag_np[:, :, peak_frame],
            gt_img=gt_mag_np[:, :, peak_frame],
            grasp_img=grasp_mag_np[:, :, peak_frame],
            time_frame_index=peak_frame,
            filename=os.path.join(output_dir, f"spatial_quality_{label}.png"),
            grasp_comparison_filename=os.path.join(output_dir, f"grasp_comparison_{label}.png"),
            data_range=data_range,
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        recon_corr, grasp_corr = plot_temporal_curves(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            filename=os.path.join(output_dir, f"temporal_curves_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        plot_single_temporal_curve(
            img_stack=recon_mag_np,
            masks=masks_np,
            time_points=aif_time_points,
            num_frames=num_frames,
            filename=os.path.join(output_dir, f"recon_temporal_curve_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        plot_time_series(
            gt_img_stack=gt_mag_np,
            recon_img_stack=recon_mag_np,
            grasp_img_stack=grasp_mag_np,
            filename=os.path.join(output_dir, f"time_points_{label}.png"),
            acceleration=acceleration,
            spokes_per_frame=spokes_per_frame,
        )
        print("Diagnostic plots saved.")
    else:
        recon_corr, grasp_corr = None, None
    return ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr

=== lsp.py ===
import torch
import numpy as np
from time import time
dtype=torch.complex64
def Project_inf(x, c, to_complex=True):
    x_max = torch.maximum((abs(x) / c), torch.tensor(1))
    if to_complex:
        x_max = x_max.to(dtype)
    s = torch.div(x, x_max)
    return s
def Wxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0:x.shape[1]-2] = x[:, 1:x.shape[1]-1]
    temp_x[:, x.shape[1]-1] = temp_x[:, x.shape[1]-1]
    res = temp_x - x
    return res
def Wtxs(x):
    temp_x = torch.zeros_like(x, dtype=dtype)
    temp_x[:, 0] = temp_x[:, 0]
    temp_x[:, 1:x.shape[1]-1] = x[:, 0:x.shape[1]-2]
    res = temp_x - x
    res[:, 0] = -x[:, 0]
    res[:, x.shape[1]-1] = x[:, x.shape[1]-2]
    return res
def LSP(param_E, param_d, param_lambda_L, param_lambda_S, param_nite, param_tol):
    '''
    :param param_E:
    :param param_d:
    :param param_lambda_L:
    :param param_lambda_S:
    :param param_nite:
    :param param_tol:
    :return:
    '''
    M = param_E(inv=True, data=param_d)
    nx, ny, nt = M.size()
    L = torch.zeros([nx * ny, nt], dtype=dtype)
    S = torch.zeros([nx * ny, nt], dtype=dtype)
    p_L = torch.zeros([nx * ny, nt], dtype=dtype)
    p_S = torch.zeros([nx * ny, nt], dtype=dtype)
    gamma = 0.5
    lambda_step = 1/10
    c = lambda_step/gamma
    loss = torch.zeros(param_nite, dtype=float)
    for itr in range(0, param_nite):
        temp_data = torch.reshape(L+S, [nx, ny, nt])
        gradient = param_E(inv=True, data=param_E(inv=False, data=temp_data) - param_d)
        gradient = torch.reshape(gradient, [nx * ny, nt])
        y_L = L - gamma * gradient - gamma * p_L
        Par_L = c * y_L + p_L
        Ut, St, Vt = torch.svd(Par_L)
        temp_St = torch.diag(Project_inf(St, param_lambda_L))
        p_L = Ut.mm(temp_St).mm(Vt.T)
        L = L - gamma * gradient - gamma * p_L
        y_S = S - gamma * gradient - gamma * Wtxs(p_S)
        Par_S = c * Wxs(y_S) + p_S
        p_S = Project_inf(Par_S, param_lambda_S)
        S = S - gamma * gradient - gamma * Wtxs(p_S)
        loss[itr] = 0
        print(' iteration: %d/%d, Loss: %f' % (itr+1, param_nite, loss[itr]))
    Ut, St, Vt = torch.svd(L)
    L = torch.matmul((Ut[:, 0]*St[0]).unsqueeze(1), Vt[:, 0].unsqueeze(0))
    L = torch.reshape(L, [nx, ny, nt])
    S = torch.reshape(S, [nx, ny, nt])
    return L, S, loss

=== dataloader.py ===
import glob
import os
import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
from typing import Union, List, Optional
import re
import csv
def target_positions_centered(Z_in: int, S_out: int, device=None, dtype=torch.float32) -> torch.Tensor:
    """
    Evenly spaced target slice positions covering the same 'z-FOV' as the input partitions.
    Shapes:
        return: (S_out,) float tensor of target slice indices in *input-partition units*
                centered so that 0 corresponds to the mid-partition
    Example:
        Z_in=83, S_out=192  -> returns 192 positions spanning [-(Z_in-1)/2, +(Z_in-1)/2]
    """
    if Z_in <= 0 or S_out <= 0:
        raise ValueError("Z_in and S_out must be positive.")
    half = (Z_in - 1) / 2.0
    return torch.linspace(-half, +half, S_out, device=device, dtype=dtype)
def _kz_grid_centered(Z_in: int, device=None, dtype=torch.float32) -> torch.Tensor:
    """
    Centered, unitless frequency grid along partitions (kz), length Z_in.
    Shapes:
        return: (Z_in,) with values ((p - (Z_in-1)/2) / Z_in), i.e., cycles per input-partition
    This pairs naturally with target_positions_centered so that W = exp(i 2 kz * z_target).
    """
    p = torch.arange(Z_in, device=device, dtype=dtype)
    return (p - (Z_in - 1) / 2.0) / Z_in
def build_reslice_weights(Z_in: int, S_out: int, device=None) -> torch.Tensor:
    """
    Build the complex phase matrix W that maps partitions -> slices in one matmul.
    Shapes:
        return: W with shape (S_out, Z_in), complex64
                W[s, z] = exp(i * 2 * kz[z] * z_targets[s])
    Where:
        kz        = centered frequency grid length Z_in
        z_targets = centered target slice indices length S_out
    """
    z_targets = target_positions_centered(Z_in, S_out, device=device)        # (S_out,)
    kz = _kz_grid_centered(Z_in, device=device)                               # (Z_in,)
    phase = 2.0 * torch.pi * (z_targets[:, None] * kz[None, :])               # (S_out, Z_in)
    W = torch.polar(torch.ones_like(phase), phase)                             # (S_out, Z_in) complex
    return W.to(torch.complex64)
def collapse_partitions_to_slices(K3d: torch.Tensor, S_out: int) -> torch.Tensor:
    """
    Collapse the partition axis into S_out slices by a single complex matrix multiply.
    Shapes:
        K3d:            (Z_in, T, C, Sp, Sa) complex64/complex128
        return Keff:    (S_out, T, C, Sp, Sa) complex64
    Notes:
        - O( S_out * Z_in * T*C*Sp*Sa ) but executed as one GEMM
        - no Python loops; everything is batched and vectorized
    """
    if K3d.dim() != 5:
        raise ValueError(f"expected (Z_in, T, C, Sp, Sa), got {tuple(K3d.shape)}")
    Z_in, T, C, Sp, Sa = K3d.shape
    device = K3d.device
    W = build_reslice_weights(Z_in, S_out, device=device)                      # (S_out, Z_in)
    K3d_flat = rearrange(K3d.to(torch.complex64), 'z t c sp sa -> z (t c sp sa)')  # (Z_in, M)
    Y_flat = W @ K3d_flat                                                       # (S_out, M)
    Keff = rearrange(Y_flat, 's (t c sp sa) -> s t c sp sa', t=T, c=C, sp=Sp, sa=Sa)
    return Keff
def resample_csmaps_along_partitions(S3d: torch.Tensor, S_out: int) -> torch.Tensor:
    """
    Linearly resample 3D coil maps along the partition axis to the same target positions.
    Shapes:
        S3d:                (1, C, H, W, Z_in) complex64/complex128
        return Seff_stack:  (S_out, 1, C, H, W) complex64
    Notes:
        - uses straight linear interpolation in partition index space
        - if you only have 2D maps, replace this with nearest-slice selection for the chosen target
    """
    if S3d.dim() != 5:
        raise ValueError(f"expected (1, C, H, W, Z_in), got {tuple(S3d.shape)}")
    _, C, H, W, Z_in = S3d.shape
    device = S3d.device
    z_targets = target_positions_centered(Z_in, S_out, device=device)          # (S_out,)
    z_cont = z_targets + (Z_in - 1) / 2.0                                      # (S_out,)
    z0 = torch.floor(z_cont).to(torch.long)                                     # (S_out,)
    z1 = torch.clamp(z0 + 1, max=Z_in - 1)                                      # (S_out,)
    alpha = (z_cont - z0.to(z_cont.dtype)).view(1, 1, 1, 1, -1)                 # (1,1,1,1,S_out)
    idx0 = z0.view(1, 1, 1, 1, -1).expand(1, C, H, W, -1)                       # (1,C,H,W,S_out)
    idx1 = z1.view(1, 1, 1, 1, -1).expand(1, C, H, W, -1)
    S_lo = torch.take_along_dim(S3d.to(torch.complex64), idx0, dim=-1)          # (1,C,H,W,S_out)
    S_hi = torch.take_along_dim(S3d.to(torch.complex64), idx1, dim=-1)          # (1,C,H,W,S_out)
    Seff = (1.0 - alpha) * S_lo + alpha * S_hi                                  # (1,C,H,W,S_out)
    Seff_stack = rearrange(Seff, 'b c h w s -> s b c h w')                       # (S_out,1,C,H,W)
    return Seff_stack
def pack_complex_to_2ch(x: torch.Tensor) -> torch.Tensor:
    """
    Pack complex tensor to 2-channel real representation.
    Shapes:
        x:       (...,) complex
        return:  (...,)-> adds a real/imag channel at the front
                 if x is (T,C,Sp,Sa) -> returns (2,T,C,Sp,Sa)
                 if x is (H,W,T)     -> returns (2,H,W,T)
    """
    xr = x.real
    xi = x.imag
    return torch.stack([xr, xi], dim=0)
class SliceDataset(Dataset):
    """
    A Dataset that:
      - Looks for all .h5/.hdf5 files under `root_dir`.
      - Each file is assumed to contain a dataset at `dataset_key`, with shape (... Z),
        where Z is the number of slices/partitions.
      - Can either use a fixed set of slices or randomly sample N slices per volume
        at the start of each epoch.
      - Returns each slice as a torch.Tensor.
    """
    def __init__(
        self,
        root_dir,
        patient_ids,
        dataset_key="kspace",
        file_pattern="*.h5",
        slice_idx: Optional[Union[int, range]] = 41,
        num_random_slices: Optional[int] = None,  # New parameter for random sampling
        N_time=8,
        N_coils=16,
        spf_aug=False,
        spokes_per_frame=None,
        weight_accelerations=False, 
        initial_spokes_range=[8, 16, 24, 36],
        interpolate_kspace=False,
        slices_to_interpolate=192,
        cluster="Randi"
    ):
        """
        Args:
            root_dir (str): Path to the folder containing all HDF5 k-space files.
            patient_ids (list): List of patient IDs to filter the files.
            dataset_key (str): The key/path inside each .h5 file to the k-space dataset.
            file_pattern (str): Glob pattern to match your HDF5 files.
            slice_idx (int, range, optional): A fixed slice index or range of indices to use.
                                              This is ignored if num_random_slices is set.
            num_random_slices (int, optional): If provided, the dataset will randomly sample
                                               this many slices from each volume at the beginning
                                               of each epoch.
        """
        super().__init__()
        self.root_dir = root_dir
        self.dataset_key = dataset_key
        self.slice_idx = slice_idx
        self.num_random_slices = num_random_slices
        self.N_time = N_time
        self.N_coils = N_coils
        self.spf_aug = spf_aug
        self.weight_acc = weight_accelerations
        self.interpolate_kspace = interpolate_kspace
        self.slices_to_interpolate = slices_to_interpolate
        self.cluster=cluster
        all_files = sorted(glob.glob(os.path.join(root_dir, file_pattern)))
        print("Number of files in root directory: ", len(all_files))
        if len(all_files) == 0:
            raise RuntimeError(
                f"No files found in {root_dir} matching pattern {file_pattern}"
            )
        filtered = []
        for fp in all_files:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in patient_ids):
                filtered.append(fp)
        self.file_list = filtered
        if len(self.file_list) == 0:
            raise RuntimeError("No files matched the provided patient_ids filter.")
        if self.num_random_slices is not None:
            print(f"Initializing in random slice sampling mode with N={self.num_random_slices} slices per volume.")
            self.volume_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    num_slices = f[self.dataset_key].shape[0]
                    self.volume_map.append((fp, num_slices))
            self.resample_slices()
        else:
            print(f"Initializing in fixed slice mode with slice_idx={self.slice_idx}.")
            self.slice_index_map = []
            for fp in self.file_list:
                with h5py.File(fp, "r") as f:
                    if self.dataset_key not in f:
                        raise KeyError(f"Dataset key '{self.dataset_key}' not found in file {fp}")
                    ds = f[self.dataset_key]
                    num_slices = ds.shape[0]
                slices_to_add = []
                if isinstance(self.slice_idx, int):
                    if self.slice_idx < num_slices:
                        slices_to_add = [self.slice_idx]
                    else:
                        print(f"Warning: slice_idx {self.slice_idx} is out of bounds for {fp} "
                              f"(size {num_slices}). Skipping this file for this slice.")
                elif isinstance(self.slice_idx, range):
                    slices_to_add = [s for s in self.slice_idx if s < num_slices]
                    if len(slices_to_add) < len(self.slice_idx):
                        print(f"Warning: Some requested slices were out of bounds for {fp}. "
                              f"Using only the valid slice indices from the provided range.")
                else:
                    raise TypeError(f"slice_idx must be an int, range, or None, but got {type(self.slice_idx)}")
                for z in slices_to_add:
                    self.slice_index_map.append((fp, z))
        print(f"Dataset initialized with {len(self.slice_index_map)} total slice examples.")
        self.spokes_per_frame = spokes_per_frame
        self.spokes_range = initial_spokes_range
        self.update_spokes_weights()
    def update_spokes_weights(self):
        if self.weight_acc:
            self.spf_weights = [1.0 / spf for spf in self.spokes_range]
        else:
            self.spf_weights = [1.0 for spf in self.spokes_range]
    def resample_slices(self):
        """
        Resamples N unique slices from each volume. This should be called at the
        beginning of each training epoch to ensure the model sees different data.
        """
        if self.num_random_slices is None:
            return
        self.slice_index_map = []
        for file_path, num_slices in self.volume_map:
            if num_slices >= self.num_random_slices:
                selected_slices = random.sample(range(num_slices), self.num_random_slices)
            else:
                print(f"Warning: Volume {os.path.basename(file_path)} has only {num_slices} slices, "
                      f"which is less than the requested {self.num_random_slices}. Using all available slices.")
                selected_slices = list(range(num_slices))
            for z in selected_slices:
                self.slice_index_map.append((file_path, z))
    def load_dynamic_img(self, patient_id, slice):
        H = W = 320
        data = np.empty((2, self.N_time, H, W), dtype=np.float32)
        for t in range(self.N_time):
            if self.cluster == "Randi":
                img_path = f'/ess/scratch/scratch1/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{slice:03d}_frame_{t:03d}.nii'
            elif self.cluster == "DSI":
                img_path = f'/net/scratch2/rachelgordon/dce-{self.N_time}tf/{patient_id}/slice_{slice:03d}_frame_{t:03d}.nii'
            else:
                raise ValueError("Undefined cluster name.")
            img = nib.load(img_path)
            img_data = img.get_fdata()
            if img_data.shape != (2, H, W):
                raise ValueError(f"{img_path} has shape {img_data.shape}; expected (2, {H}, {W})")
            data[:, t] = img_data.astype(np.float32)
        return torch.from_numpy(data)
    def load_csmaps(self, patient_id, slice):
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        csmap_path = os.path.join(ground_truth_dir, patient_id + '_cs_maps', f'cs_map_slice_{slice:03d}.npy')
        csmap = np.load(csmap_path)
        return csmap.squeeze()
    def load_all_csmaps(self, patient_id):
        """
        Loads all csmap slices for a given patient and stacks them into a single array.
        Args:
            patient_id (str): The ID of the patient.
        Returns:
            numpy.ndarray: A NumPy array containing all the stacked csmap slices
                        with the shape (1, C, H, W, Z_in).
        """
        ground_truth_dir = os.path.join(os.path.dirname(self.root_dir), 'cs_maps')
        patient_csmap_dir = os.path.join(ground_truth_dir, patient_id + '_cs_maps')
        slice_paths = sorted(glob.glob(os.path.join(patient_csmap_dir, 'cs_map_slice_*.npy')))
        if not slice_paths:
            raise FileNotFoundError(f"No csmap slices found for patient {patient_id} in {patient_csmap_dir}")
        all_slices = [np.load(path) for path in slice_paths]
        stacked_csmaps = np.stack(all_slices, axis=-1)
        print("stacked_csmaps: ", stacked_csmaps.shape)
        final_csmaps = rearrange(stacked_csmaps, 'c b h w z -> b c h w z')
        print("final_csmaps: ", final_csmaps.shape)
        return final_csmaps
    def __len__(self):
        return len(self.slice_index_map)
    def __getitem__(self, idx):
        file_path, current_slice_idx = self.slice_index_map[idx]
        current_slice_idx = int(current_slice_idx)
        patient_id = file_path.split('/')[-1].strip('.h5')
        if self.interpolate_kspace:
            csmap_stack = self.load_all_csmaps(patient_id)
            Seff_stack = resample_csmaps_along_partitions(torch.tensor(csmap_stack), S_out=self.slices_to_interpolate)        # (192, 1, C, H, W)
            csmap = Seff_stack[current_slice_idx].squeeze()                                            # (1, C, H, W)
        else:
            csmap = self.load_csmaps(patient_id, current_slice_idx)
        with h5py.File(file_path, "r") as f:
            if self.interpolate_kspace:
                ds = torch.tensor(f[self.dataset_key][:])
                print("shape before interpolation: ", ds.shape)
                Keff_stack = collapse_partitions_to_slices(ds, S_out=self.slices_to_interpolate)           # (192, T, C, Sp, Sa)
                kspace_slice = Keff_stack[current_slice_idx]                 # (2, T, C, Sp, Sa)
            else:
                kspace_slice = torch.tensor(f[self.dataset_key][current_slice_idx])
        if self.spf_aug or self.spokes_per_frame:
            total_spokes = kspace_slice.shape[0] * kspace_slice.shape[2]
            N_samples = kspace_slice.shape[-1]
            kspace = rearrange(kspace_slice, 't c sp sam -> t sp c sam')
            kspace_flat = kspace.contiguous().view(total_spokes, self.N_coils, N_samples)
            if self.spf_aug:
                print("setting random spokes per frame...")
                spokes_per_frame = random.choices(self.spokes_range, self.spf_weights, k=1)[0]
            else:
                spokes_per_frame = self.spokes_per_frame
                print(f"training with fixed spokes per frame ({spokes_per_frame})")
            N_time = total_spokes // spokes_per_frame
            kspace_binned = kspace_flat.view(N_time, spokes_per_frame, self.N_coils, N_samples)
            kspace_slice = rearrange(kspace_binned, 't sp c sam -> t c sp sam')
        else:
            N_time = self.N_time
            N_samples = kspace_slice.shape[-1]
            spokes_per_frame = kspace_slice.shape[-2]
        real_part = kspace_slice.real
        imag_part = kspace_slice.imag
        kspace_final = torch.stack([real_part, imag_part], dim=0).float()
        kspace_final = torch.flip(kspace_final, dims=[-1])
        if self.interpolate_kspace == False:
            csmap = torch.from_numpy(csmap)
        csmap_tensor = torch.rot90(csmap, k=2, dims=[-2, -1])
        csmap = csmap_tensor.numpy()
        return kspace_final, csmap, N_samples, spokes_per_frame, N_time
class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids, spokes_per_frame=36, num_frames=8):
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.model_type = model_type
        self.spokes_per_frame = spokes_per_frame
        self.num_frames = num_frames
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            print("loading grasp image from ", grasp_path)
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            print("setting grasp img to zero")
            grasp_recon_torch = 0
        kspace_path = os.path.join(sample_dir, f'simulated_kspace_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(kspace_path):
            kspace_complex = np.load(kspace_path, allow_pickle=True)
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            kspace_torch = kspace_path
        ground_truth_complex = dro['ground_truth_images']
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask
class SimulatedSPFDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, model_type, patient_ids):
        self.model_type = model_type
        self.root_dir = root_dir
        self.patient_ids = patient_ids
        self.spokes_per_frame = 16
        self.num_frames = 18
        self._update_sample_paths()
        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]
    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)
        self.sample_paths = filtered
        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")
    def load_kspace_from_csv_mapping(self, sample_id: str, mapping_file_path: str, data_dir: str) -> np.ndarray:
        """
        Parses a sample ID to get the DRO ID, uses a CSV mapping file to find the fastMRI ID,
        constructs the fastMRI HDF5 file path, and loads the k-space data.
        Args:
            sample_id (str): The sample ID string (e.g., "sample_020_sub20").
                            Expected format: "sample_XXX_subYY", where XXX is the DRO ID.
            mapping_file_path (str): The file path to the CSV file containing the
                                    DRO to fastMRIbreast ID mapping.
                                    Expected header: "DRO,fastMRIbreast".
            data_dir (str): The base directory where fastMRI HDF5 files are stored.
                            E.g., if files are in '/path/to/data/fastMRI_breast_157_2.h5',
                            then data_dir should be '/path/to/data'.
        Returns:
            numpy.ndarray: The complex k-space data from the corresponding fastMRI file.
        Raises:
            ValueError: If the sample_id format is invalid, DRO ID not found in mapping,
                        or the CSV mapping file is malformed.
            FileNotFoundError: If the mapping CSV or the constructed fastMRI HDF5 file does not exist.
            KeyError: If the 'kspace' dataset is not found within the HDF5 file.
            RuntimeError: For other issues encountered during file loading.
        """
        dro_to_fastmri_map = {}
        try:
            with open(mapping_file_path, mode='r', newline='', encoding='utf-8') as csvfile:
                reader = csv.reader(csvfile)
                try:
                    header = [h.strip() for h in next(reader)]
                except StopIteration:
                    raise ValueError(f"Mapping file is empty: {mapping_file_path}")
                if header != ['DRO', 'fastMRIbreast']:
                    raise ValueError(f"Mapping file header is invalid. Expected ['DRO', 'fastMRIbreast'], but got {header}.")
                for i, row in enumerate(reader):
                    if not row:  # Skip empty lines
                        continue
                    try:
                        dro_id = int(row[0].strip())
                        fastmri_id = int(row[1].strip())
                        dro_to_fastmri_map[dro_id] = fastmri_id
                    except (ValueError, IndexError):
                        raise ValueError(f"Invalid mapping data in row {i+2} of {mapping_file_path}: {row}. Expected two integers.")
        except FileNotFoundError:
            raise FileNotFoundError(f"Mapping CSV file not found at: {mapping_file_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading or parsing mapping file {mapping_file_path}: {e}") from e
        match = re.match(r"sample_(\d+)_sub\d+", sample_id)
        if not match:
            raise ValueError(
                f"Invalid sample_id format: '{sample_id}'. "
                f"Expected 'sample_XXX_subYY' where XXX is the DRO ID."
            )
        dro_id_from_sample = int(match.group(1))
        fastmri_id = dro_to_fastmri_map.get(dro_id_from_sample)
        if fastmri_id is None:
            raise ValueError(
                f"DRO ID {dro_id_from_sample} from sample_id '{sample_id}' "
                f"not found in the mapping file."
            )
        file_name = f"fastMRI_breast_{fastmri_id}_2.h5"
        file_path = os.path.join(data_dir, file_name)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"fastMRI HDF5 file not found at: {file_path}")
        try:
            with h5py.File(file_path, 'r') as f:
                if 'ktspace' not in f:
                    raise KeyError(f"'ktspace' dataset not found in file: {file_path}. "
                                f"Available keys: {list(f.keys())}")
                kspace_data = f['ktspace'][()] 
                return kspace_data
        except Exception as e:
            raise RuntimeError(f"Error loading k-space from {file_path}: {e}") from e
    def __len__(self):
        return len(self.sample_paths)
    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]
        print(f"  Testing {self.spokes_per_frame} spokes/frame with {self.num_frames} frames.")
        print("loading data from ", sample_dir)
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')
        if os.path.exists(grasp_path):
            grasp_recon = np.load(grasp_path)
            grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
            grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)
            grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
            grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])
        else:
            grasp_recon_torch = 0
        ground_truth_complex = dro['ground_truth_images']
        smap_torch = rearrange(torch.tensor(csmaps), 'h w c -> c h w').unsqueeze(0)
        simImg_torch = torch.tensor(ground_truth_complex).to(torch.cfloat)
        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            if tissue_name in dro:
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        mask = mask_dictionary_rebuilt
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)
        return smap_torch, simImg_torch, grasp_recon_torch, mask, grasp_path #, parMap, aif, S0, T10, mask

