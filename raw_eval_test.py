import glob
import os

import h5py
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
import nibabel as nib
from einops import rearrange
import random
import sigpy as sp
from utils import prep_nufft
from radial_lsfp import MCNUFFT
import time
from typing import Union, List, Optional
import re
import csv
import yaml
import math
import json
from tqdm import tqdm
import pandas as pd
import argparse
import json
import os
import matplotlib.pyplot as plt
import torch
import yaml
from dataloader import ZFSliceDataset, SimulatedDataset, SimulatedSPFDataset
from einops import rearrange
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from transform import VideoRotate, VideoDiffeo, SubsampleTime, MonophasicTimeWarp, TemporalNoise, TimeReverse
from ei import EILoss
from mc import MCLoss
from lsfpnet_encoding import LSFPNet, ArtifactRemovalLSFPNet
from radial_lsfp import MCNUFFT
from utils import prep_nufft, log_gradient_stats, plot_enhancement_curve, get_cosine_ei_weight, plot_reconstruction_sample, get_git_commit, save_checkpoint, load_checkpoint, to_torch_complex, GRASPRecon, sliding_window_inference, set_seed
from eval_new import eval_grasp, eval_sample
import csv
import math
import random
import time 
import seaborn as sns
from loss_metrics import LPIPSVideoMetric, SSIMVideoMetric
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import h5py
from torch.utils.tensorboard import SummaryWriter
from raw_kspace_eval import eval_raw_kspace, eval_raw_kspace_grasp

class SimulatedDataset(Dataset):
    """
    Dataset for loading the simulated data generated by your script.
    It loads the simulated k-space, coil sensitivity maps, and the
    ground truth dynamic image (DRO).
    """
    def __init__(self, root_dir, raw_kspace_path, model_type, patient_ids, dataset_key, grasp_slice_idx=95, spokes_per_frame=36, num_frames=8):

        self.root_dir = root_dir
        self.raw_kspace_path = raw_kspace_path
        self.patient_ids = patient_ids
        self.model_type = model_type
        self.spokes_per_frame = spokes_per_frame
        self.num_frames = num_frames
        self.grasp_slice_idx = grasp_slice_idx
        self.dataset_key = dataset_key
        self._update_sample_paths()


        self.TISSUE_NAMES = [
            'glandular', 'benign', 'malignant', 'muscle',
            'skin', 'liver', 'heart', 'vascular'
        ]

    def _update_sample_paths(self):
        self.dro_dir = os.path.join(self.root_dir, f'dro_{self.num_frames}frames')

        # Find all sample directories, e.g., 'sample_001_sub1', 'sample_002_sub2', etc.
        self.sample_paths = sorted(glob.glob(os.path.join(self.dro_dir, 'sample_*')))
        if not self.sample_paths:
            raise FileNotFoundError(f"No sample directories found in {self.dro_dir}. "
                                    "Please check the path to your simulated dataset.")
        
        # filter file list by patient ID substring
        filtered = []
        for fp in self.sample_paths:
            fname = os.path.basename(fp)
            # Check if any patient_id appears in the filename
            if any(pid in fname for pid in self.patient_ids):
                filtered.append(fp)

        self.sample_paths = filtered

        print(f"Found {len(self.sample_paths)} simulated samples in {self.dro_dir} for {self.num_frames} frames.")


    def get_fastMRI_id(self, sample_dir):

        sample_file = os.path.basename(sample_dir)
        id_map = pd.read_csv('data/DROSubID_vs_fastMRIbreastID.csv')

        dro_id = int(sample_file.split("_")[1])
        dro_row = id_map[id_map["DRO"] == dro_id]
        fastmri_id = int(dro_row["fastMRIbreast"].iloc[0])

        return fastmri_id
  
            
    def __len__(self):
        return len(self.sample_paths)
    


    def __getitem__(self, idx):
        sample_dir = self.sample_paths[idx]


        # Load the data from .npy files
        csmaps = np.load(os.path.join(sample_dir, 'csmaps.npy'))
        dro = np.load(os.path.join(sample_dir, 'dro_ground_truth.npz'))
        grasp_path = os.path.join(sample_dir, f'grasp_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')

        grasp_recon = np.load(grasp_path)

        # GRASP Recon: (H, W, T) -> (2, T, H, W) [real/imag, time, h, w]
        grasp_recon_torch = torch.from_numpy(grasp_recon).permute(2, 0, 1) # T, H, W
        grasp_recon_torch = torch.stack([grasp_recon_torch.real, grasp_recon_torch.imag], dim=0)

        grasp_recon_torch = torch.flip(grasp_recon_torch, dims=[-3])
        grasp_recon_torch = torch.rot90(grasp_recon_torch, k=3, dims=[-3,-1])

        kspace_path = os.path.join(sample_dir, f'simulated_kspace_spf{self.spokes_per_frame}_frames{self.num_frames}.npy')

        if os.path.exists(kspace_path):
            kspace_complex = np.load(kspace_path, allow_pickle=True)
            kspace_torch = torch.from_numpy(kspace_complex)
        else:
            kspace_torch = kspace_path


        # load raw k-space and GRASP recon
        fastmri_id = self.get_fastMRI_id(sample_dir)
        raw_grasp_path = os.path.join(os.path.dirname(self.raw_kspace_path), f'fastMRI_breast_{fastmri_id:03d}_2/grasp_recon_{self.spokes_per_frame}spf_{self.num_frames}frames_slice{self.grasp_slice_idx}.npy')
        raw_kspace_path = os.path.join(self.raw_kspace_path, f'fastMRI_breast_{fastmri_id:03d}_2.h5')
        raw_csmap_path = os.path.join(os.path.dirname(self.raw_kspace_path), f'cs_maps/fastMRI_breast_{fastmri_id:03d}_2_cs_maps/cs_map_slice_{self.grasp_slice_idx:03d}.npy')
        
        raw_csmaps = np.load(raw_csmap_path)
        # raw_csmaps = rearrange(raw_csmaps, 'c b h w -> b c h w')

        raw_grasp_recon = np.load(raw_grasp_path).squeeze()


        # GRASP Recon: (H, W, T) -> (2, T, H, W) [real/imag, time, h, w]
        raw_grasp_recon = torch.from_numpy(raw_grasp_recon).permute(2, 0, 1) # T, H, W
        raw_grasp_recon = torch.stack([raw_grasp_recon.real, raw_grasp_recon.imag], dim=0)

        raw_grasp_recon = torch.flip(raw_grasp_recon, dims=[-3])
        raw_grasp_recon = torch.rot90(raw_grasp_recon, k=1, dims=[-3,-1])


        with h5py.File(raw_kspace_path, "r") as f:
            raw_kspace_slice = torch.tensor(f[self.dataset_key][self.grasp_slice_idx])

        # time-bin k-space
        N_spokes_prep = self.num_frames * self.spokes_per_frame

        ksp_redu = raw_kspace_slice[:, :N_spokes_prep, :] # (16, 288, 640)
        ksp_prep = np.swapaxes(ksp_redu, 0, 1) # (288, 16, 640)
        ksp_prep_shape = ksp_prep.shape
        ksp_prep = np.reshape(ksp_prep, [self.num_frames, self.spokes_per_frame] + list(ksp_prep_shape[1:]))
        raw_kspace_slice = rearrange(ksp_prep, 't sp c sam -> c (sp sam) t').to(kspace_torch.dtype)


        ground_truth_complex = dro['ground_truth_images']

        parMap = dro['parMap']
        aif = dro['aif']
        S0 = dro['S0']
        T10 = dro['T10']
        # mask = dro['mask']

        # ==========================================================
        # --- RECONSTRUCT THE MASK DICTIONARY ---
        # ==========================================================
        mask_dictionary_rebuilt = {}
        for tissue_name in self.TISSUE_NAMES:
            # Check if the key for this tissue (e.g., 'malignant') exists in the file
            if tissue_name in dro:
                # Load the boolean array and add it to the dictionary
                mask_dictionary_rebuilt[tissue_name] = dro[tissue_name]
        
        # 'mask' is now the dictionary of boolean arrays, just like your functions expect
        mask = mask_dictionary_rebuilt


        # --- Convert to PyTorch Tensors ---
        # Ground truth: (H, W, T) -> (2, T, H, W) [real/imag, time, h, w]
        ground_truth_torch = torch.from_numpy(ground_truth_complex).permute(2, 0, 1) # T, H, W
        ground_truth_torch = torch.stack([ground_truth_torch.real, ground_truth_torch.imag], dim=0)

        # CSMaps: (H, W, C) -> (1, C, H, W) [batch, coils, h, w]
        csmaps_torch = torch.from_numpy(csmaps).permute(2, 0, 1).unsqueeze(0)
        raw_csmaps_torch = torch.from_numpy(raw_csmaps)#.permute(2, 0, 1).unsqueeze(0)
        raw_csmaps_torch = rearrange(raw_csmaps_torch, 'c b h w -> b c h w').to(csmaps_torch.dtype)

        return kspace_torch, csmaps_torch, ground_truth_torch, grasp_recon_torch, mask, grasp_path, raw_kspace_slice, raw_grasp_recon, raw_csmaps_torch #, parMap, aif, S0, T10, mask
    


# load params
with open('configs/config_mc_zf_debug.yaml', "r") as file:
    config = yaml.safe_load(file)

split_file = config["data"]["split_file"]

data_dir = config["data"]["root_dir"]

batch_size = config["dataloader"]["batch_size"]
max_subjects = config["dataloader"]["max_subjects"]

initial_lambdas = {'lambda_L': config['model']['lambda_L'], 
                'lambda_S': config['model']['lambda_S'], 
                'lambda_spatial_L': config['model']['lambda_spatial_L'],
                'lambda_spatial_S': config['model']['lambda_spatial_S'],
                'gamma': config['model']['gamma'],
                'lambda_step': config['model']['lambda_step']}

mc_loss_weight = config["model"]["losses"]["mc_loss"]["weight"]
adj_loss_weight = config["model"]["losses"]["adj_loss"]["weight"]

use_ei_loss = config["model"]["losses"]["use_ei_loss"]
target_w_ei = config["model"]["losses"]["ei_loss"]["weight"]
warmup = config["model"]["losses"]["ei_loss"]["warmup"]
duration = config["model"]["losses"]["ei_loss"]["duration"]

save_interval = config["training"]["save_interval"]
plot_interval = config["training"]["plot_interval"]

model_type = config["model"]["name"]

H, W = config["data"]["height"], config["data"]["width"]
N_time, N_samples, N_coils = (
    config["data"]["timeframes"],
    config["data"]["samples"],
    config["data"]["coils"]
)
Ng = config["data"]["fpg"] 

total_spokes = config["data"]["total_spokes"]

N_spokes = int(total_spokes / N_time)
N_full = config['data']['height'] * math.pi / 2

N_slices = config['data']['slices']
num_slices_to_eval = config['data']['slices_to_eval']
eval_frequency = config['data']['eval_frequency']

eval_chunk_size = config["evaluation"]["chunk_size"]
eval_chunk_overlap = config["evaluation"]["chunk_overlap"]

cluster = config["experiment"].get("cluster", "Randi")


N_time_eval, N_spokes_eval = config["data"]["eval_timeframes"], config["data"]["eval_spokes"]


device = torch.device(config["training"]["device"])


# create output directories
exp_name = "raw_eval_test"
output_dir = os.path.join(config["experiment"]["output_dir"], exp_name)
eval_dir = os.path.join(output_dir, "eval_results")
block_dir = os.path.join(output_dir, "block_outputs")
ec_dir = os.path.join(output_dir, 'enhancement_curves')

os.makedirs(output_dir, exist_ok=True)
os.makedirs(eval_dir, exist_ok=True)
os.makedirs(block_dir, exist_ok=True)
os.makedirs(ec_dir, exist_ok=True)

# load data
with open(split_file, "r") as fp:
    splits = json.load(fp)

if max_subjects < 300:
    max_train = int(max_subjects * (1 - config["data"]["val_split_ratio"]))
    train_patient_ids = splits["train"][:max_train]
    
else:
    train_patient_ids = splits["train"]

val_patient_ids = splits["val"]
val_dro_patient_ids = splits["val_dro"]




# define physics object for evaluation
eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob = prep_nufft(N_samples, N_spokes_eval, N_time_eval)
eval_ktraj = eval_ktraj.to(device)
eval_dcomp = eval_dcomp.to(device)
eval_nufft_ob = eval_nufft_ob.to(device)
eval_adjnufft_ob = eval_adjnufft_ob.to(device)


eval_physics = MCNUFFT(eval_nufft_ob, eval_adjnufft_ob, eval_ktraj, eval_dcomp)


val_dro_dataset = SimulatedDataset(
    root_dir=config["evaluation"]["simulated_dataset_path"], 
    raw_kspace_path=data_dir,
    model_type=model_type, 
    patient_ids=val_dro_patient_ids,
    dataset_key=config["data"]["dataset_key"],
    spokes_per_frame=N_spokes_eval,
    num_frames=N_time_eval)


val_dro_loader = DataLoader(
    val_dro_dataset,
    batch_size=config["dataloader"]["batch_size"],
    shuffle=False,
    num_workers=config["dataloader"]["num_workers"],
    pin_memory=True,
)


# define model
lsfp_backbone = LSFPNet(LayerNo=config["model"]["num_layers"], 
                        lambdas=initial_lambdas, 
                        channels=config['model']['channels'],
                        style_dim=config['model']['style_dim'],
                        svd_mode=config['model']['svd_mode'],
                        use_lowk_dc=config['model']['use_lowk_dc'],
                        lowk_frac=config['model']['lowk_frac'],
                        lowk_alpha=config['model']['lowk_alpha'],
                        film_bounded=config['model']['film_bounded'],
                        film_gain=config['model']['film_gain'],
                        film_identity_init=config['model']['film_identity_init'],
                        svd_noise_std=config['model']['svd_noise_std'],
                        film_L=config['model']['film_L'],
                        )


model = ArtifactRemovalLSFPNet(lsfp_backbone, block_dir, channels=2).to(device)


for dro_kspace, csmap, ground_truth, dro_grasp_img, mask, grasp_path, raw_kspace, raw_grasp_img, raw_csmaps in tqdm(val_dro_loader, desc="Step 0 Validation Evaluation"):

    csmap = csmap.squeeze(0).to(device)   # Remove batch dim
    ground_truth = ground_truth.to(device) # Shape: (1, 2, T, H, W)

    dro_kspace = dro_kspace.squeeze(0).to(device) # Remove batch dim
    dro_grasp_img = dro_grasp_img.to(device) # Shape: (1, 2, H, T, W)

    raw_kspace = raw_kspace.squeeze(0).to(device) # Remove batch dim
    raw_grasp_img = raw_grasp_img.to(device) # Shape: (1, 2, H, T, W)
    raw_csmaps = raw_csmaps.squeeze(0).to(device)   # Remove batch dim


    N_spokes = eval_ktraj.shape[1] / config['data']['samples']
    acceleration = torch.tensor([N_full / int(N_spokes)], dtype=torch.float, device=device)

    if config['model']['encode_acceleration']:
        acceleration_encoding = acceleration
    else: 
        acceleration_encoding = None

    if config['model']['encode_time_index'] == False:
        start_timepoint_index = None
    else:
        start_timepoint_index = torch.tensor([0], dtype=torch.float, device=device)



    # inference
    if N_time_eval > eval_chunk_size:
        x_recon, adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, dro_kspace, csmap, acceleration_encoding, start_timepoint_index, model, epoch="val0", device=device)  
        raw_x_recon, adj_loss = sliding_window_inference(H, W, N_time_eval, eval_ktraj, eval_dcomp, eval_nufft_ob, eval_adjnufft_ob, eval_chunk_size, eval_chunk_overlap, raw_kspace, raw_csmaps, acceleration_encoding, start_timepoint_index, model, epoch="val0", device=device)  
    else:
        x_recon, adj_loss, *_ = model(
        dro_kspace.to(device), eval_physics, csmap, acceleration_encoding, start_timepoint_index, epoch="val0", norm=config['model']['norm']
        )
        raw_x_recon, adj_loss, *_ = model(
        raw_kspace.to(device), eval_physics, raw_csmaps, acceleration_encoding, start_timepoint_index, epoch="val0", norm=config['model']['norm']
        )


    # fix orientation of raw k-space recon
    raw_x_recon = torch.rot90(raw_x_recon, k=2, dims=[-3,-2])

        
    

    # calculate eval metrics
    # DRO simulated k-space
    ssim_grasp, psnr_grasp, mse_grasp, lpips_grasp, dc_mse_grasp, dc_mae_grasp = eval_grasp(dro_kspace, csmap, ground_truth, dro_grasp_img, eval_physics, device, eval_dir, dro_eval=True)
    ssim, psnr, mse, lpips, dc_mse, dc_mae, recon_corr, grasp_corr = eval_sample(dro_kspace, csmap, ground_truth, x_recon, eval_physics, mask, dro_grasp_img, acceleration, int(N_spokes), eval_dir, label='val0', device=device, dro_eval=True)


    # raw k-space
    dc_mse_raw_grasp, dc_mae_raw_grasp = eval_grasp(raw_kspace, raw_csmaps, ground_truth, raw_grasp_img, eval_physics, device, eval_dir, dro_eval=False)
    dc_mse_raw, dc_mae_raw = eval_sample(raw_kspace, raw_csmaps, ground_truth, raw_x_recon, eval_physics, mask, raw_grasp_img, acceleration, int(N_spokes), eval_dir, label='val0', device=device, dro_eval=False)



